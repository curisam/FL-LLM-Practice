2025-09-11 07:00:05 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/gfl/fedbiscuit_u3_1.0_plain/exp_print.log
2025-09-11 07:00:05 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/gfl/fedbiscuit_u3_1.0_plain
2025-09-11 07:00:05 (root:430) INFO: [logger] non-main process: file handler not attached (rank=1/4)
2025-09-11 07:00:05 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/gfl/fedbiscuit_u3_1.0_plain
2025-09-11 07:00:06 (root:430) INFO: [logger] non-main process: file handler not attached (rank=2/4)
2025-09-11 07:00:06 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/gfl/fedbiscuit_u3_1.0_plain
2025-09-11 07:00:06 (root:430) INFO: [logger] non-main process: file handler not attached (rank=3/4)
2025-09-11 07:00:06 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/gfl/fedbiscuit_u3_1.0_plain
/home/seongyoon/anaconda3/envs/fs-llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/seongyoon/anaconda3/envs/fs-llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/seongyoon/anaconda3/envs/fs-llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/seongyoon/anaconda3/envs/fs-llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2025-09-11 07:00:06 (federatedscope.llm.dataloader.reddit_tldr:437) INFO: Process 1: Waiting for completion file...
2025-09-11 07:00:06 (federatedscope.llm.dataloader.reddit_tldr:440) INFO: Process 1: Completion file found.
2025-09-11 07:00:06 (federatedscope.llm.dataloader.reddit_tldr:437) INFO: Process 2: Waiting for completion file...
2025-09-11 07:00:06 (federatedscope.llm.dataloader.reddit_tldr:440) INFO: Process 2: Completion file found.
2025-09-11 07:00:06 (federatedscope.llm.dataloader.reddit_tldr:437) INFO: Process 3: Waiting for completion file...
2025-09-11 07:00:06 (federatedscope.llm.dataloader.reddit_tldr:440) INFO: Process 3: Completion file found.
2025-09-11 07:00:28 (federatedscope.core.data.base_translator:239) INFO: Process 3: Waiting for final_datadict completion file...
2025-09-11 07:00:28 (federatedscope.core.data.base_translator:242) INFO: Process 3: Completion file found.
2025-09-11 07:00:29 (federatedscope.core.data.base_translator:239) INFO: Process 2: Waiting for final_datadict completion file...
2025-09-11 07:00:29 (federatedscope.core.data.base_translator:242) INFO: Process 2: Completion file found.
2025-09-11 07:00:29 (federatedscope.core.data.base_translator:239) INFO: Process 1: Waiting for final_datadict completion file...
2025-09-11 07:00:29 (federatedscope.core.data.base_translator:242) INFO: Process 1: Completion file found.
2025-09-11 07:00:31 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=3/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=3/4] Train=2793, Val=146, Test=40, Total=2979
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=3/4] Train=214, Val=11, Test=40, Total=265
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=3/4] Train=691, Val=36, Test=40, Total=767
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=3/4] Train=213, Val=11, Test=40, Total=264
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=3/4] Train=285, Val=14, Test=40, Total=339
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=3/4] Train=2547, Val=134, Test=40, Total=2721
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=3/4] Train=1088, Val=57, Test=40, Total=1185
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=3/4] Train=1316, Val=69, Test=40, Total=1425
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=3/4] Train=3572, Val=188, Test=40, Total=3800
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=3/4] Train=1209, Val=63, Test=40, Total=1312
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=3/4] Train=621, Val=32, Test=40, Total=693
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=3/4] Train=2605, Val=137, Test=40, Total=2782
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=3/4] Train=1372, Val=72, Test=40, Total=1484
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=3/4] Train=3055, Val=160, Test=40, Total=3255
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=3/4] Train=14550, Val=200, Test=40, Total=14790
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=3/4] Train=2589, Val=136, Test=40, Total=2765
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=3/4] Train=5883, Val=200, Test=40, Total=6123
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=3/4] Train=2576, Val=135, Test=40, Total=2751
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=3/4] Train=2102, Val=110, Test=40, Total=2252
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=3/4] Train=2399, Val=126, Test=40, Total=2565
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=3/4] Train=2915, Val=153, Test=40, Total=3108
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=3/4] Train=224, Val=11, Test=40, Total=275
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=3/4] Train=583, Val=30, Test=40, Total=653
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=3/4] Train=4944, Val=200, Test=40, Total=5184
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=3/4] Train=4647, Val=200, Test=40, Total=4887
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=3/4] Train=3063, Val=161, Test=40, Total=3264
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=3/4] Train=2342, Val=123, Test=40, Total=2505
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=3/4] Train=1434, Val=75, Test=40, Total=1549
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=3/4] Train=6191, Val=200, Test=40, Total=6431
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=3/4] Train=3247, Val=170, Test=40, Total=3457
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=3/4] Train=3679, Val=193, Test=40, Total=3912
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=3/4] Train=2144, Val=112, Test=40, Total=2296
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=3/4] Train=1409, Val=74, Test=40, Total=1523
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=3/4] Train=4486, Val=200, Test=40, Total=4726
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=3/4] Train=4736, Val=200, Test=40, Total=4976
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=3/4] Train=1030, Val=54, Test=40, Total=1124
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=3/4] Train=4273, Val=200, Test=40, Total=4513
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=3/4] Train=6171, Val=200, Test=40, Total=6411
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=3/4] Train=1594, Val=83, Test=40, Total=1717
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=3/4] Train=4005, Val=200, Test=40, Total=4245
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=3/4] Train=2275, Val=119, Test=40, Total=2434
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=3/4] Train=5772, Val=200, Test=40, Total=6012
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=3/4] Train=1694, Val=89, Test=40, Total=1823
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=3/4] Train=7916, Val=200, Test=40, Total=8156
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=3/4] Train=1901, Val=100, Test=40, Total=2041
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=3/4] Train=2100, Val=110, Test=40, Total=2250
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=3/4] Train=2812, Val=147, Test=40, Total=2999
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=3/4] Train=880, Val=46, Test=40, Total=966
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=3/4] Train=2521, Val=132, Test=40, Total=2693
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=3/4] Train=2527, Val=133, Test=40, Total=2700
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=3/4] Train=1580, Val=83, Test=40, Total=1703
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=3/4] Train=3589, Val=188, Test=40, Total=3817
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=3/4] Train=6791, Val=200, Test=40, Total=7031
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=2/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=2/4] Train=2793, Val=146, Test=40, Total=2979
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=2/4] Train=214, Val=11, Test=40, Total=265
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=2/4] Train=691, Val=36, Test=40, Total=767
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=2/4] Train=213, Val=11, Test=40, Total=264
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=2/4] Train=285, Val=14, Test=40, Total=339
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=2/4] Train=2547, Val=134, Test=40, Total=2721
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=2/4] Train=1088, Val=57, Test=40, Total=1185
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=2/4] Train=1316, Val=69, Test=40, Total=1425
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=2/4] Train=3572, Val=188, Test=40, Total=3800
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=2/4] Train=1209, Val=63, Test=40, Total=1312
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=2/4] Train=621, Val=32, Test=40, Total=693
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=2/4] Train=2605, Val=137, Test=40, Total=2782
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=2/4] Train=1372, Val=72, Test=40, Total=1484
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=2/4] Train=3055, Val=160, Test=40, Total=3255
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=2/4] Train=14550, Val=200, Test=40, Total=14790
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=2/4] Train=2589, Val=136, Test=40, Total=2765
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=2/4] Train=5883, Val=200, Test=40, Total=6123
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=2/4] Train=2576, Val=135, Test=40, Total=2751
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=2/4] Train=2102, Val=110, Test=40, Total=2252
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=2/4] Train=2399, Val=126, Test=40, Total=2565
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=2/4] Train=2915, Val=153, Test=40, Total=3108
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=2/4] Train=224, Val=11, Test=40, Total=275
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=2/4] Train=583, Val=30, Test=40, Total=653
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=2/4] Train=4944, Val=200, Test=40, Total=5184
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=2/4] Train=4647, Val=200, Test=40, Total=4887
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=2/4] Train=3063, Val=161, Test=40, Total=3264
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=2/4] Train=2342, Val=123, Test=40, Total=2505
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=2/4] Train=1434, Val=75, Test=40, Total=1549
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=2/4] Train=6191, Val=200, Test=40, Total=6431
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=2/4] Train=3247, Val=170, Test=40, Total=3457
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=2/4] Train=3679, Val=193, Test=40, Total=3912
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=2/4] Train=2144, Val=112, Test=40, Total=2296
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=2/4] Train=1409, Val=74, Test=40, Total=1523
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=2/4] Train=4486, Val=200, Test=40, Total=4726
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=2/4] Train=4736, Val=200, Test=40, Total=4976
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=2/4] Train=1030, Val=54, Test=40, Total=1124
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=2/4] Train=4273, Val=200, Test=40, Total=4513
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=2/4] Train=6171, Val=200, Test=40, Total=6411
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=2/4] Train=1594, Val=83, Test=40, Total=1717
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=2/4] Train=4005, Val=200, Test=40, Total=4245
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=2/4] Train=2275, Val=119, Test=40, Total=2434
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=2/4] Train=5772, Val=200, Test=40, Total=6012
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=2/4] Train=1694, Val=89, Test=40, Total=1823
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=2/4] Train=7916, Val=200, Test=40, Total=8156
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=2/4] Train=1901, Val=100, Test=40, Total=2041
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=2/4] Train=2100, Val=110, Test=40, Total=2250
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=2/4] Train=2812, Val=147, Test=40, Total=2999
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=2/4] Train=880, Val=46, Test=40, Total=966
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=2/4] Train=2521, Val=132, Test=40, Total=2693
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=2/4] Train=2527, Val=133, Test=40, Total=2700
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=2/4] Train=1580, Val=83, Test=40, Total=1703
2025-09-11 07:01:09 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=2/4] Train=3589, Val=188, Test=40, Total=3817
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=2/4] Train=6791, Val=200, Test=40, Total=7031
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=1/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=1/4] Train=2793, Val=146, Test=40, Total=2979
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=1/4] Train=214, Val=11, Test=40, Total=265
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=1/4] Train=691, Val=36, Test=40, Total=767
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=1/4] Train=213, Val=11, Test=40, Total=264
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=1/4] Train=285, Val=14, Test=40, Total=339
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=1/4] Train=2547, Val=134, Test=40, Total=2721
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=1/4] Train=1088, Val=57, Test=40, Total=1185
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=1/4] Train=1316, Val=69, Test=40, Total=1425
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=1/4] Train=3572, Val=188, Test=40, Total=3800
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=1/4] Train=1209, Val=63, Test=40, Total=1312
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=1/4] Train=621, Val=32, Test=40, Total=693
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=1/4] Train=2605, Val=137, Test=40, Total=2782
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=1/4] Train=1372, Val=72, Test=40, Total=1484
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=1/4] Train=3055, Val=160, Test=40, Total=3255
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=1/4] Train=14550, Val=200, Test=40, Total=14790
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=1/4] Train=2589, Val=136, Test=40, Total=2765
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=1/4] Train=5883, Val=200, Test=40, Total=6123
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=1/4] Train=2576, Val=135, Test=40, Total=2751
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=1/4] Train=2102, Val=110, Test=40, Total=2252
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=1/4] Train=2399, Val=126, Test=40, Total=2565
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=1/4] Train=2915, Val=153, Test=40, Total=3108
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=1/4] Train=224, Val=11, Test=40, Total=275
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=1/4] Train=583, Val=30, Test=40, Total=653
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=1/4] Train=4944, Val=200, Test=40, Total=5184
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=1/4] Train=4647, Val=200, Test=40, Total=4887
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=1/4] Train=3063, Val=161, Test=40, Total=3264
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=1/4] Train=2342, Val=123, Test=40, Total=2505
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=1/4] Train=1434, Val=75, Test=40, Total=1549
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=1/4] Train=6191, Val=200, Test=40, Total=6431
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=1/4] Train=3247, Val=170, Test=40, Total=3457
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=1/4] Train=3679, Val=193, Test=40, Total=3912
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=1/4] Train=2144, Val=112, Test=40, Total=2296
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=1/4] Train=1409, Val=74, Test=40, Total=1523
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=1/4] Train=4486, Val=200, Test=40, Total=4726
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=1/4] Train=4736, Val=200, Test=40, Total=4976
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=1/4] Train=1030, Val=54, Test=40, Total=1124
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=1/4] Train=4273, Val=200, Test=40, Total=4513
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=1/4] Train=6171, Val=200, Test=40, Total=6411
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=1/4] Train=1594, Val=83, Test=40, Total=1717
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=1/4] Train=4005, Val=200, Test=40, Total=4245
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=1/4] Train=2275, Val=119, Test=40, Total=2434
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=1/4] Train=5772, Val=200, Test=40, Total=6012
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=1/4] Train=1694, Val=89, Test=40, Total=1823
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=1/4] Train=7916, Val=200, Test=40, Total=8156
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=1/4] Train=1901, Val=100, Test=40, Total=2041
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=1/4] Train=2100, Val=110, Test=40, Total=2250
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=1/4] Train=2812, Val=147, Test=40, Total=2999
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=1/4] Train=880, Val=46, Test=40, Total=966
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=1/4] Train=2521, Val=132, Test=40, Total=2693
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=1/4] Train=2527, Val=133, Test=40, Total=2700
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=1/4] Train=1580, Val=83, Test=40, Total=1703
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=1/4] Train=3589, Val=188, Test=40, Total=3817
2025-09-11 07:01:10 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=1/4] Train=6791, Val=200, Test=40, Total=7031
2025-09-11 07:01:11 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-09-11 07:01:11 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-09-11 07:01:12 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-09-11 07:01:12 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-09-11 07:01:13 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-09-11 07:01:13 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-09-11 07:01:13 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-09-11 07:01:14 (federatedscope.core.configs.config:256) INFO: the used configs are: 
adapter:
  use: False
aggregator:
  BFT_args:
    
  byzantine_node_num: 0
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
  robust_rule: fedavg
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.9637]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.1592]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: True
  drop_last: False
  file_path: 
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  load_splits: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  save_splits: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.9, 0.09, 0.01]
  splits_path: ./final_data_splits
  splitter: meta
  splitter_args: []
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: reddit-tldr-comparison-choice@llm
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 2
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 0
eval:
  best_res_update_round_wise_key: test_loss
  count_flops: False
  freq: 25
  metrics: ['loss', 'acc']
  monitoring: []
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['val', 'test']
expname: 
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_idx_for_local_train: 0
  client_num: 53
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  master_addr: 127.0.0.1
  master_port: 29500
  merge_test_data: False
  merge_val_data: False
  method: FedAvg
  mode: standalone
  online_aggr: False
  process_num: 1
  resource_info_file: 
  restore_from: 
  sample_client_num: 5
  sample_client_rate: -1.0
  sampler: uniform
  save_client_model: False
  save_freq: 25
  save_to: checkpoints_1.0/tldr_choice_qwen_fedbiscuit_u3_plain.ckpt
  share_local_model: True
  total_round_num: 175
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
fedswa:
  use: False
finetune:
  batch_or_epoch: epoch
  before_eval: False
  epoch_linear: 10
  freeze_param: 
  local_param: []
  local_update_steps: 1
  lr_linear: 0.005
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
  simple_tuning: False
  weight_decay: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  fts:
    M: 100
    M_target: 200
    allow_load_existing_info: True
    diff: False
    fed_bo_max_iter: 50
    g_var: 1e-06
    gp_opt_schedule: 1
    local_bo_epochs: 50
    local_bo_max_iter: 50
    ls: 1.0
    obs_noise: 1e-06
    ss: 
    target_clients: []
    use: False
    v_kernel: 1.0
    var: 0.1
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  pfedhpo:
    discrete: False
    ss: 
    target_fl_total_round: 1000
    train_anchor: False
    train_fl: False
    use: False
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  trial_index: 0
  working_folder: hpo
llm:
  accelerator:
    config: 
    use: True
  adapter:
    args: [{'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}]
    balance: False
    count: 3
    grouping:
      round: 25
      use: True
    local_only: False
    mv_to_cpu: False
    use: True
    warmup:
      round: 0
      use: True
  cache:
    model: 
  chat:
    max_history_len: 10
    max_len: 1024
  deepspeed:
    ds_config: 
    use: False
  fedrlhf:
    config_file: 
    frequency: 100
    pretrained: False
    train:
      batch_or_epoch: batch
      local_update_steps: 10
    use: False
  grad_accum_step: 2
  max_new_token: 60
  num_completions: 2
  offsite_tuning:
    emu_align:
      data:
        root: data
        splits: [0.8, 0.1, 0.1]
        type: alpaca@llm
      exit_after_align: False
      init_enable_ground_truth: False
      initial_only: True
      kl_divergence: raw
      layerwise_distill: False
      restore_from: 
      save_to: 
      sim_loss: l2
      train:
        batch_or_epoch: batch
        enable_ground_truth: False
        initial_update_rounds: 50
        kd_loss_weight: 0.9
        lm_loss_weight: 0.1
        local_update_steps: 10
        optimizer:
          lr: 0.01
          type: SGD
      use: False
    emu_l: 1
    emu_r: 10
    eval_type: emu
    kwargs: [{}]
    llm_generated:
      ratio: 0.1
      use: False
    save_full_model: False
    strategy: drop_layer
    use: False
  retry_on_nan_loss: False
  reward_coeff: 0.1
  rlhf: False
  tok_len: 1024
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.5
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 256
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  llm_kwargs: [{}]
  llm_type: CausalLM
  load_from_local_pretrained_fs_config: 
  load_from_local_pretrained_model_path: checkpoints_u10_warmup_1.0/final_tldr_choice_qwen_fedbiscuit_u10_warmup_1.0_round_250.ckpt
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 1
  pretrain_tasks: []
  stage: 
  task: node
  type: Qwen/Qwen2-0.5B@huggingface_llm
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/tldr/choice_qwen/gfl/fedbiscuit_u3_1.0_plain
personalization:
  K: 5
  beta: 1.0
  epoch_feature: 1
  epoch_linear: 2
  local_param: []
  local_update_steps: 30
  lr: 1e-05
  lr_feature: 0.1
  lr_linear: 0.1
  regular_weight: 0.1
  share_non_trainable_para: False
  weight_decay: 0.0
print_decimal_digits: 6
quantization:
  method: none
  nbits: 8
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  data_para_dids: []
  is_enable_half: True
  local_update_steps: 30
  optimizer:
    betas: (0.9, 0.95)
    lr: 1e-05
    type: AdamW
  scheduler:
    gamma: 1.0
    milestones: [75, 125]
    type: 
    warmup_ratio: 0.0
trainer:
  choices: ['A', 'B']
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.0001
    gamma: 0.03
    inc_factor: 1.0
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: llmrewardchoicetrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2025-09-11 07:01:15 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-09-11 07:01:15 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-09-11 07:01:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=3] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140456019976256 | out_emb=(Linear) num=151646 ptr=140456019976256 | lora_ptr=None
2025-09-11 07:01:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=2] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140117199904832 | out_emb=(Linear) num=151646 ptr=140117199904832 | lora_ptr=None
trainable params: 4,399,104 || all params: 498,172,032 || trainable%: 0.8830491712549612
trainable params: 4,399,104 || all params: 498,172,032 || trainable%: 0.8830491712549612
2025-09-11 07:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=1] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=139977080791104 | out_emb=(Linear) num=151646 ptr=139977080791104 | lora_ptr=None
trainable params: 4,399,104 || all params: 498,172,032 || trainable%: 0.8830491712549612
2025-09-11 07:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140276954652736 | out_emb=(Linear) num=151646 ptr=140276954652736 | lora_ptr=None
trainable params: 4,399,104 || all params: 498,172,032 || trainable%: 0.8830491712549612
2025-09-11 07:01:25 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_u10_warmup_1.0/final_tldr_choice_qwen_fedbiscuit_u10_warmup_1.0_round_250.ckpt (round=250) | missing=291 unexpected=2352
2025-09-11 07:01:25 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-09-11 07:01:26 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_u10_warmup_1.0/final_tldr_choice_qwen_fedbiscuit_u10_warmup_1.0_round_250.ckpt (round=250) | missing=291 unexpected=2352
2025-09-11 07:01:26 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-09-11 07:01:27 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_u10_warmup_1.0/final_tldr_choice_qwen_fedbiscuit_u10_warmup_1.0_round_250.ckpt (round=250) | missing=291 unexpected=2352
2025-09-11 07:01:27 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-09-11 07:01:28 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:29 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_u10_warmup_1.0/final_tldr_choice_qwen_fedbiscuit_u10_warmup_1.0_round_250.ckpt (round=250) | missing=291 unexpected=2352
2025-09-11 07:01:29 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-09-11 07:01:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:30 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:30 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:30 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
/home/seongyoon/anaconda3/envs/fs-llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2025-09-11 07:01:31 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:32 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
/home/seongyoon/anaconda3/envs/fs-llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2025-09-11 07:01:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:32 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
/home/seongyoon/anaconda3/envs/fs-llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2025-09-11 07:01:32 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
/home/seongyoon/anaconda3/envs/fs-llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
2025-09-11 07:01:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:33 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-09-11 07:01:33 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:35 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-09-11 07:01:35 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-09-11 07:01:35 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-09-11 07:01:35 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:35 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:35 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:35 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-09-11 07:01:35 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:37 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-09-11 07:01:37 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-09-11 07:01:37 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-09-11 07:01:37 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:37 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:37 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:38 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-09-11 07:01:38 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:40 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-09-11 07:01:40 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:40 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-09-11 07:01:40 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:40 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-09-11 07:01:40 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-09-11 07:01:40 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:40 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:42 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-09-11 07:01:42 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:42 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-09-11 07:01:43 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:43 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-09-11 07:01:43 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-09-11 07:01:43 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:43 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:44 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-09-11 07:01:45 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:45 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-09-11 07:01:45 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-09-11 07:01:45 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-09-11 07:01:45 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:45 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:45 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:47 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-09-11 07:01:47 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:47 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-09-11 07:01:48 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:48 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-09-11 07:01:48 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-09-11 07:01:48 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:48 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:50 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-09-11 07:01:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:50 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-09-11 07:01:50 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-09-11 07:01:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:50 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-09-11 07:01:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:52 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-09-11 07:01:52 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:52 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-09-11 07:01:53 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-09-11 07:01:53 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-09-11 07:01:53 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:53 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:53 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:55 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-09-11 07:01:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:55 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-09-11 07:01:55 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-09-11 07:01:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:55 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-09-11 07:01:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:57 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-09-11 07:01:57 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:57 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-09-11 07:01:57 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-09-11 07:01:58 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:58 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:01:58 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-09-11 07:01:58 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:00 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-09-11 07:02:00 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:00 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-09-11 07:02:00 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-09-11 07:02:00 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:00 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:01 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-09-11 07:02:01 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:02 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-09-11 07:02:02 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:02 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-09-11 07:02:02 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-09-11 07:02:02 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:03 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:03 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-09-11 07:02:03 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:04 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-09-11 07:02:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:05 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-09-11 07:02:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:05 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-09-11 07:02:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:06 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-09-11 07:02:06 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:07 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-09-11 07:02:07 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:07 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-09-11 07:02:07 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:08 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-09-11 07:02:08 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:08 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-09-11 07:02:08 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:09 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-09-11 07:02:10 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:10 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-09-11 07:02:10 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:10 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-09-11 07:02:10 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:11 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-09-11 07:02:11 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:12 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-09-11 07:02:12 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:13 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-09-11 07:02:13 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:13 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-09-11 07:02:13 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:14 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-09-11 07:02:14 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:15 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-09-11 07:02:16 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:16 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-09-11 07:02:16 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:16 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-09-11 07:02:17 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:17 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-09-11 07:02:17 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:18 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-09-11 07:02:18 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:18 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-09-11 07:02:19 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:19 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-09-11 07:02:19 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:20 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-09-11 07:02:20 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:21 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-09-11 07:02:21 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:21 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-09-11 07:02:21 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:22 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-09-11 07:02:22 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:22 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-09-11 07:02:23 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:23 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-09-11 07:02:23 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:23 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-09-11 07:02:24 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:24 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-09-11 07:02:24 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:25 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-09-11 07:02:25 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:26 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-09-11 07:02:26 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:26 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-09-11 07:02:26 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:27 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-09-11 07:02:27 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:28 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-09-11 07:02:28 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:28 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-09-11 07:02:28 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:28 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-09-11 07:02:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:29 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-09-11 07:02:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:30 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-09-11 07:02:30 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:31 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-09-11 07:02:31 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:31 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-09-11 07:02:31 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:32 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-09-11 07:02:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:33 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-09-11 07:02:33 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-09-11 07:02:33 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:33 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:33 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-09-11 07:02:34 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:34 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-09-11 07:02:34 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:35 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-09-11 07:02:36 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:36 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-09-11 07:02:36 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:36 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-09-11 07:02:36 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:37 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-09-11 07:02:37 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:38 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-09-11 07:02:38 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:38 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-09-11 07:02:38 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-09-11 07:02:38 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:39 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:39 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-09-11 07:02:39 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:40 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-09-11 07:02:41 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:41 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-09-11 07:02:41 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:41 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-09-11 07:02:41 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:42 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-09-11 07:02:42 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:43 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-09-11 07:02:43 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:43 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-09-11 07:02:44 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:44 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-09-11 07:02:44 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:45 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-09-11 07:02:45 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:46 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-09-11 07:02:46 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:46 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-09-11 07:02:46 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:46 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-09-11 07:02:46 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:47 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-09-11 07:02:47 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:48 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-09-11 07:02:48 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:48 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-09-11 07:02:49 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-09-11 07:02:49 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:49 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:50 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-09-11 07:02:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:50 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-09-11 07:02:51 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:51 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-09-11 07:02:51 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-09-11 07:02:51 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:51 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:52 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-09-11 07:02:52 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:53 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-09-11 07:02:53 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:54 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-09-11 07:02:54 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-09-11 07:02:54 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:54 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:55 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-09-11 07:02:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:55 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-09-11 07:02:56 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:56 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-09-11 07:02:56 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-09-11 07:02:56 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:56 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:57 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-09-11 07:02:57 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:58 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-09-11 07:02:58 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:59 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-09-11 07:02:59 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:02:59 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-09-11 07:02:59 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:00 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-09-11 07:03:00 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:00 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-09-11 07:03:01 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:01 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-09-11 07:03:01 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:01 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-09-11 07:03:02 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:02 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-09-11 07:03:02 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:03 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-09-11 07:03:03 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:04 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-09-11 07:03:04 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:04 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-09-11 07:03:05 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-09-11 07:03:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:06 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-09-11 07:03:06 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:07 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-09-11 07:03:07 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:07 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-09-11 07:03:07 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:08 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-09-11 07:03:08 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:09 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-09-11 07:03:09 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:10 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-09-11 07:03:10 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:10 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-09-11 07:03:10 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:11 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-09-11 07:03:11 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:11 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-09-11 07:03:12 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:12 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-09-11 07:03:12 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:13 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-09-11 07:03:13 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:13 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-09-11 07:03:14 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:14 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-09-11 07:03:14 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:15 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-09-11 07:03:15 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:15 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-09-11 07:03:15 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:16 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-09-11 07:03:16 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:16 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-09-11 07:03:17 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:17 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-09-11 07:03:17 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:18 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-09-11 07:03:18 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:18 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-09-11 07:03:19 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:19 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-09-11 07:03:19 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:20 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-09-11 07:03:20 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:20 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-09-11 07:03:21 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:21 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-09-11 07:03:21 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:21 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-09-11 07:03:22 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:22 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-09-11 07:03:22 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:23 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-09-11 07:03:23 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:24 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-09-11 07:03:24 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:24 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-09-11 07:03:24 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:25 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-09-11 07:03:25 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:26 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-09-11 07:03:26 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:26 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-09-11 07:03:26 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:26 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-09-11 07:03:26 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:27 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-09-11 07:03:27 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:28 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-09-11 07:03:29 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-09-11 07:03:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:29 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-09-11 07:03:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:30 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-09-11 07:03:30 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:31 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-09-11 07:03:31 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-09-11 07:03:31 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:31 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-09-11 07:03:31 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:31 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:32 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-09-11 07:03:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:34 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-09-11 07:03:34 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-09-11 07:03:34 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-09-11 07:03:34 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:34 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:34 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:35 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-09-11 07:03:35 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:36 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-09-11 07:03:36 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:36 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-09-11 07:03:36 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-09-11 07:03:36 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:37 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:38 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-09-11 07:03:38 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:39 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-09-11 07:03:39 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:39 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-09-11 07:03:39 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-09-11 07:03:39 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:39 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:40 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-09-11 07:03:40 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:41 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-09-11 07:03:41 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:41 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-09-11 07:03:41 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-09-11 07:03:42 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:42 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:43 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-09-11 07:03:43 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-09-11 07:03:43 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-09-11 07:03:43 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-09-11 07:03:43 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight'}.
2025-09-11 07:03:43 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-09-11 07:03:43 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-09-11 07:03:43 (federatedscope.llm.llm_local.server:103) INFO: Waited all clients join, start now...
2025-09-11 07:03:43 (federatedscope.llm.llm_local.server:111) INFO: ----------- Starting training (Round #0) -------------
2025-09-11 07:03:43 (federatedscope.llm.llm_local.server:114) INFO: Server: Performing a grouping step...
2025-09-11 07:03:44 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-09-11 07:03:44 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:44 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-09-11 07:03:44 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-09-11 07:03:44 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-09-11 07:03:44 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-09-11 07:03:44 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-09-11 07:03:44 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight'}.
2025-09-11 07:03:44 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-09-11 07:03:44 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-09-11 07:03:44 (federatedscope.llm.llm_local.server:103) INFO: Waited all clients join, start now...
2025-09-11 07:03:44 (federatedscope.llm.llm_local.server:111) INFO: ----------- Starting training (Round #0) -------------
2025-09-11 07:03:44 (federatedscope.llm.llm_local.server:114) INFO: Server: Performing a grouping step...
2025-09-11 07:03:44 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:46 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
osilab6:3741467:3741467 [0] NCCL INFO Bootstrap : Using enp194s0f0:143.248.157.69<0>
osilab6:3741467:3741467 [0] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
osilab6:3741467:3741467 [0] NCCL INFO NET/Plugin : No plugin found, using internal implementation
osilab6:3741467:3741467 [0] NCCL INFO cudaDriverVersion 12020
NCCL version 2.18.5+cuda12.1
osilab6:3741469:3741469 [2] NCCL INFO cudaDriverVersion 12020
osilab6:3741469:3741469 [2] NCCL INFO Bootstrap : Using enp194s0f0:143.248.157.69<0>
osilab6:3741469:3741469 [2] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
osilab6:3741469:3741469 [2] NCCL INFO NET/Plugin : No plugin found, using internal implementation
osilab6:3741467:3745117 [0] NCCL INFO NET/IB : No device found.
osilab6:3741467:3745117 [0] NCCL INFO NET/Socket : Using [0]enp194s0f0:143.248.157.69<0> [1]br-baacb8116f62:172.18.0.1<0> [2]enxbabf9f493f81:fe80::cc45:5b64:362a:a4f7%enxbabf9f493f81<0> [3]veth7c41e46:fe80::786b:4dff:fe5c:cb8a%veth7c41e46<0> [4]veth0930612:fe80::a0bd:dcff:fedf:53df%veth0930612<0> [5]veth840722f:fe80::f86e:3eff:fe53:c33f%veth840722f<0>
osilab6:3741467:3745117 [0] NCCL INFO Using network Socket
osilab6:3741469:3745118 [2] NCCL INFO NET/IB : No device found.
osilab6:3741469:3745118 [2] NCCL INFO NET/Socket : Using [0]enp194s0f0:143.248.157.69<0> [1]br-baacb8116f62:172.18.0.1<0> [2]enxbabf9f493f81:fe80::cc45:5b64:362a:a4f7%enxbabf9f493f81<0> [3]veth7c41e46:fe80::786b:4dff:fe5c:cb8a%veth7c41e46<0> [4]veth0930612:fe80::a0bd:dcff:fedf:53df%veth0930612<0> [5]veth840722f:fe80::f86e:3eff:fe53:c33f%veth840722f<0>
osilab6:3741469:3745118 [2] NCCL INFO Using network Socket
2025-09-11 07:03:46 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-11 07:03:46 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-09-11 07:03:46 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-09-11 07:03:46 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-09-11 07:03:46 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-09-11 07:03:46 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight'}.
2025-09-11 07:03:46 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-09-11 07:03:46 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-09-11 07:03:46 (federatedscope.llm.llm_local.server:103) INFO: Waited all clients join, start now...
2025-09-11 07:03:46 (federatedscope.llm.llm_local.server:111) INFO: ----------- Starting training (Round #0) -------------
2025-09-11 07:03:46 (federatedscope.llm.llm_local.server:114) INFO: Server: Performing a grouping step...
osilab6:3741468:3741468 [1] NCCL INFO cudaDriverVersion 12020
osilab6:3741468:3741468 [1] NCCL INFO Bootstrap : Using enp194s0f0:143.248.157.69<0>
osilab6:3741468:3741468 [1] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
osilab6:3741468:3741468 [1] NCCL INFO NET/Plugin : No plugin found, using internal implementation
osilab6:3741468:3745124 [1] NCCL INFO NET/IB : No device found.
osilab6:3741468:3745124 [1] NCCL INFO NET/Socket : Using [0]enp194s0f0:143.248.157.69<0> [1]br-baacb8116f62:172.18.0.1<0> [2]enxbabf9f493f81:fe80::cc45:5b64:362a:a4f7%enxbabf9f493f81<0> [3]veth7c41e46:fe80::786b:4dff:fe5c:cb8a%veth7c41e46<0> [4]veth0930612:fe80::a0bd:dcff:fedf:53df%veth0930612<0> [5]veth840722f:fe80::f86e:3eff:fe53:c33f%veth840722f<0>
osilab6:3741468:3745124 [1] NCCL INFO Using network Socket
2025-09-11 07:03:49 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-09-11 07:03:49 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-09-11 07:03:49 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 1344.
2025-09-11 07:03:49 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 1634.
2025-09-11 07:03:49 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 1344. 
Preserved para names in local update: {'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight'}.
2025-09-11 07:03:49 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-09-11 07:03:49 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-09-11 07:03:49 (federatedscope.llm.llm_local.server:103) INFO: Waited all clients join, start now...
2025-09-11 07:03:49 (federatedscope.llm.llm_local.server:111) INFO: ----------- Starting training (Round #0) -------------
2025-09-11 07:03:49 (federatedscope.llm.llm_local.server:114) INFO: Server: Performing a grouping step...
osilab6:3741471:3741471 [3] NCCL INFO cudaDriverVersion 12020
osilab6:3741471:3741471 [3] NCCL INFO Bootstrap : Using enp194s0f0:143.248.157.69<0>
osilab6:3741471:3741471 [3] NCCL INFO NET/Plugin : Plugin load (libnccl-net.so) returned 2 : libnccl-net.so: cannot open shared object file: No such file or directory
osilab6:3741471:3741471 [3] NCCL INFO NET/Plugin : No plugin found, using internal implementation
osilab6:3741471:3745148 [3] NCCL INFO NET/IB : No device found.
osilab6:3741471:3745148 [3] NCCL INFO NET/Socket : Using [0]enp194s0f0:143.248.157.69<0> [1]br-baacb8116f62:172.18.0.1<0> [2]enxbabf9f493f81:fe80::cc45:5b64:362a:a4f7%enxbabf9f493f81<0> [3]veth7c41e46:fe80::786b:4dff:fe5c:cb8a%veth7c41e46<0> [4]veth0930612:fe80::a0bd:dcff:fedf:53df%veth0930612<0> [5]veth840722f:fe80::f86e:3eff:fe53:c33f%veth840722f<0>
osilab6:3741471:3745148 [3] NCCL INFO Using network Socket
osilab6:3741471:3745148 [3] NCCL INFO comm 0x66a03a30 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId e1000 commId 0x39f7afb552ef6034 - Init START
osilab6:3741469:3745118 [2] NCCL INFO comm 0x20cf1030 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId c1000 commId 0x39f7afb552ef6034 - Init START
osilab6:3741468:3745124 [1] NCCL INFO comm 0x33a86c00 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId a1000 commId 0x39f7afb552ef6034 - Init START
osilab6:3741467:3745117 [0] NCCL INFO comm 0x560d5a20 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 81000 commId 0x39f7afb552ef6034 - Init START
osilab6:3741471:3745148 [3] NCCL INFO Setting affinity for GPU 7 to ffffffff,00000000,ffffffff,00000000
osilab6:3741471:3745148 [3] NCCL INFO NVLS multicast support is not available on dev 3
osilab6:3741468:3745124 [1] NCCL INFO Setting affinity for GPU 5 to ffffffff,00000000,ffffffff,00000000
osilab6:3741468:3745124 [1] NCCL INFO NVLS multicast support is not available on dev 1
osilab6:3741469:3745118 [2] NCCL INFO Setting affinity for GPU 6 to ffffffff,00000000,ffffffff,00000000
osilab6:3741469:3745118 [2] NCCL INFO NVLS multicast support is not available on dev 2
osilab6:3741467:3745117 [0] NCCL INFO Setting affinity for GPU 4 to ffffffff,00000000,ffffffff,00000000
osilab6:3741467:3745117 [0] NCCL INFO NVLS multicast support is not available on dev 0
osilab6:3741471:3745148 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2
osilab6:3741471:3745148 [3] NCCL INFO P2P Chunksize set to 131072
osilab6:3741469:3745118 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1
osilab6:3741469:3745118 [2] NCCL INFO P2P Chunksize set to 131072
osilab6:3741467:3745117 [0] NCCL INFO Channel 00/04 :    0   1   2   3
osilab6:3741467:3745117 [0] NCCL INFO Channel 01/04 :    0   1   2   3
osilab6:3741467:3745117 [0] NCCL INFO Channel 02/04 :    0   1   2   3
osilab6:3741468:3745124 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0
osilab6:3741467:3745117 [0] NCCL INFO Channel 03/04 :    0   1   2   3
osilab6:3741468:3745124 [1] NCCL INFO P2P Chunksize set to 131072
osilab6:3741467:3745117 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1
osilab6:3741467:3745117 [0] NCCL INFO P2P Chunksize set to 131072
osilab6:3741471:3745148 [3] NCCL INFO Channel 00/0 : 3[7] -> 0[4] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Channel 00/0 : 2[6] -> 3[7] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Channel 00/0 : 1[5] -> 2[6] via P2P/IPC
osilab6:3741471:3745148 [3] NCCL INFO Channel 01/0 : 3[7] -> 0[4] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Channel 01/0 : 2[6] -> 3[7] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Channel 01/0 : 1[5] -> 2[6] via P2P/IPC
osilab6:3741471:3745148 [3] NCCL INFO Channel 02/0 : 3[7] -> 0[4] via P2P/IPC
osilab6:3741467:3745117 [0] NCCL INFO Channel 00/0 : 0[4] -> 1[5] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Channel 02/0 : 2[6] -> 3[7] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Channel 02/0 : 1[5] -> 2[6] via P2P/IPC
osilab6:3741471:3745148 [3] NCCL INFO Channel 03/0 : 3[7] -> 0[4] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Channel 03/0 : 2[6] -> 3[7] via P2P/IPC
osilab6:3741467:3745117 [0] NCCL INFO Channel 01/0 : 0[4] -> 1[5] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Channel 03/0 : 1[5] -> 2[6] via P2P/IPC
osilab6:3741467:3745117 [0] NCCL INFO Channel 02/0 : 0[4] -> 1[5] via P2P/IPC
osilab6:3741467:3745117 [0] NCCL INFO Channel 03/0 : 0[4] -> 1[5] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Connected all rings
osilab6:3741471:3745148 [3] NCCL INFO Connected all rings
osilab6:3741471:3745148 [3] NCCL INFO Channel 00/0 : 3[7] -> 2[6] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Connected all rings
osilab6:3741471:3745148 [3] NCCL INFO Channel 01/0 : 3[7] -> 2[6] via P2P/IPC
osilab6:3741467:3745117 [0] NCCL INFO Connected all rings
osilab6:3741471:3745148 [3] NCCL INFO Channel 02/0 : 3[7] -> 2[6] via P2P/IPC
osilab6:3741471:3745148 [3] NCCL INFO Channel 03/0 : 3[7] -> 2[6] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Channel 00/0 : 2[6] -> 1[5] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Channel 01/0 : 2[6] -> 1[5] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Channel 00/0 : 1[5] -> 0[4] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Channel 02/0 : 2[6] -> 1[5] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Channel 01/0 : 1[5] -> 0[4] via P2P/IPC
osilab6:3741469:3745118 [2] NCCL INFO Channel 03/0 : 2[6] -> 1[5] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Channel 02/0 : 1[5] -> 0[4] via P2P/IPC
osilab6:3741468:3745124 [1] NCCL INFO Channel 03/0 : 1[5] -> 0[4] via P2P/IPC
osilab6:3741471:3745148 [3] NCCL INFO Connected all trees
osilab6:3741471:3745148 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
osilab6:3741471:3745148 [3] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
osilab6:3741467:3745117 [0] NCCL INFO Connected all trees
osilab6:3741467:3745117 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
osilab6:3741467:3745117 [0] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
osilab6:3741469:3745118 [2] NCCL INFO Connected all trees
osilab6:3741469:3745118 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
osilab6:3741469:3745118 [2] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
osilab6:3741468:3745124 [1] NCCL INFO Connected all trees
osilab6:3741468:3745124 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
osilab6:3741468:3745124 [1] NCCL INFO 4 coll channels, 0 nvls channels, 4 p2p channels, 2 p2p channels per peer
osilab6:3741469:3745118 [2] NCCL INFO comm 0x20cf1030 rank 2 nranks 4 cudaDev 2 nvmlDev 6 busId c1000 commId 0x39f7afb552ef6034 - Init COMPLETE
osilab6:3741468:3745124 [1] NCCL INFO comm 0x33a86c00 rank 1 nranks 4 cudaDev 1 nvmlDev 5 busId a1000 commId 0x39f7afb552ef6034 - Init COMPLETE
osilab6:3741467:3745117 [0] NCCL INFO comm 0x560d5a20 rank 0 nranks 4 cudaDev 0 nvmlDev 4 busId 81000 commId 0x39f7afb552ef6034 - Init COMPLETE
osilab6:3741471:3745148 [3] NCCL INFO comm 0x66a03a30 rank 3 nranks 4 cudaDev 3 nvmlDev 7 busId e1000 commId 0x39f7afb552ef6034 - Init COMPLETE
2025-09-11 07:03:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:03:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:03:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:03:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:03:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=37, total=146)
2025-09-11 07:03:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=36, total=146)
2025-09-11 07:03:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-11 07:03:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=36, total=146)
2025-09-11 07:03:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139977080791104 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:03:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140276954652736 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:03:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140117199904832 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:03:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140456019976256 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:03:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:03:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:03:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:03:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:03:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-11 07:03:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=37
2025-09-11 07:03:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=36
2025-09-11 07:03:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=36
2025-09-11 07:03:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=101.321960, avg_loss=0.693986, seen=146, correct=78, accuracy=0.534247
2025-09-11 07:03:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:03:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:03:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:03:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:03:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:03:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:03:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:03:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1768MB
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1768MB
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2054MB allocated=1768MB
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1768MB
2025-09-11 07:04:00 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 101.32196044921875, 'val_avg_loss': 0.693986030474101, 'val_seen': 146, 'val_correct': 78, 'val_acc': 0.5342465753424658}
2025-09-11 07:04:00 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 101.32196044921875, 'val_avg_loss': 0.693986030474101, 'val_seen': 146, 'val_correct': 78, 'val_acc': 0.5342465753424658}
2025-09-11 07:04:00 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 101.32196044921875, 'val_avg_loss': 0.693986030474101, 'val_seen': 146, 'val_correct': 78, 'val_acc': 0.5342465753424658}
2025-09-11 07:04:00 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 101.32196044921875, 'val_avg_loss': 0.693986030474101, 'val_seen': 146, 'val_correct': 78, 'val_acc': 0.5342465753424658}
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.011082, avg_loss=0.675277, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:04:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1768MB
2025-09-11 07:04:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1768MB
2025-09-11 07:04:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1768MB
2025-09-11 07:04:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1768MB
2025-09-11 07:04:02 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.01108169555664, 'test_avg_loss': 0.6752770423889161, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:04:02 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.01108169555664, 'test_avg_loss': 0.6752770423889161, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:04:02 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.01108169555664, 'test_avg_loss': 0.6752770423889161, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:04:02 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.01108169555664, 'test_avg_loss': 0.6752770423889161, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:04:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-11 07:04:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=37, total=146)
2025-09-11 07:04:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=36, total=146)
2025-09-11 07:04:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=36, total=146)
2025-09-11 07:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:04:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:04:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:04:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:04:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=36
2025-09-11 07:04:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-11 07:04:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=37
2025-09-11 07:04:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=36
2025-09-11 07:04:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=100.138161, avg_loss=0.685878, seen=146, correct=92, accuracy=0.630137
2025-09-11 07:04:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:08 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 100.1381607055664, 'val_avg_loss': 0.6858778130518247, 'val_seen': 146, 'val_correct': 92, 'val_acc': 0.6301369863013698}
2025-09-11 07:04:08 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 100.1381607055664, 'val_avg_loss': 0.6858778130518247, 'val_seen': 146, 'val_correct': 92, 'val_acc': 0.6301369863013698}
2025-09-11 07:04:08 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 100.1381607055664, 'val_avg_loss': 0.6858778130518247, 'val_seen': 146, 'val_correct': 92, 'val_acc': 0.6301369863013698}
2025-09-11 07:04:08 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 100.1381607055664, 'val_avg_loss': 0.6858778130518247, 'val_seen': 146, 'val_correct': 92, 'val_acc': 0.6301369863013698}
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.572327, avg_loss=0.689308, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:10 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.57232666015625, 'test_avg_loss': 0.6893081665039062, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:10 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.57232666015625, 'test_avg_loss': 0.6893081665039062, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:10 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.57232666015625, 'test_avg_loss': 0.6893081665039062, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:10 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.57232666015625, 'test_avg_loss': 0.6893081665039062, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-11 07:04:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=37, total=146)
2025-09-11 07:04:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=36, total=146)
2025-09-11 07:04:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=36, total=146)
2025-09-11 07:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:04:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:04:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:04:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-11 07:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=36
2025-09-11 07:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=37
2025-09-11 07:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=36
2025-09-11 07:04:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=100.607170, avg_loss=0.689090, seen=146, correct=75, accuracy=0.513699
2025-09-11 07:04:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:16 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 100.60717010498047, 'val_avg_loss': 0.6890902061984964, 'val_seen': 146, 'val_correct': 75, 'val_acc': 0.5136986301369864}
2025-09-11 07:04:16 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 100.60717010498047, 'val_avg_loss': 0.6890902061984964, 'val_seen': 146, 'val_correct': 75, 'val_acc': 0.5136986301369864}
2025-09-11 07:04:16 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 100.60717010498047, 'val_avg_loss': 0.6890902061984964, 'val_seen': 146, 'val_correct': 75, 'val_acc': 0.5136986301369864}
2025-09-11 07:04:16 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 100.60717010498047, 'val_avg_loss': 0.6890902061984964, 'val_seen': 146, 'val_correct': 75, 'val_acc': 0.5136986301369864}
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.406742, avg_loss=0.685169, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:04:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1743MB
2025-09-11 07:04:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1743MB
2025-09-11 07:04:19 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.406742095947266, 'test_avg_loss': 0.6851685523986817, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:19 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.406742095947266, 'test_avg_loss': 0.6851685523986817, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:19 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.406742095947266, 'test_avg_loss': 0.6851685523986817, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:19 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.406742095947266, 'test_avg_loss': 0.6851685523986817, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:04:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:04:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:04:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.612769, avg_loss=0.692070, seen=11, correct=6, accuracy=0.545455
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:04:22 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.61276912689209, 'val_avg_loss': 0.6920699206265536, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-11 07:04:22 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.61276912689209, 'val_avg_loss': 0.6920699206265536, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-11 07:04:22 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.61276912689209, 'val_avg_loss': 0.6920699206265536, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-11 07:04:22 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.61276912689209, 'val_avg_loss': 0.6920699206265536, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.227852, avg_loss=0.680696, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:04:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:04:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:04:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:04:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:04:24 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.22785186767578, 'test_avg_loss': 0.6806962966918946, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:24 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.22785186767578, 'test_avg_loss': 0.6806962966918946, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:24 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.22785186767578, 'test_avg_loss': 0.6806962966918946, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:24 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.22785186767578, 'test_avg_loss': 0.6806962966918946, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:04:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:04:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:04:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.892160, avg_loss=0.717469, seen=11, correct=4, accuracy=0.363636
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:26 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.892159938812256, 'val_avg_loss': 0.7174690853465687, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:04:26 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.892159938812256, 'val_avg_loss': 0.7174690853465687, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:04:26 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.892159938812256, 'val_avg_loss': 0.7174690853465687, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:04:26 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.892159938812256, 'val_avg_loss': 0.7174690853465687, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:04:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:04:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.551924, avg_loss=0.688798, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:29 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.551923751831055, 'test_avg_loss': 0.6887980937957764, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:29 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.551923751831055, 'test_avg_loss': 0.6887980937957764, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:29 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.551923751831055, 'test_avg_loss': 0.6887980937957764, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:29 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.551923751831055, 'test_avg_loss': 0.6887980937957764, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
(6, 6, 5, 30)
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:04:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:04:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:04:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.274863, avg_loss=0.661351, seen=11, correct=7, accuracy=0.636364
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:31 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.274862766265869, 'val_avg_loss': 0.6613511605696245, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-11 07:04:31 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.274862766265869, 'val_avg_loss': 0.6613511605696245, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-11 07:04:31 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.274862766265869, 'val_avg_loss': 0.6613511605696245, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-11 07:04:31 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.274862766265869, 'val_avg_loss': 0.6613511605696245, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.161415, avg_loss=0.654035, seen=40, correct=26, accuracy=0.650000
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1752MB
2025-09-11 07:04:33 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.161415100097656, 'test_avg_loss': 0.6540353775024415, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:04:33 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.161415100097656, 'test_avg_loss': 0.6540353775024415, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:04:33 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.161415100097656, 'test_avg_loss': 0.6540353775024415, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:04:33 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.161415100097656, 'test_avg_loss': 0.6540353775024415, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:04:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=9, total=36)
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=9, total=36)
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=9, total=36)
2025-09-11 07:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(18, 12, 2, 30)
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(18, 12, 2, 30)
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(18, 12, 2, 30)
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(18, 12, 2, 30)
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=9
2025-09-11 07:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-11 07:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=9
2025-09-11 07:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=9
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.552090, avg_loss=0.709780, seen=36, correct=17, accuracy=0.472222
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1785MB
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1785MB
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1785MB
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1785MB
2025-09-11 07:04:36 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 25.55208969116211, 'val_avg_loss': 0.7097802691989474, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-11 07:04:36 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 25.55208969116211, 'val_avg_loss': 0.7097802691989474, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-11 07:04:36 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 25.55208969116211, 'val_avg_loss': 0.7097802691989474, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-11 07:04:36 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 25.55208969116211, 'val_avg_loss': 0.7097802691989474, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.832895, avg_loss=0.670822, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:04:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1785MB
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1785MB
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1785MB
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1785MB
2025-09-11 07:04:39 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.832895278930664, 'test_avg_loss': 0.6708223819732666, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:04:39 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.832895278930664, 'test_avg_loss': 0.6708223819732666, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:04:39 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.832895278930664, 'test_avg_loss': 0.6708223819732666, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:04:39 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.832895278930664, 'test_avg_loss': 0.6708223819732666, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=9, total=36)
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=9, total=36)
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=9, total=36)
2025-09-11 07:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(18, 12, 2, 30)
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(18, 12, 2, 30)
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(18, 12, 2, 30)
(18, 12, 2, 30)
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=9
2025-09-11 07:04:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-11 07:04:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=9
2025-09-11 07:04:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=9
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.458946, avg_loss=0.707193, seen=36, correct=17, accuracy=0.472222
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:41 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 25.458946228027344, 'val_avg_loss': 0.7071929507785373, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-11 07:04:41 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 25.458946228027344, 'val_avg_loss': 0.7071929507785373, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-11 07:04:41 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 25.458946228027344, 'val_avg_loss': 0.7071929507785373, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-11 07:04:41 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 25.458946228027344, 'val_avg_loss': 0.7071929507785373, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.092045, avg_loss=0.677301, seen=40, correct=26, accuracy=0.650000
2025-09-11 07:04:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:43 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.092044830322266, 'test_avg_loss': 0.6773011207580566, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:04:43 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.092044830322266, 'test_avg_loss': 0.6773011207580566, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:04:43 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.092044830322266, 'test_avg_loss': 0.6773011207580566, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:04:43 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.092044830322266, 'test_avg_loss': 0.6773011207580566, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=9, total=36)
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=9, total=36)
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=9, total=36)
2025-09-11 07:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(18, 12, 2, 30)
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(18, 12, 2, 30)
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(18, 12, 2, 30)
(18, 12, 2, 30)
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=9
2025-09-11 07:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=9
2025-09-11 07:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-11 07:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=9
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.782085, avg_loss=0.716169, seen=36, correct=16, accuracy=0.444444
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:46 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 25.782085418701172, 'val_avg_loss': 0.7161690394083658, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-09-11 07:04:46 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 25.782085418701172, 'val_avg_loss': 0.7161690394083658, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-09-11 07:04:46 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 25.782085418701172, 'val_avg_loss': 0.7161690394083658, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-09-11 07:04:46 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 25.782085418701172, 'val_avg_loss': 0.7161690394083658, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.151600, avg_loss=0.703790, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1760MB
2025-09-11 07:04:49 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.151599884033203, 'test_avg_loss': 0.7037899971008301, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:04:49 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.151599884033203, 'test_avg_loss': 0.7037899971008301, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:04:49 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.151599884033203, 'test_avg_loss': 0.7037899971008301, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:04:49 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.151599884033203, 'test_avg_loss': 0.7037899971008301, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:04:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
(6, 6, 5, 30)
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:04:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:04:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:04:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.117360, avg_loss=0.737942, seen=11, correct=5, accuracy=0.454545
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:04:51 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.11736011505127, 'val_avg_loss': 0.7379418286410245, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-11 07:04:51 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.11736011505127, 'val_avg_loss': 0.7379418286410245, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-11 07:04:51 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.11736011505127, 'val_avg_loss': 0.7379418286410245, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-11 07:04:51 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.11736011505127, 'val_avg_loss': 0.7379418286410245, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.011219, avg_loss=0.650280, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:04:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:04:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:04:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:04:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:04:53 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.011219024658203, 'test_avg_loss': 0.6502804756164551, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:04:53 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.011219024658203, 'test_avg_loss': 0.6502804756164551, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:04:53 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.011219024658203, 'test_avg_loss': 0.6502804756164551, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:04:53 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.011219024658203, 'test_avg_loss': 0.6502804756164551, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)(6, 6, 5, 30)

2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:04:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:04:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:04:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.656233, avg_loss=0.786930, seen=11, correct=4, accuracy=0.363636
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:04:56 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 8.656232833862305, 'val_avg_loss': 0.7869302576238458, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:04:56 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 8.656232833862305, 'val_avg_loss': 0.7869302576238458, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:04:56 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 8.656232833862305, 'val_avg_loss': 0.7869302576238458, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:04:56 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 8.656232833862305, 'val_avg_loss': 0.7869302576238458, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:04:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:04:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:04:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:04:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:04:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:04:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:04:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:04:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.460640, avg_loss=0.686516, seen=40, correct=17, accuracy=0.425000
2025-09-11 07:04:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:04:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:04:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:04:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:04:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:04:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:04:59 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.46063995361328, 'test_avg_loss': 0.686515998840332, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:04:59 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.46063995361328, 'test_avg_loss': 0.686515998840332, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:04:59 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.46063995361328, 'test_avg_loss': 0.686515998840332, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:04:59 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.46063995361328, 'test_avg_loss': 0.686515998840332, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:05:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:05:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:05:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.799021, avg_loss=0.799911, seen=11, correct=4, accuracy=0.363636
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:05:01 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.799020767211914, 'val_avg_loss': 0.7999109788374468, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:05:01 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.799020767211914, 'val_avg_loss': 0.7999109788374468, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:05:01 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.799020767211914, 'val_avg_loss': 0.7999109788374468, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:05:01 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.799020767211914, 'val_avg_loss': 0.7999109788374468, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.092836, avg_loss=0.652321, seen=40, correct=27, accuracy=0.675000
2025-09-11 07:05:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:05:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:05:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:05:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1769MB
2025-09-11 07:05:03 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.092836380004883, 'test_avg_loss': 0.6523209095001221, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:05:03 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.092836380004883, 'test_avg_loss': 0.6523209095001221, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:05:03 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.092836380004883, 'test_avg_loss': 0.6523209095001221, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:05:03 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.092836380004883, 'test_avg_loss': 0.6523209095001221, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:05:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-11 07:05:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=14)
2025-09-11 07:05:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=4, total=14)
2025-09-11 07:05:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=3, total=14)
2025-09-11 07:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(7, 2, 5, 30)
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(7, 2, 5, 30)
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(7, 2, 5, 30)
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(7, 2, 5, 30)
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=4
2025-09-11 07:05:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-11 07:05:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:05:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=3
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.759947, avg_loss=0.697139, seen=14, correct=5, accuracy=0.357143
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-11 07:05:07 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 9.759946823120117, 'val_avg_loss': 0.697139058794294, 'val_seen': 14, 'val_correct': 5, 'val_acc': 0.35714285714285715}
2025-09-11 07:05:07 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 9.759946823120117, 'val_avg_loss': 0.697139058794294, 'val_seen': 14, 'val_correct': 5, 'val_acc': 0.35714285714285715}
2025-09-11 07:05:07 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 9.759946823120117, 'val_avg_loss': 0.697139058794294, 'val_seen': 14, 'val_correct': 5, 'val_acc': 0.35714285714285715}
2025-09-11 07:05:07 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 9.759946823120117, 'val_avg_loss': 0.697139058794294, 'val_seen': 14, 'val_correct': 5, 'val_acc': 0.35714285714285715}
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.592079, avg_loss=0.714802, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:05:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-11 07:05:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-11 07:05:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-11 07:05:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1802MB
2025-09-11 07:05:09 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.592079162597656, 'test_avg_loss': 0.7148019790649414, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:05:09 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.592079162597656, 'test_avg_loss': 0.7148019790649414, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:05:09 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.592079162597656, 'test_avg_loss': 0.7148019790649414, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:05:09 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.592079162597656, 'test_avg_loss': 0.7148019790649414, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=14)
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=4, total=14)
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=3, total=14)
2025-09-11 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(7, 2, 5, 30)
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(7, 2, 5, 30)
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(7, 2, 5, 30)
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(7, 2, 5, 30)
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=4
2025-09-11 07:05:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-11 07:05:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:05:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=3
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.343403, avg_loss=0.667386, seen=14, correct=8, accuracy=0.571429
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:11 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 9.343402862548828, 'val_avg_loss': 0.6673859187534877, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-11 07:05:11 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 9.343402862548828, 'val_avg_loss': 0.6673859187534877, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-11 07:05:11 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 9.343402862548828, 'val_avg_loss': 0.6673859187534877, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-11 07:05:11 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 9.343402862548828, 'val_avg_loss': 0.6673859187534877, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-11 07:05:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:05:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.382801, avg_loss=0.709570, seen=40, correct=18, accuracy=0.450000
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:14 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.382801055908203, 'test_avg_loss': 0.7095700263977051, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:05:14 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.382801055908203, 'test_avg_loss': 0.7095700263977051, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:05:14 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.382801055908203, 'test_avg_loss': 0.7095700263977051, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:05:14 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.382801055908203, 'test_avg_loss': 0.7095700263977051, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=14)
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=3, total=14)
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=4, total=14)
2025-09-11 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(7, 2, 5, 30)
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(7, 2, 5, 30)
(7, 2, 5, 30)
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(7, 2, 5, 30)
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=4
2025-09-11 07:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=3
2025-09-11 07:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.756433, avg_loss=0.696888, seen=14, correct=7, accuracy=0.500000
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:16 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.756433486938477, 'val_avg_loss': 0.6968881062098912, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}
2025-09-11 07:05:16 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.756433486938477, 'val_avg_loss': 0.6968881062098912, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}
2025-09-11 07:05:16 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.756433486938477, 'val_avg_loss': 0.6968881062098912, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}
2025-09-11 07:05:16 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.756433486938477, 'val_avg_loss': 0.6968881062098912, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.890764, avg_loss=0.747269, seen=40, correct=12, accuracy=0.300000
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1777MB
2025-09-11 07:05:18 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.890764236450195, 'test_avg_loss': 0.7472691059112548, 'test_seen': 40, 'test_correct': 12, 'test_acc': 0.3}
2025-09-11 07:05:18 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.890764236450195, 'test_avg_loss': 0.7472691059112548, 'test_seen': 40, 'test_correct': 12, 'test_acc': 0.3}
2025-09-11 07:05:18 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.890764236450195, 'test_avg_loss': 0.7472691059112548, 'test_seen': 40, 'test_correct': 12, 'test_acc': 0.3}
2025-09-11 07:05:18 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.890764236450195, 'test_avg_loss': 0.7472691059112548, 'test_seen': 40, 'test_correct': 12, 'test_acc': 0.3}
2025-09-11 07:05:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=134)
2025-09-11 07:05:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-11 07:05:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=134)
2025-09-11 07:05:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=134)
2025-09-11 07:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:05:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:05:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:05:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:05:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=94.051544, avg_loss=0.701877, seen=134, correct=69, accuracy=0.514925
2025-09-11 07:05:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:05:25 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 0 with val results: {'val_total': 134, 'val_loss': 94.05154418945312, 'val_avg_loss': 0.70187719544368, 'val_seen': 134, 'val_correct': 69, 'val_acc': 0.5149253731343284}
2025-09-11 07:05:25 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 0 with val results: {'val_total': 134, 'val_loss': 94.05154418945312, 'val_avg_loss': 0.70187719544368, 'val_seen': 134, 'val_correct': 69, 'val_acc': 0.5149253731343284}
2025-09-11 07:05:25 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 0 with val results: {'val_total': 134, 'val_loss': 94.05154418945312, 'val_avg_loss': 0.70187719544368, 'val_seen': 134, 'val_correct': 69, 'val_acc': 0.5149253731343284}
2025-09-11 07:05:25 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 0 with val results: {'val_total': 134, 'val_loss': 94.05154418945312, 'val_avg_loss': 0.70187719544368, 'val_seen': 134, 'val_correct': 69, 'val_acc': 0.5149253731343284}
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.829882, avg_loss=0.695747, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:05:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:05:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:05:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:05:27 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.82988166809082, 'test_avg_loss': 0.6957470417022705, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:05:27 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.82988166809082, 'test_avg_loss': 0.6957470417022705, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:05:27 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.82988166809082, 'test_avg_loss': 0.6957470417022705, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:05:27 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.82988166809082, 'test_avg_loss': 0.6957470417022705, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:05:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=134)
2025-09-11 07:05:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=134)
2025-09-11 07:05:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-11 07:05:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=134)
2025-09-11 07:05:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:05:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:05:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:05:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:05:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:05:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:05:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:05:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:05:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=92.297592, avg_loss=0.688788, seen=134, correct=74, accuracy=0.552239
2025-09-11 07:05:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:33 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 1 with val results: {'val_total': 134, 'val_loss': 92.29759216308594, 'val_avg_loss': 0.6887880012170592, 'val_seen': 134, 'val_correct': 74, 'val_acc': 0.5522388059701493}
2025-09-11 07:05:33 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 1 with val results: {'val_total': 134, 'val_loss': 92.29759216308594, 'val_avg_loss': 0.6887880012170592, 'val_seen': 134, 'val_correct': 74, 'val_acc': 0.5522388059701493}
2025-09-11 07:05:33 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 1 with val results: {'val_total': 134, 'val_loss': 92.29759216308594, 'val_avg_loss': 0.6887880012170592, 'val_seen': 134, 'val_correct': 74, 'val_acc': 0.5522388059701493}
2025-09-11 07:05:33 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 1 with val results: {'val_total': 134, 'val_loss': 92.29759216308594, 'val_avg_loss': 0.6887880012170592, 'val_seen': 134, 'val_correct': 74, 'val_acc': 0.5522388059701493}
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.886705, avg_loss=0.697168, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:05:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:35 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88670539855957, 'test_avg_loss': 0.6971676349639893, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:05:35 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88670539855957, 'test_avg_loss': 0.6971676349639893, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:05:35 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88670539855957, 'test_avg_loss': 0.6971676349639893, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:05:35 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88670539855957, 'test_avg_loss': 0.6971676349639893, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:05:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=134)
2025-09-11 07:05:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=134)
2025-09-11 07:05:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-11 07:05:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=134)
2025-09-11 07:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:05:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:05:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:05:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:05:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:05:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:05:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:05:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=91.502274, avg_loss=0.682853, seen=134, correct=76, accuracy=0.567164
2025-09-11 07:05:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:41 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 2 with val results: {'val_total': 134, 'val_loss': 91.50227355957031, 'val_avg_loss': 0.6828527877579874, 'val_seen': 134, 'val_correct': 76, 'val_acc': 0.5671641791044776}
2025-09-11 07:05:41 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 2 with val results: {'val_total': 134, 'val_loss': 91.50227355957031, 'val_avg_loss': 0.6828527877579874, 'val_seen': 134, 'val_correct': 76, 'val_acc': 0.5671641791044776}
2025-09-11 07:05:41 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 2 with val results: {'val_total': 134, 'val_loss': 91.50227355957031, 'val_avg_loss': 0.6828527877579874, 'val_seen': 134, 'val_correct': 76, 'val_acc': 0.5671641791044776}
2025-09-11 07:05:41 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 2 with val results: {'val_total': 134, 'val_loss': 91.50227355957031, 'val_avg_loss': 0.6828527877579874, 'val_seen': 134, 'val_correct': 76, 'val_acc': 0.5671641791044776}
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.204226, avg_loss=0.705106, seen=40, correct=17, accuracy=0.425000
2025-09-11 07:05:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1786MB
2025-09-11 07:05:44 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.204225540161133, 'test_avg_loss': 0.7051056385040283, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:05:44 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.204225540161133, 'test_avg_loss': 0.7051056385040283, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:05:44 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.204225540161133, 'test_avg_loss': 0.7051056385040283, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:05:44 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.204225540161133, 'test_avg_loss': 0.7051056385040283, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:05:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:05:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=14, total=57)
2025-09-11 07:05:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=14, total=57)
2025-09-11 07:05:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-11 07:05:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=14, total=57)
2025-09-11 07:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(29, 1, 2, 30)
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(29, 1, 2, 30)
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(29, 1, 2, 30)
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(29, 1, 2, 30)
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=14
2025-09-11 07:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=14
2025-09-11 07:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-11 07:05:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=14
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.387997, avg_loss=0.673474, seen=57, correct=30, accuracy=0.526316
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1819MB
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-11 07:05:47 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 0 with val results: {'val_total': 57, 'val_loss': 38.387996673583984, 'val_avg_loss': 0.6734736258523506, 'val_seen': 57, 'val_correct': 30, 'val_acc': 0.5263157894736842}
2025-09-11 07:05:47 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 0 with val results: {'val_total': 57, 'val_loss': 38.387996673583984, 'val_avg_loss': 0.6734736258523506, 'val_seen': 57, 'val_correct': 30, 'val_acc': 0.5263157894736842}
2025-09-11 07:05:47 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 0 with val results: {'val_total': 57, 'val_loss': 38.387996673583984, 'val_avg_loss': 0.6734736258523506, 'val_seen': 57, 'val_correct': 30, 'val_acc': 0.5263157894736842}
2025-09-11 07:05:47 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 0 with val results: {'val_total': 57, 'val_loss': 38.387996673583984, 'val_avg_loss': 0.6734736258523506, 'val_seen': 57, 'val_correct': 30, 'val_acc': 0.5263157894736842}
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:05:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.729073, avg_loss=0.643227, seen=40, correct=26, accuracy=0.650000
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-11 07:05:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-11 07:05:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-11 07:05:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1819MB
2025-09-11 07:05:50 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.72907257080078, 'test_avg_loss': 0.6432268142700195, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:05:50 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.72907257080078, 'test_avg_loss': 0.6432268142700195, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:05:50 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.72907257080078, 'test_avg_loss': 0.6432268142700195, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:05:50 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.72907257080078, 'test_avg_loss': 0.6432268142700195, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=14, total=57)
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=14, total=57)
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=14, total=57)
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-11 07:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(29, 1, 2, 30)
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(29, 1, 2, 30)
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(29, 1, 2, 30)
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(29, 1, 2, 30)
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=14
2025-09-11 07:05:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=14
2025-09-11 07:05:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-11 07:05:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=14
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.433891, avg_loss=0.674279, seen=57, correct=34, accuracy=0.596491
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:53 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 1 with val results: {'val_total': 57, 'val_loss': 38.43389129638672, 'val_avg_loss': 0.6742787946734512, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-11 07:05:53 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 1 with val results: {'val_total': 57, 'val_loss': 38.43389129638672, 'val_avg_loss': 0.6742787946734512, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-11 07:05:53 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 1 with val results: {'val_total': 57, 'val_loss': 38.43389129638672, 'val_avg_loss': 0.6742787946734512, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-11 07:05:53 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 1 with val results: {'val_total': 57, 'val_loss': 38.43389129638672, 'val_avg_loss': 0.6742787946734512, 'val_seen': 57, 'val_correct': 34, 'val_acc': 0.5964912280701754}
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:05:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

(20, 10, 2, 30)
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.256607, avg_loss=0.681415, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:56 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.256607055664062, 'test_avg_loss': 0.6814151763916015, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:05:56 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.256607055664062, 'test_avg_loss': 0.6814151763916015, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:05:56 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.256607055664062, 'test_avg_loss': 0.6814151763916015, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:05:56 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.256607055664062, 'test_avg_loss': 0.6814151763916015, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=14, total=57)
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=14, total=57)
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=14, total=57)
2025-09-11 07:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(29, 1, 2, 30)
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(29, 1, 2, 30)
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(29, 1, 2, 30)(29, 1, 2, 30)

2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=1, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:05:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=14
2025-09-11 07:05:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-11 07:05:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=14
2025-09-11 07:05:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=14
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.062008, avg_loss=0.685298, seen=57, correct=32, accuracy=0.561404
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:05:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:05:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:05:59 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 2 with val results: {'val_total': 57, 'val_loss': 39.062007904052734, 'val_avg_loss': 0.6852983842816269, 'val_seen': 57, 'val_correct': 32, 'val_acc': 0.5614035087719298}
2025-09-11 07:05:59 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 2 with val results: {'val_total': 57, 'val_loss': 39.062007904052734, 'val_avg_loss': 0.6852983842816269, 'val_seen': 57, 'val_correct': 32, 'val_acc': 0.5614035087719298}
2025-09-11 07:05:59 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 2 with val results: {'val_total': 57, 'val_loss': 39.062007904052734, 'val_avg_loss': 0.6852983842816269, 'val_seen': 57, 'val_correct': 32, 'val_acc': 0.5614035087719298}
2025-09-11 07:05:59 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 2 with val results: {'val_total': 57, 'val_loss': 39.062007904052734, 'val_avg_loss': 0.6852983842816269, 'val_seen': 57, 'val_correct': 32, 'val_acc': 0.5614035087719298}
2025-09-11 07:06:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:06:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:06:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:06:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:06:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:06:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.517223, avg_loss=0.687931, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:06:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:06:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:06:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:06:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1794MB
2025-09-11 07:06:02 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.517223358154297, 'test_avg_loss': 0.6879305839538574, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:06:02 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.517223358154297, 'test_avg_loss': 0.6879305839538574, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:06:02 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.517223358154297, 'test_avg_loss': 0.6879305839538574, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:06:02 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.517223358154297, 'test_avg_loss': 0.6879305839538574, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:06:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=17, total=69)
2025-09-11 07:06:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=17, total=69)
2025-09-11 07:06:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=17, total=69)
2025-09-11 07:06:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-11 07:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:06:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=17
2025-09-11 07:06:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=17
2025-09-11 07:06:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=17
2025-09-11 07:06:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-11 07:06:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.694626, avg_loss=0.691226, seen=69, correct=37, accuracy=0.536232
2025-09-11 07:06:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:06:06 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 0 with val results: {'val_total': 69, 'val_loss': 47.69462585449219, 'val_avg_loss': 0.691226461659307, 'val_seen': 69, 'val_correct': 37, 'val_acc': 0.5362318840579711}
2025-09-11 07:06:06 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 0 with val results: {'val_total': 69, 'val_loss': 47.69462585449219, 'val_avg_loss': 0.691226461659307, 'val_seen': 69, 'val_correct': 37, 'val_acc': 0.5362318840579711}
2025-09-11 07:06:06 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 0 with val results: {'val_total': 69, 'val_loss': 47.69462585449219, 'val_avg_loss': 0.691226461659307, 'val_seen': 69, 'val_correct': 37, 'val_acc': 0.5362318840579711}
2025-09-11 07:06:06 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 0 with val results: {'val_total': 69, 'val_loss': 47.69462585449219, 'val_avg_loss': 0.691226461659307, 'val_seen': 69, 'val_correct': 37, 'val_acc': 0.5362318840579711}
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:06:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:06:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.989630, avg_loss=0.724741, seen=40, correct=17, accuracy=0.425000
2025-09-11 07:06:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:06:09 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.9896297454834, 'test_avg_loss': 0.724740743637085, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:06:09 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.9896297454834, 'test_avg_loss': 0.724740743637085, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:06:09 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.9896297454834, 'test_avg_loss': 0.724740743637085, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:06:09 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.9896297454834, 'test_avg_loss': 0.724740743637085, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=17, total=69)
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=17, total=69)
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=17, total=69)
2025-09-11 07:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-11 07:06:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=17
2025-09-11 07:06:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=17
2025-09-11 07:06:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=17
2025-09-11 07:06:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=48.362087, avg_loss=0.700900, seen=69, correct=38, accuracy=0.550725
2025-09-11 07:06:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:13 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 1 with val results: {'val_total': 69, 'val_loss': 48.36208724975586, 'val_avg_loss': 0.700899815213853, 'val_seen': 69, 'val_correct': 38, 'val_acc': 0.5507246376811594}
2025-09-11 07:06:13 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 1 with val results: {'val_total': 69, 'val_loss': 48.36208724975586, 'val_avg_loss': 0.700899815213853, 'val_seen': 69, 'val_correct': 38, 'val_acc': 0.5507246376811594}
2025-09-11 07:06:13 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 1 with val results: {'val_total': 69, 'val_loss': 48.36208724975586, 'val_avg_loss': 0.700899815213853, 'val_seen': 69, 'val_correct': 38, 'val_acc': 0.5507246376811594}
2025-09-11 07:06:13 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 1 with val results: {'val_total': 69, 'val_loss': 48.36208724975586, 'val_avg_loss': 0.700899815213853, 'val_seen': 69, 'val_correct': 38, 'val_acc': 0.5507246376811594}
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:06:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:06:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:06:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:06:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.639877, avg_loss=0.690997, seen=40, correct=18, accuracy=0.450000
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:15 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.639877319335938, 'test_avg_loss': 0.6909969329833985, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:06:15 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.639877319335938, 'test_avg_loss': 0.6909969329833985, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:06:15 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.639877319335938, 'test_avg_loss': 0.6909969329833985, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:06:15 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.639877319335938, 'test_avg_loss': 0.6909969329833985, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:06:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=17, total=69)
2025-09-11 07:06:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=17, total=69)
2025-09-11 07:06:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=17, total=69)
2025-09-11 07:06:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-11 07:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:06:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:06:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-11 07:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=17
2025-09-11 07:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=17
2025-09-11 07:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=17
2025-09-11 07:06:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=48.560757, avg_loss=0.703779, seen=69, correct=31, accuracy=0.449275
2025-09-11 07:06:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:18 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 2 with val results: {'val_total': 69, 'val_loss': 48.56075668334961, 'val_avg_loss': 0.7037790823673856, 'val_seen': 69, 'val_correct': 31, 'val_acc': 0.4492753623188406}
2025-09-11 07:06:18 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 2 with val results: {'val_total': 69, 'val_loss': 48.56075668334961, 'val_avg_loss': 0.7037790823673856, 'val_seen': 69, 'val_correct': 31, 'val_acc': 0.4492753623188406}
2025-09-11 07:06:18 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 2 with val results: {'val_total': 69, 'val_loss': 48.56075668334961, 'val_avg_loss': 0.7037790823673856, 'val_seen': 69, 'val_correct': 31, 'val_acc': 0.4492753623188406}
2025-09-11 07:06:18 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 2 with val results: {'val_total': 69, 'val_loss': 48.56075668334961, 'val_avg_loss': 0.7037790823673856, 'val_seen': 69, 'val_correct': 31, 'val_acc': 0.4492753623188406}
2025-09-11 07:06:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:06:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:06:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:06:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:06:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:06:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.474701, avg_loss=0.711868, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1803MB
2025-09-11 07:06:21 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.474700927734375, 'test_avg_loss': 0.7118675231933593, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:06:21 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.474700927734375, 'test_avg_loss': 0.7118675231933593, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:06:21 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.474700927734375, 'test_avg_loss': 0.7118675231933593, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:06:21 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.474700927734375, 'test_avg_loss': 0.7118675231933593, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:06:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=47, total=188)
2025-09-11 07:06:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-11 07:06:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=47, total=188)
2025-09-11 07:06:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=47, total=188)
2025-09-11 07:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:06:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=47
2025-09-11 07:06:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-11 07:06:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=47
2025-09-11 07:06:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=47
2025-09-11 07:06:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=130.628265, avg_loss=0.694831, seen=188, correct=98, accuracy=0.521277
2025-09-11 07:06:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-11 07:06:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-11 07:06:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-11 07:06:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-11 07:06:28 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 0 with val results: {'val_total': 188, 'val_loss': 130.62826538085938, 'val_avg_loss': 0.6948311988343584, 'val_seen': 188, 'val_correct': 98, 'val_acc': 0.5212765957446809}
2025-09-11 07:06:28 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 0 with val results: {'val_total': 188, 'val_loss': 130.62826538085938, 'val_avg_loss': 0.6948311988343584, 'val_seen': 188, 'val_correct': 98, 'val_acc': 0.5212765957446809}
2025-09-11 07:06:28 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 0 with val results: {'val_total': 188, 'val_loss': 130.62826538085938, 'val_avg_loss': 0.6948311988343584, 'val_seen': 188, 'val_correct': 98, 'val_acc': 0.5212765957446809}
2025-09-11 07:06:28 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 0 with val results: {'val_total': 188, 'val_loss': 130.62826538085938, 'val_avg_loss': 0.6948311988343584, 'val_seen': 188, 'val_correct': 98, 'val_acc': 0.5212765957446809}
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:06:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:06:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:06:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.266848, avg_loss=0.681671, seen=40, correct=17, accuracy=0.425000
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1836MB
2025-09-11 07:06:31 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.266847610473633, 'test_avg_loss': 0.6816711902618409, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:06:31 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.266847610473633, 'test_avg_loss': 0.6816711902618409, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:06:31 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.266847610473633, 'test_avg_loss': 0.6816711902618409, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:06:31 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.266847610473633, 'test_avg_loss': 0.6816711902618409, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=47, total=188)
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=47, total=188)
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=47, total=188)
2025-09-11 07:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=47
2025-09-11 07:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=47
2025-09-11 07:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=47
2025-09-11 07:06:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-11 07:06:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=130.167709, avg_loss=0.692381, seen=188, correct=91, accuracy=0.484043
2025-09-11 07:06:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:38 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 1 with val results: {'val_total': 188, 'val_loss': 130.16770935058594, 'val_avg_loss': 0.6923814327158826, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937}
2025-09-11 07:06:38 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 1 with val results: {'val_total': 188, 'val_loss': 130.16770935058594, 'val_avg_loss': 0.6923814327158826, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937}
2025-09-11 07:06:38 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 1 with val results: {'val_total': 188, 'val_loss': 130.16770935058594, 'val_avg_loss': 0.6923814327158826, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937}
2025-09-11 07:06:38 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 1 with val results: {'val_total': 188, 'val_loss': 130.16770935058594, 'val_avg_loss': 0.6923814327158826, 'val_seen': 188, 'val_correct': 91, 'val_acc': 0.48404255319148937}
2025-09-11 07:06:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:06:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:06:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:06:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:06:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:06:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:06:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:06:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:06:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.233082, avg_loss=0.705827, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:06:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:41 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.233081817626953, 'test_avg_loss': 0.7058270454406739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:06:41 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.233081817626953, 'test_avg_loss': 0.7058270454406739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:06:41 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.233081817626953, 'test_avg_loss': 0.7058270454406739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:06:41 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.233081817626953, 'test_avg_loss': 0.7058270454406739, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:06:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=47, total=188)
2025-09-11 07:06:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-11 07:06:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=47, total=188)
2025-09-11 07:06:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=47, total=188)
2025-09-11 07:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:06:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=47
2025-09-11 07:06:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=47
2025-09-11 07:06:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-11 07:06:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=47
2025-09-11 07:06:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=132.746155, avg_loss=0.706097, seen=188, correct=93, accuracy=0.494681
2025-09-11 07:06:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:48 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 2 with val results: {'val_total': 188, 'val_loss': 132.74615478515625, 'val_avg_loss': 0.7060965680061503, 'val_seen': 188, 'val_correct': 93, 'val_acc': 0.4946808510638298}
2025-09-11 07:06:48 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 2 with val results: {'val_total': 188, 'val_loss': 132.74615478515625, 'val_avg_loss': 0.7060965680061503, 'val_seen': 188, 'val_correct': 93, 'val_acc': 0.4946808510638298}
2025-09-11 07:06:48 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 2 with val results: {'val_total': 188, 'val_loss': 132.74615478515625, 'val_avg_loss': 0.7060965680061503, 'val_seen': 188, 'val_correct': 93, 'val_acc': 0.4946808510638298}
2025-09-11 07:06:48 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 2 with val results: {'val_total': 188, 'val_loss': 132.74615478515625, 'val_avg_loss': 0.7060965680061503, 'val_seen': 188, 'val_correct': 93, 'val_acc': 0.4946808510638298}
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:06:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:06:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:06:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:06:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:06:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:06:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:06:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:06:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.186260, avg_loss=0.679657, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:06:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1811MB
2025-09-11 07:06:51 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.186260223388672, 'test_avg_loss': 0.6796565055847168, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:06:51 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.186260223388672, 'test_avg_loss': 0.6796565055847168, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:06:51 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.186260223388672, 'test_avg_loss': 0.6796565055847168, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:06:51 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.186260223388672, 'test_avg_loss': 0.6796565055847168, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:06:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:06:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-11 07:06:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=16, total=63)
2025-09-11 07:06:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=16, total=63)
2025-09-11 07:06:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=15, total=63)
2025-09-11 07:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:06:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=16
2025-09-11 07:06:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-11 07:06:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=16
2025-09-11 07:06:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=15
2025-09-11 07:06:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=44.332798, avg_loss=0.703695, seen=63, correct=31, accuracy=0.492063
2025-09-11 07:06:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:06:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:06:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:06:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:06:54 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 0 with val results: {'val_total': 63, 'val_loss': 44.33279800415039, 'val_avg_loss': 0.7036952064150855, 'val_seen': 63, 'val_correct': 31, 'val_acc': 0.49206349206349204}
2025-09-11 07:06:54 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 0 with val results: {'val_total': 63, 'val_loss': 44.33279800415039, 'val_avg_loss': 0.7036952064150855, 'val_seen': 63, 'val_correct': 31, 'val_acc': 0.49206349206349204}
2025-09-11 07:06:54 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 0 with val results: {'val_total': 63, 'val_loss': 44.33279800415039, 'val_avg_loss': 0.7036952064150855, 'val_seen': 63, 'val_correct': 31, 'val_acc': 0.49206349206349204}
2025-09-11 07:06:54 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 0 with val results: {'val_total': 63, 'val_loss': 44.33279800415039, 'val_avg_loss': 0.7036952064150855, 'val_seen': 63, 'val_correct': 31, 'val_acc': 0.49206349206349204}
2025-09-11 07:06:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:06:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:06:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:06:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:06:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:06:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:06:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:06:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:06:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:06:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.621464, avg_loss=0.665537, seen=40, correct=28, accuracy=0.700000
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:06:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:06:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:06:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:06:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:06:57 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.621463775634766, 'test_avg_loss': 0.6655365943908691, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-11 07:06:57 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.621463775634766, 'test_avg_loss': 0.6655365943908691, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-11 07:06:57 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.621463775634766, 'test_avg_loss': 0.6655365943908691, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-11 07:06:57 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.621463775634766, 'test_avg_loss': 0.6655365943908691, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-11 07:06:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=16, total=63)
2025-09-11 07:06:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=15, total=63)
2025-09-11 07:06:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-11 07:06:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=16, total=63)
2025-09-11 07:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:06:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:06:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:06:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:06:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=16
2025-09-11 07:06:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-11 07:06:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=16
2025-09-11 07:06:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=15
2025-09-11 07:06:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=45.921001, avg_loss=0.728905, seen=63, correct=30, accuracy=0.476190
2025-09-11 07:06:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:01 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 1 with val results: {'val_total': 63, 'val_loss': 45.92100143432617, 'val_avg_loss': 0.728904784671844, 'val_seen': 63, 'val_correct': 30, 'val_acc': 0.47619047619047616}
2025-09-11 07:07:01 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 1 with val results: {'val_total': 63, 'val_loss': 45.92100143432617, 'val_avg_loss': 0.728904784671844, 'val_seen': 63, 'val_correct': 30, 'val_acc': 0.47619047619047616}
2025-09-11 07:07:01 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 1 with val results: {'val_total': 63, 'val_loss': 45.92100143432617, 'val_avg_loss': 0.728904784671844, 'val_seen': 63, 'val_correct': 30, 'val_acc': 0.47619047619047616}
2025-09-11 07:07:01 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 1 with val results: {'val_total': 63, 'val_loss': 45.92100143432617, 'val_avg_loss': 0.728904784671844, 'val_seen': 63, 'val_correct': 30, 'val_acc': 0.47619047619047616}
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.101509, avg_loss=0.727538, seen=40, correct=18, accuracy=0.450000
2025-09-11 07:07:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:03 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.10150909423828, 'test_avg_loss': 0.727537727355957, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:07:03 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.10150909423828, 'test_avg_loss': 0.727537727355957, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:07:03 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.10150909423828, 'test_avg_loss': 0.727537727355957, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:07:03 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.10150909423828, 'test_avg_loss': 0.727537727355957, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:07:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-11 07:07:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=16, total=63)
2025-09-11 07:07:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=16, total=63)
2025-09-11 07:07:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=15, total=63)
2025-09-11 07:07:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:07:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)2025-09-11 07:07:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2

(30, 30, 1, 30)
2025-09-11 07:07:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=16
2025-09-11 07:07:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-11 07:07:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=15
2025-09-11 07:07:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=16
2025-09-11 07:07:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.577454, avg_loss=0.691706, seen=63, correct=31, accuracy=0.492063
2025-09-11 07:07:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:07 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 2 with val results: {'val_total': 63, 'val_loss': 43.57745361328125, 'val_avg_loss': 0.6917056129092262, 'val_seen': 63, 'val_correct': 31, 'val_acc': 0.49206349206349204}
2025-09-11 07:07:07 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 2 with val results: {'val_total': 63, 'val_loss': 43.57745361328125, 'val_avg_loss': 0.6917056129092262, 'val_seen': 63, 'val_correct': 31, 'val_acc': 0.49206349206349204}
2025-09-11 07:07:07 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 2 with val results: {'val_total': 63, 'val_loss': 43.57745361328125, 'val_avg_loss': 0.6917056129092262, 'val_seen': 63, 'val_correct': 31, 'val_acc': 0.49206349206349204}
2025-09-11 07:07:07 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 2 with val results: {'val_total': 63, 'val_loss': 43.57745361328125, 'val_avg_loss': 0.6917056129092262, 'val_seen': 63, 'val_correct': 31, 'val_acc': 0.49206349206349204}
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

(20, 10, 2, 30)
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.467064, avg_loss=0.711677, seen=40, correct=17, accuracy=0.425000
2025-09-11 07:07:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1820MB
2025-09-11 07:07:09 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.467063903808594, 'test_avg_loss': 0.7116765975952148, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:07:09 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.467063903808594, 'test_avg_loss': 0.7116765975952148, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:07:09 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.467063903808594, 'test_avg_loss': 0.7116765975952148, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:07:09 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.467063903808594, 'test_avg_loss': 0.7116765975952148, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:07:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-11 07:07:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=8, total=32)
2025-09-11 07:07:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=8, total=32)
2025-09-11 07:07:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=8, total=32)
2025-09-11 07:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(16, 14, 2, 30)
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=8
2025-09-11 07:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=8
2025-09-11 07:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-11 07:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=8
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=21.112858, avg_loss=0.659777, seen=32, correct=21, accuracy=0.656250
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:07:13 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 0 with val results: {'val_total': 32, 'val_loss': 21.112857818603516, 'val_avg_loss': 0.6597768068313599, 'val_seen': 32, 'val_correct': 21, 'val_acc': 0.65625}
2025-09-11 07:07:13 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 0 with val results: {'val_total': 32, 'val_loss': 21.112857818603516, 'val_avg_loss': 0.6597768068313599, 'val_seen': 32, 'val_correct': 21, 'val_acc': 0.65625}
2025-09-11 07:07:13 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 0 with val results: {'val_total': 32, 'val_loss': 21.112857818603516, 'val_avg_loss': 0.6597768068313599, 'val_seen': 32, 'val_correct': 21, 'val_acc': 0.65625}
2025-09-11 07:07:13 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 0 with val results: {'val_total': 32, 'val_loss': 21.112857818603516, 'val_avg_loss': 0.6597768068313599, 'val_seen': 32, 'val_correct': 21, 'val_acc': 0.65625}
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.978418, avg_loss=0.674460, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:07:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:07:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:07:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:07:16 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.978418350219727, 'test_avg_loss': 0.6744604587554932, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:16 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.978418350219727, 'test_avg_loss': 0.6744604587554932, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:16 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.978418350219727, 'test_avg_loss': 0.6744604587554932, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:16 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.978418350219727, 'test_avg_loss': 0.6744604587554932, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-11 07:07:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=8, total=32)
2025-09-11 07:07:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=8, total=32)
2025-09-11 07:07:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=8, total=32)
2025-09-11 07:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(16, 14, 2, 30)
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=8
2025-09-11 07:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=8
2025-09-11 07:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=8
2025-09-11 07:07:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=22.520817, avg_loss=0.703776, seen=32, correct=15, accuracy=0.468750
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:18 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 1 with val results: {'val_total': 32, 'val_loss': 22.520816802978516, 'val_avg_loss': 0.7037755250930786, 'val_seen': 32, 'val_correct': 15, 'val_acc': 0.46875}
2025-09-11 07:07:18 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 1 with val results: {'val_total': 32, 'val_loss': 22.520816802978516, 'val_avg_loss': 0.7037755250930786, 'val_seen': 32, 'val_correct': 15, 'val_acc': 0.46875}
2025-09-11 07:07:18 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 1 with val results: {'val_total': 32, 'val_loss': 22.520816802978516, 'val_avg_loss': 0.7037755250930786, 'val_seen': 32, 'val_correct': 15, 'val_acc': 0.46875}
2025-09-11 07:07:18 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 1 with val results: {'val_total': 32, 'val_loss': 22.520816802978516, 'val_avg_loss': 0.7037755250930786, 'val_seen': 32, 'val_correct': 15, 'val_acc': 0.46875}
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.098984, avg_loss=0.677475, seen=40, correct=26, accuracy=0.650000
2025-09-11 07:07:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:20 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.098983764648438, 'test_avg_loss': 0.6774745941162109, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:07:20 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.098983764648438, 'test_avg_loss': 0.6774745941162109, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:07:20 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.098983764648438, 'test_avg_loss': 0.6774745941162109, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:07:20 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.098983764648438, 'test_avg_loss': 0.6774745941162109, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=8, total=32)
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=8, total=32)
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=8, total=32)
2025-09-11 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(16, 14, 2, 30)
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(16, 14, 2, 30)
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=14, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=8
2025-09-11 07:07:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=8
2025-09-11 07:07:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=8
2025-09-11 07:07:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=22.128685, avg_loss=0.691521, seen=32, correct=16, accuracy=0.500000
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:23 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 2 with val results: {'val_total': 32, 'val_loss': 22.128684997558594, 'val_avg_loss': 0.691521406173706, 'val_seen': 32, 'val_correct': 16, 'val_acc': 0.5}
2025-09-11 07:07:23 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 2 with val results: {'val_total': 32, 'val_loss': 22.128684997558594, 'val_avg_loss': 0.691521406173706, 'val_seen': 32, 'val_correct': 16, 'val_acc': 0.5}
2025-09-11 07:07:23 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 2 with val results: {'val_total': 32, 'val_loss': 22.128684997558594, 'val_avg_loss': 0.691521406173706, 'val_seen': 32, 'val_correct': 16, 'val_acc': 0.5}
2025-09-11 07:07:23 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 2 with val results: {'val_total': 32, 'val_loss': 22.128684997558594, 'val_avg_loss': 0.691521406173706, 'val_seen': 32, 'val_correct': 16, 'val_acc': 0.5}
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.080206, avg_loss=0.652005, seen=40, correct=26, accuracy=0.650000
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1828MB
2025-09-11 07:07:26 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.0802059173584, 'test_avg_loss': 0.6520051479339599, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:07:26 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.0802059173584, 'test_avg_loss': 0.6520051479339599, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:07:26 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.0802059173584, 'test_avg_loss': 0.6520051479339599, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:07:26 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.0802059173584, 'test_avg_loss': 0.6520051479339599, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:07:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-11 07:07:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=137)
2025-09-11 07:07:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=137)
2025-09-11 07:07:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=34, total=137)
2025-09-11 07:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:07:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:07:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=34
2025-09-11 07:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-11 07:07:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=94.342636, avg_loss=0.688632, seen=137, correct=78, accuracy=0.569343
2025-09-11 07:07:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:07:32 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 0 with val results: {'val_total': 137, 'val_loss': 94.34263610839844, 'val_avg_loss': 0.6886323803532732, 'val_seen': 137, 'val_correct': 78, 'val_acc': 0.5693430656934306}
2025-09-11 07:07:32 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 0 with val results: {'val_total': 137, 'val_loss': 94.34263610839844, 'val_avg_loss': 0.6886323803532732, 'val_seen': 137, 'val_correct': 78, 'val_acc': 0.5693430656934306}
2025-09-11 07:07:32 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 0 with val results: {'val_total': 137, 'val_loss': 94.34263610839844, 'val_avg_loss': 0.6886323803532732, 'val_seen': 137, 'val_correct': 78, 'val_acc': 0.5693430656934306}
2025-09-11 07:07:32 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 0 with val results: {'val_total': 137, 'val_loss': 94.34263610839844, 'val_avg_loss': 0.6886323803532732, 'val_seen': 137, 'val_correct': 78, 'val_acc': 0.5693430656934306}
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.880260, avg_loss=0.647007, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:07:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:07:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:07:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:07:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:07:34 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.880260467529297, 'test_avg_loss': 0.6470065116882324, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:34 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.880260467529297, 'test_avg_loss': 0.6470065116882324, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:34 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.880260467529297, 'test_avg_loss': 0.6470065116882324, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:34 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.880260467529297, 'test_avg_loss': 0.6470065116882324, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-11 07:07:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=137)
2025-09-11 07:07:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=137)
2025-09-11 07:07:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=34, total=137)
2025-09-11 07:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:07:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:07:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=34
2025-09-11 07:07:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-11 07:07:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:07:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:07:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=95.932800, avg_loss=0.700239, seen=137, correct=73, accuracy=0.532847
2025-09-11 07:07:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:39 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 1 with val results: {'val_total': 137, 'val_loss': 95.93280029296875, 'val_avg_loss': 0.7002394181968522, 'val_seen': 137, 'val_correct': 73, 'val_acc': 0.5328467153284672}
2025-09-11 07:07:39 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 1 with val results: {'val_total': 137, 'val_loss': 95.93280029296875, 'val_avg_loss': 0.7002394181968522, 'val_seen': 137, 'val_correct': 73, 'val_acc': 0.5328467153284672}
2025-09-11 07:07:39 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 1 with val results: {'val_total': 137, 'val_loss': 95.93280029296875, 'val_avg_loss': 0.7002394181968522, 'val_seen': 137, 'val_correct': 73, 'val_acc': 0.5328467153284672}
2025-09-11 07:07:39 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 1 with val results: {'val_total': 137, 'val_loss': 95.93280029296875, 'val_avg_loss': 0.7002394181968522, 'val_seen': 137, 'val_correct': 73, 'val_acc': 0.5328467153284672}
2025-09-11 07:07:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.562853, avg_loss=0.664071, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:42 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.56285285949707, 'test_avg_loss': 0.6640713214874268, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:42 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.56285285949707, 'test_avg_loss': 0.6640713214874268, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:42 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.56285285949707, 'test_avg_loss': 0.6640713214874268, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:42 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.56285285949707, 'test_avg_loss': 0.6640713214874268, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=137)
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=137)
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=34, total=137)
2025-09-11 07:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:07:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:07:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-11 07:07:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=34
2025-09-11 07:07:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=97.559921, avg_loss=0.712116, seen=137, correct=65, accuracy=0.474453
2025-09-11 07:07:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1837MB
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:47 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 2 with val results: {'val_total': 137, 'val_loss': 97.55992126464844, 'val_avg_loss': 0.7121162136105725, 'val_seen': 137, 'val_correct': 65, 'val_acc': 0.4744525547445255}
2025-09-11 07:07:47 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 2 with val results: {'val_total': 137, 'val_loss': 97.55992126464844, 'val_avg_loss': 0.7121162136105725, 'val_seen': 137, 'val_correct': 65, 'val_acc': 0.4744525547445255}
2025-09-11 07:07:47 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 2 with val results: {'val_total': 137, 'val_loss': 97.55992126464844, 'val_avg_loss': 0.7121162136105725, 'val_seen': 137, 'val_correct': 65, 'val_acc': 0.4744525547445255}
2025-09-11 07:07:47 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 2 with val results: {'val_total': 137, 'val_loss': 97.55992126464844, 'val_avg_loss': 0.7121162136105725, 'val_seen': 137, 'val_correct': 65, 'val_acc': 0.4744525547445255}
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.680046, avg_loss=0.667001, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1837MB
2025-09-11 07:07:50 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.68004608154297, 'test_avg_loss': 0.6670011520385742, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:07:50 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.68004608154297, 'test_avg_loss': 0.6670011520385742, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:07:50 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.68004608154297, 'test_avg_loss': 0.6670011520385742, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:07:50 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.68004608154297, 'test_avg_loss': 0.6670011520385742, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:07:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:07:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-11 07:07:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=18, total=72)
2025-09-11 07:07:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=18, total=72)
2025-09-11 07:07:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=72)
2025-09-11 07:07:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:07:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=18
2025-09-11 07:07:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-11 07:07:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=18
2025-09-11 07:07:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:07:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=49.141499, avg_loss=0.682521, seen=72, correct=38, accuracy=0.527778
2025-09-11 07:07:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:07:53 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 0 with val results: {'val_total': 72, 'val_loss': 49.14149856567383, 'val_avg_loss': 0.6825208134121366, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:07:53 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 0 with val results: {'val_total': 72, 'val_loss': 49.14149856567383, 'val_avg_loss': 0.6825208134121366, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:07:53 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 0 with val results: {'val_total': 72, 'val_loss': 49.14149856567383, 'val_avg_loss': 0.6825208134121366, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:07:53 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 0 with val results: {'val_total': 72, 'val_loss': 49.14149856567383, 'val_avg_loss': 0.6825208134121366, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:07:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:07:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:07:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.564335, avg_loss=0.689108, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:07:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:07:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:07:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:07:56 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.564334869384766, 'test_avg_loss': 0.6891083717346191, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:07:56 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.564334869384766, 'test_avg_loss': 0.6891083717346191, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:07:56 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.564334869384766, 'test_avg_loss': 0.6891083717346191, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:07:56 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.564334869384766, 'test_avg_loss': 0.6891083717346191, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:07:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-11 07:07:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=18, total=72)
2025-09-11 07:07:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=18, total=72)
2025-09-11 07:07:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=72)
2025-09-11 07:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:07:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:07:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:07:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=18
2025-09-11 07:07:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-11 07:07:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:07:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=18
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=50.562202, avg_loss=0.702253, seen=72, correct=38, accuracy=0.527778
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:07:59 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 1 with val results: {'val_total': 72, 'val_loss': 50.56220245361328, 'val_avg_loss': 0.7022528118557401, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:07:59 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 1 with val results: {'val_total': 72, 'val_loss': 50.56220245361328, 'val_avg_loss': 0.7022528118557401, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:07:59 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 1 with val results: {'val_total': 72, 'val_loss': 50.56220245361328, 'val_avg_loss': 0.7022528118557401, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:07:59 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 1 with val results: {'val_total': 72, 'val_loss': 50.56220245361328, 'val_avg_loss': 0.7022528118557401, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:07:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:08:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:08:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:08:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.688286, avg_loss=0.692207, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:01 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.68828582763672, 'test_avg_loss': 0.692207145690918, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:08:01 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.68828582763672, 'test_avg_loss': 0.692207145690918, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:08:01 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.68828582763672, 'test_avg_loss': 0.692207145690918, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:08:01 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.68828582763672, 'test_avg_loss': 0.692207145690918, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:08:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-11 07:08:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=18, total=72)
2025-09-11 07:08:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=18, total=72)
2025-09-11 07:08:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=72)
2025-09-11 07:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:08:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:08:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=18
2025-09-11 07:08:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-11 07:08:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=18
2025-09-11 07:08:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:08:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=51.109276, avg_loss=0.709851, seen=72, correct=38, accuracy=0.527778
2025-09-11 07:08:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:04 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 2 with val results: {'val_total': 72, 'val_loss': 51.109275817871094, 'val_avg_loss': 0.7098510530259874, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:08:04 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 2 with val results: {'val_total': 72, 'val_loss': 51.109275817871094, 'val_avg_loss': 0.7098510530259874, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:08:04 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 2 with val results: {'val_total': 72, 'val_loss': 51.109275817871094, 'val_avg_loss': 0.7098510530259874, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:08:04 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 2 with val results: {'val_total': 72, 'val_loss': 51.109275817871094, 'val_avg_loss': 0.7098510530259874, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:08:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:08:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:08:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:08:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:08:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:08:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:08:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:08:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.645151, avg_loss=0.666129, seen=40, correct=24, accuracy=0.600000
2025-09-11 07:08:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1845MB
2025-09-11 07:08:07 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.645151138305664, 'test_avg_loss': 0.6661287784576416, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:08:07 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.645151138305664, 'test_avg_loss': 0.6661287784576416, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:08:07 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.645151138305664, 'test_avg_loss': 0.6661287784576416, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:08:07 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.645151138305664, 'test_avg_loss': 0.6661287784576416, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:08:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:08:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:08:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:08:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:08:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-11 07:08:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=40, total=160)
2025-09-11 07:08:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=40, total=160)
2025-09-11 07:08:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=40, total=160)
2025-09-11 07:08:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:08:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=40
2025-09-11 07:08:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=40
2025-09-11 07:08:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=40
2025-09-11 07:08:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-11 07:08:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=108.090591, avg_loss=0.675566, seen=160, correct=86, accuracy=0.537500
2025-09-11 07:08:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:08:13 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 0 with val results: {'val_total': 160, 'val_loss': 108.09059143066406, 'val_avg_loss': 0.6755661964416504, 'val_seen': 160, 'val_correct': 86, 'val_acc': 0.5375}
2025-09-11 07:08:13 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 0 with val results: {'val_total': 160, 'val_loss': 108.09059143066406, 'val_avg_loss': 0.6755661964416504, 'val_seen': 160, 'val_correct': 86, 'val_acc': 0.5375}
2025-09-11 07:08:13 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 0 with val results: {'val_total': 160, 'val_loss': 108.09059143066406, 'val_avg_loss': 0.6755661964416504, 'val_seen': 160, 'val_correct': 86, 'val_acc': 0.5375}
2025-09-11 07:08:13 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 0 with val results: {'val_total': 160, 'val_loss': 108.09059143066406, 'val_avg_loss': 0.6755661964416504, 'val_seen': 160, 'val_correct': 86, 'val_acc': 0.5375}
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:08:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:08:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:08:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:08:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:08:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.836857, avg_loss=0.695921, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:08:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:08:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:08:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:08:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:08:15 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.836856842041016, 'test_avg_loss': 0.6959214210510254, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:08:15 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.836856842041016, 'test_avg_loss': 0.6959214210510254, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:08:15 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.836856842041016, 'test_avg_loss': 0.6959214210510254, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:08:15 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.836856842041016, 'test_avg_loss': 0.6959214210510254, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:08:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=40, total=160)
2025-09-11 07:08:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-11 07:08:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=40, total=160)
2025-09-11 07:08:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=40, total=160)
2025-09-11 07:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:08:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)(30, 30, 1, 30)

(30, 30, 1, 30)
2025-09-11 07:08:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-11 07:08:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=40
2025-09-11 07:08:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=40
2025-09-11 07:08:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=40
2025-09-11 07:08:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=112.725327, avg_loss=0.704533, seen=160, correct=77, accuracy=0.481250
2025-09-11 07:08:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:22 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 1 with val results: {'val_total': 160, 'val_loss': 112.72532653808594, 'val_avg_loss': 0.7045332908630371, 'val_seen': 160, 'val_correct': 77, 'val_acc': 0.48125}
2025-09-11 07:08:22 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 1 with val results: {'val_total': 160, 'val_loss': 112.72532653808594, 'val_avg_loss': 0.7045332908630371, 'val_seen': 160, 'val_correct': 77, 'val_acc': 0.48125}
2025-09-11 07:08:22 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 1 with val results: {'val_total': 160, 'val_loss': 112.72532653808594, 'val_avg_loss': 0.7045332908630371, 'val_seen': 160, 'val_correct': 77, 'val_acc': 0.48125}
2025-09-11 07:08:22 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 1 with val results: {'val_total': 160, 'val_loss': 112.72532653808594, 'val_avg_loss': 0.7045332908630371, 'val_seen': 160, 'val_correct': 77, 'val_acc': 0.48125}
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:08:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:08:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:08:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:08:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.475174, avg_loss=0.661879, seen=40, correct=24, accuracy=0.600000
2025-09-11 07:08:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:24 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.475173950195312, 'test_avg_loss': 0.6618793487548829, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:08:24 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.475173950195312, 'test_avg_loss': 0.6618793487548829, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:08:24 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.475173950195312, 'test_avg_loss': 0.6618793487548829, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:08:24 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.475173950195312, 'test_avg_loss': 0.6618793487548829, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:08:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-11 07:08:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=40, total=160)
2025-09-11 07:08:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=40, total=160)
2025-09-11 07:08:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=40, total=160)
2025-09-11 07:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:08:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-11 07:08:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=40
2025-09-11 07:08:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=40
2025-09-11 07:08:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=40
2025-09-11 07:08:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=114.546959, avg_loss=0.715918, seen=160, correct=77, accuracy=0.481250
2025-09-11 07:08:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:30 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 2 with val results: {'val_total': 160, 'val_loss': 114.54695892333984, 'val_avg_loss': 0.715918493270874, 'val_seen': 160, 'val_correct': 77, 'val_acc': 0.48125}
2025-09-11 07:08:30 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 2 with val results: {'val_total': 160, 'val_loss': 114.54695892333984, 'val_avg_loss': 0.715918493270874, 'val_seen': 160, 'val_correct': 77, 'val_acc': 0.48125}
2025-09-11 07:08:30 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 2 with val results: {'val_total': 160, 'val_loss': 114.54695892333984, 'val_avg_loss': 0.715918493270874, 'val_seen': 160, 'val_correct': 77, 'val_acc': 0.48125}
2025-09-11 07:08:30 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 2 with val results: {'val_total': 160, 'val_loss': 114.54695892333984, 'val_avg_loss': 0.715918493270874, 'val_seen': 160, 'val_correct': 77, 'val_acc': 0.48125}
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:08:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:08:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:08:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.630405, avg_loss=0.665760, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1853MB
2025-09-11 07:08:33 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.63040542602539, 'test_avg_loss': 0.6657601356506347, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:08:33 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.63040542602539, 'test_avg_loss': 0.6657601356506347, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:08:33 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.63040542602539, 'test_avg_loss': 0.6657601356506347, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:08:33 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.63040542602539, 'test_avg_loss': 0.6657601356506347, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:08:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:08:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:08:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:08:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:08:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:08:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:08:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:08:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:08:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:08:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:08:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.441330, avg_loss=0.682207, seen=200, correct=113, accuracy=0.565000
2025-09-11 07:08:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-11 07:08:40 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 0 with val results: {'val_total': 200, 'val_loss': 136.4413299560547, 'val_avg_loss': 0.6822066497802735, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:08:40 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 0 with val results: {'val_total': 200, 'val_loss': 136.4413299560547, 'val_avg_loss': 0.6822066497802735, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:08:40 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 0 with val results: {'val_total': 200, 'val_loss': 136.4413299560547, 'val_avg_loss': 0.6822066497802735, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:08:40 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 0 with val results: {'val_total': 200, 'val_loss': 136.4413299560547, 'val_avg_loss': 0.6822066497802735, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:08:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:08:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:08:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:08:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:08:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.887466, avg_loss=0.672187, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:08:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-11 07:08:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-11 07:08:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-11 07:08:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1887MB
2025-09-11 07:08:42 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.887466430664062, 'test_avg_loss': 0.6721866607666016, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:08:42 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.887466430664062, 'test_avg_loss': 0.6721866607666016, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:08:42 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.887466430664062, 'test_avg_loss': 0.6721866607666016, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:08:42 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.887466430664062, 'test_avg_loss': 0.6721866607666016, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:08:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:08:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:08:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:08:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:08:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:08:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:08:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:08:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:08:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:08:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.429779, avg_loss=0.677149, seen=200, correct=112, accuracy=0.560000
2025-09-11 07:08:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:08:50 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 1 with val results: {'val_total': 200, 'val_loss': 135.42977905273438, 'val_avg_loss': 0.6771488952636718, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-11 07:08:50 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 1 with val results: {'val_total': 200, 'val_loss': 135.42977905273438, 'val_avg_loss': 0.6771488952636718, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-11 07:08:50 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 1 with val results: {'val_total': 200, 'val_loss': 135.42977905273438, 'val_avg_loss': 0.6771488952636718, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-11 07:08:50 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 1 with val results: {'val_total': 200, 'val_loss': 135.42977905273438, 'val_avg_loss': 0.6771488952636718, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:08:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:08:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:08:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:08:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.570000, avg_loss=0.664250, seen=40, correct=27, accuracy=0.675000
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:08:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:08:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:08:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:08:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:08:53 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.56999969482422, 'test_avg_loss': 0.6642499923706054, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:08:53 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.56999969482422, 'test_avg_loss': 0.6642499923706054, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:08:53 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.56999969482422, 'test_avg_loss': 0.6642499923706054, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:08:53 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.56999969482422, 'test_avg_loss': 0.6642499923706054, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:08:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:08:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:08:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:08:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:08:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:08:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:08:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:08:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:08:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:08:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:08:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:08:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.936554, avg_loss=0.669683, seen=200, correct=113, accuracy=0.565000
2025-09-11 07:08:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:09:00 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 2 with val results: {'val_total': 200, 'val_loss': 133.93655395507812, 'val_avg_loss': 0.6696827697753907, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:09:00 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 2 with val results: {'val_total': 200, 'val_loss': 133.93655395507812, 'val_avg_loss': 0.6696827697753907, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:09:00 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 2 with val results: {'val_total': 200, 'val_loss': 133.93655395507812, 'val_avg_loss': 0.6696827697753907, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:09:00 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 2 with val results: {'val_total': 200, 'val_loss': 133.93655395507812, 'val_avg_loss': 0.6696827697753907, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:09:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:09:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:09:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:09:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.077793, avg_loss=0.701945, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:09:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:09:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:09:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:09:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1862MB
2025-09-11 07:09:02 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.07779312133789, 'test_avg_loss': 0.7019448280334473, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:09:02 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.07779312133789, 'test_avg_loss': 0.7019448280334473, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:09:02 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.07779312133789, 'test_avg_loss': 0.7019448280334473, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:09:02 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.07779312133789, 'test_avg_loss': 0.7019448280334473, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:09:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-11 07:09:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=136)
2025-09-11 07:09:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=136)
2025-09-11 07:09:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=34, total=136)
2025-09-11 07:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:09:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:09:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:09:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:09:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:09:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=34
2025-09-11 07:09:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.947144, avg_loss=0.661376, seen=136, correct=84, accuracy=0.617647
2025-09-11 07:09:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:09:08 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 0 with val results: {'val_total': 136, 'val_loss': 89.9471435546875, 'val_avg_loss': 0.6613760555491728, 'val_seen': 136, 'val_correct': 84, 'val_acc': 0.6176470588235294}
2025-09-11 07:09:08 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 0 with val results: {'val_total': 136, 'val_loss': 89.9471435546875, 'val_avg_loss': 0.6613760555491728, 'val_seen': 136, 'val_correct': 84, 'val_acc': 0.6176470588235294}
2025-09-11 07:09:08 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 0 with val results: {'val_total': 136, 'val_loss': 89.9471435546875, 'val_avg_loss': 0.6613760555491728, 'val_seen': 136, 'val_correct': 84, 'val_acc': 0.6176470588235294}
2025-09-11 07:09:08 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 0 with val results: {'val_total': 136, 'val_loss': 89.9471435546875, 'val_avg_loss': 0.6613760555491728, 'val_seen': 136, 'val_correct': 84, 'val_acc': 0.6176470588235294}
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:09:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:09:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:09:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:09:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.895950, avg_loss=0.672399, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:09:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:09:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:09:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:09:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:09:10 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.895950317382812, 'test_avg_loss': 0.6723987579345703, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:09:10 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.895950317382812, 'test_avg_loss': 0.6723987579345703, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:09:10 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.895950317382812, 'test_avg_loss': 0.6723987579345703, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:09:10 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.895950317382812, 'test_avg_loss': 0.6723987579345703, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:09:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=136)
2025-09-11 07:09:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-11 07:09:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=34, total=136)
2025-09-11 07:09:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=136)
2025-09-11 07:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:09:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:09:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:09:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:09:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:09:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=34
2025-09-11 07:09:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=92.089363, avg_loss=0.677128, seen=136, correct=79, accuracy=0.580882
2025-09-11 07:09:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:15 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 1 with val results: {'val_total': 136, 'val_loss': 92.08936309814453, 'val_avg_loss': 0.6771276698392981, 'val_seen': 136, 'val_correct': 79, 'val_acc': 0.5808823529411765}
2025-09-11 07:09:15 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 1 with val results: {'val_total': 136, 'val_loss': 92.08936309814453, 'val_avg_loss': 0.6771276698392981, 'val_seen': 136, 'val_correct': 79, 'val_acc': 0.5808823529411765}
2025-09-11 07:09:15 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 1 with val results: {'val_total': 136, 'val_loss': 92.08936309814453, 'val_avg_loss': 0.6771276698392981, 'val_seen': 136, 'val_correct': 79, 'val_acc': 0.5808823529411765}
2025-09-11 07:09:15 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 1 with val results: {'val_total': 136, 'val_loss': 92.08936309814453, 'val_avg_loss': 0.6771276698392981, 'val_seen': 136, 'val_correct': 79, 'val_acc': 0.5808823529411765}
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:09:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:09:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:09:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.823212, avg_loss=0.695580, seen=40, correct=18, accuracy=0.450000
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:18 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.823211669921875, 'test_avg_loss': 0.6955802917480469, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:09:18 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.823211669921875, 'test_avg_loss': 0.6955802917480469, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:09:18 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.823211669921875, 'test_avg_loss': 0.6955802917480469, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:09:18 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.823211669921875, 'test_avg_loss': 0.6955802917480469, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=136)
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=136)
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=34, total=136)
2025-09-11 07:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=34
2025-09-11 07:09:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:09:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:09:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:09:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=93.453270, avg_loss=0.687156, seen=136, correct=71, accuracy=0.522059
2025-09-11 07:09:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1870MB
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:23 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 2 with val results: {'val_total': 136, 'val_loss': 93.4532699584961, 'val_avg_loss': 0.6871563967536477, 'val_seen': 136, 'val_correct': 71, 'val_acc': 0.5220588235294118}
2025-09-11 07:09:23 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 2 with val results: {'val_total': 136, 'val_loss': 93.4532699584961, 'val_avg_loss': 0.6871563967536477, 'val_seen': 136, 'val_correct': 71, 'val_acc': 0.5220588235294118}
2025-09-11 07:09:23 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 2 with val results: {'val_total': 136, 'val_loss': 93.4532699584961, 'val_avg_loss': 0.6871563967536477, 'val_seen': 136, 'val_correct': 71, 'val_acc': 0.5220588235294118}
2025-09-11 07:09:23 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 2 with val results: {'val_total': 136, 'val_loss': 93.4532699584961, 'val_avg_loss': 0.6871563967536477, 'val_seen': 136, 'val_correct': 71, 'val_acc': 0.5220588235294118}
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:09:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.723755, avg_loss=0.718094, seen=40, correct=17, accuracy=0.425000
2025-09-11 07:09:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1870MB
2025-09-11 07:09:26 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.7237548828125, 'test_avg_loss': 0.7180938720703125, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:09:26 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.7237548828125, 'test_avg_loss': 0.7180938720703125, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:09:26 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.7237548828125, 'test_avg_loss': 0.7180938720703125, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:09:26 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.7237548828125, 'test_avg_loss': 0.7180938720703125, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:09:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:09:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:09:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:09:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:09:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:09:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:09:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:09:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:09:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.937622, avg_loss=0.669688, seen=200, correct=109, accuracy=0.545000
2025-09-11 07:09:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:09:34 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.9376220703125, 'val_avg_loss': 0.6696881103515625, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:34 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.9376220703125, 'val_avg_loss': 0.6696881103515625, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:34 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.9376220703125, 'val_avg_loss': 0.6696881103515625, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:34 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.9376220703125, 'val_avg_loss': 0.6696881103515625, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:09:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:09:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:09:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.930147, avg_loss=0.673254, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:09:36 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.930147171020508, 'test_avg_loss': 0.6732536792755127, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:09:36 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.930147171020508, 'test_avg_loss': 0.6732536792755127, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:09:36 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.930147171020508, 'test_avg_loss': 0.6732536792755127, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:09:36 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.930147171020508, 'test_avg_loss': 0.6732536792755127, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:09:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.143112, avg_loss=0.680716, seen=200, correct=109, accuracy=0.545000
2025-09-11 07:09:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:44 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 1 with val results: {'val_total': 200, 'val_loss': 136.1431121826172, 'val_avg_loss': 0.6807155609130859, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:44 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 1 with val results: {'val_total': 200, 'val_loss': 136.1431121826172, 'val_avg_loss': 0.6807155609130859, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:44 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 1 with val results: {'val_total': 200, 'val_loss': 136.1431121826172, 'val_avg_loss': 0.6807155609130859, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:44 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 1 with val results: {'val_total': 200, 'val_loss': 136.1431121826172, 'val_avg_loss': 0.6807155609130859, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:09:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:09:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:09:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:09:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.088839, avg_loss=0.677221, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:09:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:47 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.088838577270508, 'test_avg_loss': 0.6772209644317627, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:09:47 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.088838577270508, 'test_avg_loss': 0.6772209644317627, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:09:47 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.088838577270508, 'test_avg_loss': 0.6772209644317627, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:09:47 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.088838577270508, 'test_avg_loss': 0.6772209644317627, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:09:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:09:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:09:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:09:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:09:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.060471, avg_loss=0.695302, seen=200, correct=109, accuracy=0.545000
2025-09-11 07:09:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:54 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.0604705810547, 'val_avg_loss': 0.6953023529052734, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:54 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.0604705810547, 'val_avg_loss': 0.6953023529052734, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:54 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.0604705810547, 'val_avg_loss': 0.6953023529052734, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:54 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.0604705810547, 'val_avg_loss': 0.6953023529052734, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:09:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:09:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:09:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:09:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:09:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:09:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.476328, avg_loss=0.686908, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:09:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:09:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1879MB
2025-09-11 07:09:57 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.476327896118164, 'test_avg_loss': 0.6869081974029541, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:09:57 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.476327896118164, 'test_avg_loss': 0.6869081974029541, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:09:57 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.476327896118164, 'test_avg_loss': 0.6869081974029541, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:09:57 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.476327896118164, 'test_avg_loss': 0.6869081974029541, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:09:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:09:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-11 07:09:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=135)
2025-09-11 07:09:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=135)
2025-09-11 07:09:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=135)
2025-09-11 07:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:09:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:09:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:10:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:10:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:10:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:10:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.339569, avg_loss=0.706219, seen=135, correct=71, accuracy=0.525926
2025-09-11 07:10:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:10:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:10:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:10:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:10:04 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 0 with val results: {'val_total': 135, 'val_loss': 95.33956909179688, 'val_avg_loss': 0.7062190303096065, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-11 07:10:04 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 0 with val results: {'val_total': 135, 'val_loss': 95.33956909179688, 'val_avg_loss': 0.7062190303096065, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-11 07:10:04 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 0 with val results: {'val_total': 135, 'val_loss': 95.33956909179688, 'val_avg_loss': 0.7062190303096065, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-11 07:10:04 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 0 with val results: {'val_total': 135, 'val_loss': 95.33956909179688, 'val_avg_loss': 0.7062190303096065, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-11 07:10:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:10:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:10:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:10:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:10:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:10:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:10:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:10:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:10:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.253277, avg_loss=0.681332, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:10:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:10:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:10:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:10:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:10:07 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.253276824951172, 'test_avg_loss': 0.6813319206237793, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:10:07 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.253276824951172, 'test_avg_loss': 0.6813319206237793, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:10:07 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.253276824951172, 'test_avg_loss': 0.6813319206237793, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:10:07 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.253276824951172, 'test_avg_loss': 0.6813319206237793, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:10:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-11 07:10:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=135)
2025-09-11 07:10:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=135)
2025-09-11 07:10:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=135)
2025-09-11 07:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:10:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:10:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:10:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:10:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:10:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.825073, avg_loss=0.709815, seen=135, correct=64, accuracy=0.474074
2025-09-11 07:10:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:13 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 1 with val results: {'val_total': 135, 'val_loss': 95.8250732421875, 'val_avg_loss': 0.709815357349537, 'val_seen': 135, 'val_correct': 64, 'val_acc': 0.4740740740740741}
2025-09-11 07:10:13 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 1 with val results: {'val_total': 135, 'val_loss': 95.8250732421875, 'val_avg_loss': 0.709815357349537, 'val_seen': 135, 'val_correct': 64, 'val_acc': 0.4740740740740741}
2025-09-11 07:10:13 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 1 with val results: {'val_total': 135, 'val_loss': 95.8250732421875, 'val_avg_loss': 0.709815357349537, 'val_seen': 135, 'val_correct': 64, 'val_acc': 0.4740740740740741}
2025-09-11 07:10:13 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 1 with val results: {'val_total': 135, 'val_loss': 95.8250732421875, 'val_avg_loss': 0.709815357349537, 'val_seen': 135, 'val_correct': 64, 'val_acc': 0.4740740740740741}
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.217705, avg_loss=0.680443, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:16 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.21770477294922, 'test_avg_loss': 0.6804426193237305, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:10:16 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.21770477294922, 'test_avg_loss': 0.6804426193237305, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:10:16 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.21770477294922, 'test_avg_loss': 0.6804426193237305, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:10:16 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.21770477294922, 'test_avg_loss': 0.6804426193237305, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:10:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=34, total=135)
2025-09-11 07:10:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-11 07:10:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=135)
2025-09-11 07:10:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=34, total=135)
2025-09-11 07:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:10:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=34
2025-09-11 07:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=34
2025-09-11 07:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:10:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.300308, avg_loss=0.698521, seen=135, correct=71, accuracy=0.525926
2025-09-11 07:10:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:21 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 2 with val results: {'val_total': 135, 'val_loss': 94.30030822753906, 'val_avg_loss': 0.6985208016854746, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-11 07:10:21 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 2 with val results: {'val_total': 135, 'val_loss': 94.30030822753906, 'val_avg_loss': 0.6985208016854746, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-11 07:10:21 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 2 with val results: {'val_total': 135, 'val_loss': 94.30030822753906, 'val_avg_loss': 0.6985208016854746, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-11 07:10:21 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 2 with val results: {'val_total': 135, 'val_loss': 94.30030822753906, 'val_avg_loss': 0.6985208016854746, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-11 07:10:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:10:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:10:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:10:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:10:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:10:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:10:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:10:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:10:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:10:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.700325, avg_loss=0.642508, seen=40, correct=29, accuracy=0.725000
2025-09-11 07:10:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1888MB
2025-09-11 07:10:24 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.70032501220703, 'test_avg_loss': 0.6425081253051758, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-11 07:10:24 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.70032501220703, 'test_avg_loss': 0.6425081253051758, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-11 07:10:24 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.70032501220703, 'test_avg_loss': 0.6425081253051758, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-11 07:10:24 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.70032501220703, 'test_avg_loss': 0.6425081253051758, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-11 07:10:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:10:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:10:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:10:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:10:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-11 07:10:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=110)
2025-09-11 07:10:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=27, total=110)
2025-09-11 07:10:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=27, total=110)
2025-09-11 07:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:10:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=27
2025-09-11 07:10:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:10:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=27
2025-09-11 07:10:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:10:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=79.142288, avg_loss=0.719475, seen=110, correct=49, accuracy=0.445455
2025-09-11 07:10:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:10:29 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 0 with val results: {'val_total': 110, 'val_loss': 79.14228820800781, 'val_avg_loss': 0.7194753473455255, 'val_seen': 110, 'val_correct': 49, 'val_acc': 0.44545454545454544}
2025-09-11 07:10:29 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 0 with val results: {'val_total': 110, 'val_loss': 79.14228820800781, 'val_avg_loss': 0.7194753473455255, 'val_seen': 110, 'val_correct': 49, 'val_acc': 0.44545454545454544}
2025-09-11 07:10:29 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 0 with val results: {'val_total': 110, 'val_loss': 79.14228820800781, 'val_avg_loss': 0.7194753473455255, 'val_seen': 110, 'val_correct': 49, 'val_acc': 0.44545454545454544}
2025-09-11 07:10:29 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 0 with val results: {'val_total': 110, 'val_loss': 79.14228820800781, 'val_avg_loss': 0.7194753473455255, 'val_seen': 110, 'val_correct': 49, 'val_acc': 0.44545454545454544}
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

(20, 10, 2, 30)2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:10:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:10:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:10:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.576981, avg_loss=0.714425, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:10:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:10:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:10:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:10:31 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.576980590820312, 'test_avg_loss': 0.7144245147705078, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:10:31 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.576980590820312, 'test_avg_loss': 0.7144245147705078, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:10:31 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.576980590820312, 'test_avg_loss': 0.7144245147705078, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:10:31 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.576980590820312, 'test_avg_loss': 0.7144245147705078, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:10:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-11 07:10:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=27, total=110)
2025-09-11 07:10:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=27, total=110)
2025-09-11 07:10:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=110)
2025-09-11 07:10:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:10:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:10:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=27
2025-09-11 07:10:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:10:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=27
2025-09-11 07:10:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:10:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=77.801552, avg_loss=0.707287, seen=110, correct=57, accuracy=0.518182
2025-09-11 07:10:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:36 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 1 with val results: {'val_total': 110, 'val_loss': 77.80155181884766, 'val_avg_loss': 0.7072868347167969, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182}
2025-09-11 07:10:36 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 1 with val results: {'val_total': 110, 'val_loss': 77.80155181884766, 'val_avg_loss': 0.7072868347167969, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182}
2025-09-11 07:10:36 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 1 with val results: {'val_total': 110, 'val_loss': 77.80155181884766, 'val_avg_loss': 0.7072868347167969, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182}
2025-09-11 07:10:36 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 1 with val results: {'val_total': 110, 'val_loss': 77.80155181884766, 'val_avg_loss': 0.7072868347167969, 'val_seen': 110, 'val_correct': 57, 'val_acc': 0.5181818181818182}
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)(20, 10, 2, 30)


2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:10:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:10:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:10:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.846983, avg_loss=0.721175, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:39 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.846982955932617, 'test_avg_loss': 0.7211745738983154, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:10:39 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.846982955932617, 'test_avg_loss': 0.7211745738983154, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:10:39 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.846982955932617, 'test_avg_loss': 0.7211745738983154, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:10:39 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.846982955932617, 'test_avg_loss': 0.7211745738983154, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:10:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=27, total=110)
2025-09-11 07:10:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=27, total=110)
2025-09-11 07:10:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=110)
2025-09-11 07:10:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-11 07:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:10:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:10:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=27
2025-09-11 07:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=27
2025-09-11 07:10:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=78.931709, avg_loss=0.717561, seen=110, correct=53, accuracy=0.481818
2025-09-11 07:10:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:43 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 2 with val results: {'val_total': 110, 'val_loss': 78.93170928955078, 'val_avg_loss': 0.7175609935413707, 'val_seen': 110, 'val_correct': 53, 'val_acc': 0.4818181818181818}
2025-09-11 07:10:43 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 2 with val results: {'val_total': 110, 'val_loss': 78.93170928955078, 'val_avg_loss': 0.7175609935413707, 'val_seen': 110, 'val_correct': 53, 'val_acc': 0.4818181818181818}
2025-09-11 07:10:43 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 2 with val results: {'val_total': 110, 'val_loss': 78.93170928955078, 'val_avg_loss': 0.7175609935413707, 'val_seen': 110, 'val_correct': 53, 'val_acc': 0.4818181818181818}
2025-09-11 07:10:43 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 2 with val results: {'val_total': 110, 'val_loss': 78.93170928955078, 'val_avg_loss': 0.7175609935413707, 'val_seen': 110, 'val_correct': 53, 'val_acc': 0.4818181818181818}
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:10:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:10:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:10:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:10:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.309368, avg_loss=0.732734, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:10:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1896MB
2025-09-11 07:10:45 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.309368133544922, 'test_avg_loss': 0.732734203338623, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:10:45 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.309368133544922, 'test_avg_loss': 0.732734203338623, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:10:45 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.309368133544922, 'test_avg_loss': 0.732734203338623, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:10:45 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.309368133544922, 'test_avg_loss': 0.732734203338623, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:10:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:10:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:10:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:10:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:10:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-11 07:10:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=31, total=126)
2025-09-11 07:10:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=31, total=126)
2025-09-11 07:10:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=32, total=126)
2025-09-11 07:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:10:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=31
2025-09-11 07:10:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=32
2025-09-11 07:10:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-11 07:10:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=31
2025-09-11 07:10:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=86.716606, avg_loss=0.688227, seen=126, correct=70, accuracy=0.555556
2025-09-11 07:10:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-11 07:10:52 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 0 with val results: {'val_total': 126, 'val_loss': 86.71660614013672, 'val_avg_loss': 0.688227032858228, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}
2025-09-11 07:10:52 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 0 with val results: {'val_total': 126, 'val_loss': 86.71660614013672, 'val_avg_loss': 0.688227032858228, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}
2025-09-11 07:10:52 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 0 with val results: {'val_total': 126, 'val_loss': 86.71660614013672, 'val_avg_loss': 0.688227032858228, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}
2025-09-11 07:10:52 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 0 with val results: {'val_total': 126, 'val_loss': 86.71660614013672, 'val_avg_loss': 0.688227032858228, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:10:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:10:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:10:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:10:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.476992, avg_loss=0.661925, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:10:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-11 07:10:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-11 07:10:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-11 07:10:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1929MB
2025-09-11 07:10:54 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.476991653442383, 'test_avg_loss': 0.6619247913360595, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:10:54 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.476991653442383, 'test_avg_loss': 0.6619247913360595, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:10:54 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.476991653442383, 'test_avg_loss': 0.6619247913360595, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:10:54 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.476991653442383, 'test_avg_loss': 0.6619247913360595, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:10:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=31, total=126)
2025-09-11 07:10:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=31, total=126)
2025-09-11 07:10:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=32, total=126)
2025-09-11 07:10:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-11 07:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:10:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:10:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:10:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-11 07:10:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=31
2025-09-11 07:10:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=31
2025-09-11 07:10:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=32
2025-09-11 07:10:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.759903, avg_loss=0.680634, seen=126, correct=75, accuracy=0.595238
2025-09-11 07:10:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:10:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:00 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 1 with val results: {'val_total': 126, 'val_loss': 85.75990295410156, 'val_avg_loss': 0.6806341504293775, 'val_seen': 126, 'val_correct': 75, 'val_acc': 0.5952380952380952}
2025-09-11 07:11:00 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 1 with val results: {'val_total': 126, 'val_loss': 85.75990295410156, 'val_avg_loss': 0.6806341504293775, 'val_seen': 126, 'val_correct': 75, 'val_acc': 0.5952380952380952}
2025-09-11 07:11:00 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 1 with val results: {'val_total': 126, 'val_loss': 85.75990295410156, 'val_avg_loss': 0.6806341504293775, 'val_seen': 126, 'val_correct': 75, 'val_acc': 0.5952380952380952}
2025-09-11 07:11:00 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 1 with val results: {'val_total': 126, 'val_loss': 85.75990295410156, 'val_avg_loss': 0.6806341504293775, 'val_seen': 126, 'val_correct': 75, 'val_acc': 0.5952380952380952}
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.866463, avg_loss=0.646662, seen=40, correct=29, accuracy=0.725000
2025-09-11 07:11:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:03 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.86646270751953, 'test_avg_loss': 0.6466615676879883, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-11 07:11:03 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.86646270751953, 'test_avg_loss': 0.6466615676879883, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-11 07:11:03 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.86646270751953, 'test_avg_loss': 0.6466615676879883, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-11 07:11:03 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.86646270751953, 'test_avg_loss': 0.6466615676879883, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=31, total=126)
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=31, total=126)
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=32, total=126)
2025-09-11 07:11:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:11:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-11 07:11:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=32
2025-09-11 07:11:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=31
2025-09-11 07:11:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=31
2025-09-11 07:11:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=88.428017, avg_loss=0.701810, seen=126, correct=61, accuracy=0.484127
2025-09-11 07:11:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:09 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 2 with val results: {'val_total': 126, 'val_loss': 88.42801666259766, 'val_avg_loss': 0.7018096560523623, 'val_seen': 126, 'val_correct': 61, 'val_acc': 0.48412698412698413}
2025-09-11 07:11:09 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 2 with val results: {'val_total': 126, 'val_loss': 88.42801666259766, 'val_avg_loss': 0.7018096560523623, 'val_seen': 126, 'val_correct': 61, 'val_acc': 0.48412698412698413}
2025-09-11 07:11:09 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 2 with val results: {'val_total': 126, 'val_loss': 88.42801666259766, 'val_avg_loss': 0.7018096560523623, 'val_seen': 126, 'val_correct': 61, 'val_acc': 0.48412698412698413}
2025-09-11 07:11:09 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 2 with val results: {'val_total': 126, 'val_loss': 88.42801666259766, 'val_avg_loss': 0.7018096560523623, 'val_seen': 126, 'val_correct': 61, 'val_acc': 0.48412698412698413}
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.540920, avg_loss=0.688523, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1904MB
2025-09-11 07:11:11 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.54092025756836, 'test_avg_loss': 0.6885230064392089, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:11:11 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.54092025756836, 'test_avg_loss': 0.6885230064392089, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:11:11 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.54092025756836, 'test_avg_loss': 0.6885230064392089, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:11:11 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.54092025756836, 'test_avg_loss': 0.6885230064392089, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:11:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-11 07:11:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=38, total=153)
2025-09-11 07:11:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=38, total=153)
2025-09-11 07:11:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=38, total=153)
2025-09-11 07:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:11:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:11:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:11:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-11 07:11:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=38
2025-09-11 07:11:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=38
2025-09-11 07:11:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=38
2025-09-11 07:11:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=105.784302, avg_loss=0.691401, seen=153, correct=77, accuracy=0.503268
2025-09-11 07:11:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:11:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:11:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:11:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:11:17 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 0 with val results: {'val_total': 153, 'val_loss': 105.7843017578125, 'val_avg_loss': 0.6914006650837419, 'val_seen': 153, 'val_correct': 77, 'val_acc': 0.5032679738562091}
2025-09-11 07:11:17 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 0 with val results: {'val_total': 153, 'val_loss': 105.7843017578125, 'val_avg_loss': 0.6914006650837419, 'val_seen': 153, 'val_correct': 77, 'val_acc': 0.5032679738562091}
2025-09-11 07:11:17 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 0 with val results: {'val_total': 153, 'val_loss': 105.7843017578125, 'val_avg_loss': 0.6914006650837419, 'val_seen': 153, 'val_correct': 77, 'val_acc': 0.5032679738562091}
2025-09-11 07:11:17 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 0 with val results: {'val_total': 153, 'val_loss': 105.7843017578125, 'val_avg_loss': 0.6914006650837419, 'val_seen': 153, 'val_correct': 77, 'val_acc': 0.5032679738562091}
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.324169, avg_loss=0.708104, seen=40, correct=15, accuracy=0.375000
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:11:20 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.324169158935547, 'test_avg_loss': 0.7081042289733886, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:11:20 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.324169158935547, 'test_avg_loss': 0.7081042289733886, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:11:20 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.324169158935547, 'test_avg_loss': 0.7081042289733886, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:11:20 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.324169158935547, 'test_avg_loss': 0.7081042289733886, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=38, total=153)
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=38, total=153)
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=38, total=153)
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-11 07:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:11:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-11 07:11:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=38
2025-09-11 07:11:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=38
2025-09-11 07:11:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=38
2025-09-11 07:11:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=105.900948, avg_loss=0.692163, seen=153, correct=90, accuracy=0.588235
2025-09-11 07:11:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:26 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 1 with val results: {'val_total': 153, 'val_loss': 105.90094757080078, 'val_avg_loss': 0.6921630560183057, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-11 07:11:26 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 1 with val results: {'val_total': 153, 'val_loss': 105.90094757080078, 'val_avg_loss': 0.6921630560183057, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-11 07:11:26 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 1 with val results: {'val_total': 153, 'val_loss': 105.90094757080078, 'val_avg_loss': 0.6921630560183057, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-11 07:11:26 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 1 with val results: {'val_total': 153, 'val_loss': 105.90094757080078, 'val_avg_loss': 0.6921630560183057, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.014431, avg_loss=0.700361, seen=40, correct=18, accuracy=0.450000
2025-09-11 07:11:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:28 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.01443099975586, 'test_avg_loss': 0.7003607749938965, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:11:28 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.01443099975586, 'test_avg_loss': 0.7003607749938965, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:11:28 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.01443099975586, 'test_avg_loss': 0.7003607749938965, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:11:28 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.01443099975586, 'test_avg_loss': 0.7003607749938965, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:11:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=38, total=153)
2025-09-11 07:11:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-11 07:11:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=38, total=153)
2025-09-11 07:11:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=38, total=153)
2025-09-11 07:11:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:11:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:11:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:11:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:11:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-11 07:11:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=38
2025-09-11 07:11:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=38
2025-09-11 07:11:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=38
2025-09-11 07:11:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.972687, avg_loss=0.686096, seen=153, correct=91, accuracy=0.594771
2025-09-11 07:11:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:36 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 2 with val results: {'val_total': 153, 'val_loss': 104.97268676757812, 'val_avg_loss': 0.6860959919449551, 'val_seen': 153, 'val_correct': 91, 'val_acc': 0.5947712418300654}
2025-09-11 07:11:36 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 2 with val results: {'val_total': 153, 'val_loss': 104.97268676757812, 'val_avg_loss': 0.6860959919449551, 'val_seen': 153, 'val_correct': 91, 'val_acc': 0.5947712418300654}
2025-09-11 07:11:36 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 2 with val results: {'val_total': 153, 'val_loss': 104.97268676757812, 'val_avg_loss': 0.6860959919449551, 'val_seen': 153, 'val_correct': 91, 'val_acc': 0.5947712418300654}
2025-09-11 07:11:36 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 2 with val results: {'val_total': 153, 'val_loss': 104.97268676757812, 'val_avg_loss': 0.6860959919449551, 'val_seen': 153, 'val_correct': 91, 'val_acc': 0.5947712418300654}
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.437504, avg_loss=0.735938, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:11:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1913MB
2025-09-11 07:11:38 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.437503814697266, 'test_avg_loss': 0.7359375953674316, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:11:38 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.437503814697266, 'test_avg_loss': 0.7359375953674316, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:11:38 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.437503814697266, 'test_avg_loss': 0.7359375953674316, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:11:38 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.437503814697266, 'test_avg_loss': 0.7359375953674316, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:11:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.186232, avg_loss=0.653294, seen=11, correct=7, accuracy=0.636364
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:11:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:11:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:11:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:11:40 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.18623161315918, 'val_avg_loss': 0.6532937830144708, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-11 07:11:40 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.18623161315918, 'val_avg_loss': 0.6532937830144708, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-11 07:11:40 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.18623161315918, 'val_avg_loss': 0.6532937830144708, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-11 07:11:40 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 0 with val results: {'val_total': 11, 'val_loss': 7.18623161315918, 'val_avg_loss': 0.6532937830144708, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-11 07:11:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:11:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:11:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.910877, avg_loss=0.722772, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:11:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:11:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:11:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:11:43 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.910877227783203, 'test_avg_loss': 0.7227719306945801, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:11:43 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.910877227783203, 'test_avg_loss': 0.7227719306945801, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:11:43 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.910877227783203, 'test_avg_loss': 0.7227719306945801, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:11:43 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.910877227783203, 'test_avg_loss': 0.7227719306945801, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
(6, 6, 5, 30)
(6, 6, 5, 30)
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:11:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:11:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:11:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.845193, avg_loss=0.713199, seen=11, correct=6, accuracy=0.545455
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:45 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.845193386077881, 'val_avg_loss': 0.7131993987343528, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-11 07:11:45 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.845193386077881, 'val_avg_loss': 0.7131993987343528, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-11 07:11:45 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.845193386077881, 'val_avg_loss': 0.7131993987343528, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-11 07:11:45 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.845193386077881, 'val_avg_loss': 0.7131993987343528, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.491159, avg_loss=0.712279, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:11:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:47 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.491159439086914, 'test_avg_loss': 0.7122789859771729, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:11:47 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.491159439086914, 'test_avg_loss': 0.7122789859771729, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:11:47 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.491159439086914, 'test_avg_loss': 0.7122789859771729, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:11:47 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.491159439086914, 'test_avg_loss': 0.7122789859771729, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=3, total=11)
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=2, total=11)
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=3, total=11)
2025-09-11 07:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(6, 6, 5, 30)
(6, 6, 5, 30)
(6, 6, 5, 30)
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
(6, 6, 5, 30)
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=6, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-11 07:11:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=2
2025-09-11 07:11:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=3
2025-09-11 07:11:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=3
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.540648, avg_loss=0.776423, seen=11, correct=2, accuracy=0.181818
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:49 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.540647506713867, 'val_avg_loss': 0.7764225006103516, 'val_seen': 11, 'val_correct': 2, 'val_acc': 0.18181818181818182}
2025-09-11 07:11:49 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.540647506713867, 'val_avg_loss': 0.7764225006103516, 'val_seen': 11, 'val_correct': 2, 'val_acc': 0.18181818181818182}
2025-09-11 07:11:49 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.540647506713867, 'val_avg_loss': 0.7764225006103516, 'val_seen': 11, 'val_correct': 2, 'val_acc': 0.18181818181818182}
2025-09-11 07:11:49 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.540647506713867, 'val_avg_loss': 0.7764225006103516, 'val_seen': 11, 'val_correct': 2, 'val_acc': 0.18181818181818182}
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.525455, avg_loss=0.713136, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:11:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1921MB
2025-09-11 07:11:51 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.525455474853516, 'test_avg_loss': 0.7131363868713378, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:11:51 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.525455474853516, 'test_avg_loss': 0.7131363868713378, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:11:51 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.525455474853516, 'test_avg_loss': 0.7131363868713378, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:11:51 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.525455474853516, 'test_avg_loss': 0.7131363868713378, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:11:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:11:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=7, total=30)
2025-09-11 07:11:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-11 07:11:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=8, total=30)
2025-09-11 07:11:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=7, total=30)
2025-09-11 07:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(15, 15, 2, 30)
2025-09-11 07:11:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(15, 15, 2, 30)
2025-09-11 07:11:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(15, 15, 2, 30)
2025-09-11 07:11:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(15, 15, 2, 30)
2025-09-11 07:11:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=7
2025-09-11 07:11:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-11 07:11:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=7
2025-09-11 07:11:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=8
2025-09-11 07:11:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=20.192333, avg_loss=0.673078, seen=30, correct=19, accuracy=0.633333
2025-09-11 07:11:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:11:54 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 0 with val results: {'val_total': 30, 'val_loss': 20.192333221435547, 'val_avg_loss': 0.6730777740478515, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:11:54 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 0 with val results: {'val_total': 30, 'val_loss': 20.192333221435547, 'val_avg_loss': 0.6730777740478515, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:11:54 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 0 with val results: {'val_total': 30, 'val_loss': 20.192333221435547, 'val_avg_loss': 0.6730777740478515, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:11:54 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 0 with val results: {'val_total': 30, 'val_loss': 20.192333221435547, 'val_avg_loss': 0.6730777740478515, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

(20, 10, 2, 30)
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:11:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:11:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:11:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.859730, avg_loss=0.671493, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:11:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:11:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:11:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:11:57 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.859729766845703, 'test_avg_loss': 0.6714932441711425, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:11:57 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.859729766845703, 'test_avg_loss': 0.6714932441711425, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:11:57 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.859729766845703, 'test_avg_loss': 0.6714932441711425, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:11:57 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.859729766845703, 'test_avg_loss': 0.6714932441711425, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:11:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-11 07:11:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=7, total=30)
2025-09-11 07:11:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=7, total=30)
2025-09-11 07:11:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=8, total=30)
2025-09-11 07:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(15, 15, 2, 30)
2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(15, 15, 2, 30)(15, 15, 2, 30)

2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(15, 15, 2, 30)
2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-11 07:11:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=8
2025-09-11 07:11:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=7
2025-09-11 07:11:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=7
2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=20.331299, avg_loss=0.677710, seen=30, correct=19, accuracy=0.633333
2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:11:59 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 1 with val results: {'val_total': 30, 'val_loss': 20.331298828125, 'val_avg_loss': 0.6777099609375, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:11:59 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 1 with val results: {'val_total': 30, 'val_loss': 20.331298828125, 'val_avg_loss': 0.6777099609375, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:11:59 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 1 with val results: {'val_total': 30, 'val_loss': 20.331298828125, 'val_avg_loss': 0.6777099609375, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:11:59 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 1 with val results: {'val_total': 30, 'val_loss': 20.331298828125, 'val_avg_loss': 0.6777099609375, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:11:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:12:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:12:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:12:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:12:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.000904, avg_loss=0.675023, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:12:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:01 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.000904083251953, 'test_avg_loss': 0.6750226020812988, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:12:01 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.000904083251953, 'test_avg_loss': 0.6750226020812988, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:12:01 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.000904083251953, 'test_avg_loss': 0.6750226020812988, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:12:01 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.000904083251953, 'test_avg_loss': 0.6750226020812988, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:12:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-11 07:12:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=7, total=30)
2025-09-11 07:12:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=7, total=30)
2025-09-11 07:12:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=8, total=30)
2025-09-11 07:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(15, 15, 2, 30)
(15, 15, 2, 30)
2025-09-11 07:12:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(15, 15, 2, 30)
(15, 15, 2, 30)
2025-09-11 07:12:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-11 07:12:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=7
2025-09-11 07:12:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=7
2025-09-11 07:12:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=8
2025-09-11 07:12:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=20.088898, avg_loss=0.669630, seen=30, correct=19, accuracy=0.633333
2025-09-11 07:12:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:04 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 2 with val results: {'val_total': 30, 'val_loss': 20.088897705078125, 'val_avg_loss': 0.6696299235026042, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:12:04 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 2 with val results: {'val_total': 30, 'val_loss': 20.088897705078125, 'val_avg_loss': 0.6696299235026042, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:12:04 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 2 with val results: {'val_total': 30, 'val_loss': 20.088897705078125, 'val_avg_loss': 0.6696299235026042, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:12:04 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 2 with val results: {'val_total': 30, 'val_loss': 20.088897705078125, 'val_avg_loss': 0.6696299235026042, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-11 07:12:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:12:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:12:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:12:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:12:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:12:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:12:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:12:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:12:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:12:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.683901, avg_loss=0.692098, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:12:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1930MB
2025-09-11 07:12:07 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.683900833129883, 'test_avg_loss': 0.692097520828247, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:12:07 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.683900833129883, 'test_avg_loss': 0.692097520828247, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:12:07 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.683900833129883, 'test_avg_loss': 0.692097520828247, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:12:07 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.683900833129883, 'test_avg_loss': 0.692097520828247, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:12:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:12:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:12:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:12:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:12:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:12:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:12:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:12:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:12:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:12:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:12:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:12:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:12:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.967758, avg_loss=0.669839, seen=200, correct=113, accuracy=0.565000
2025-09-11 07:12:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:12:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:12:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:12:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:12:15 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.96775817871094, 'val_avg_loss': 0.6698387908935547, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:12:15 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.96775817871094, 'val_avg_loss': 0.6698387908935547, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:12:15 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.96775817871094, 'val_avg_loss': 0.6698387908935547, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:12:15 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.96775817871094, 'val_avg_loss': 0.6698387908935547, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:12:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:12:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:12:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:12:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:12:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:12:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:12:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.773697, avg_loss=0.744342, seen=40, correct=15, accuracy=0.375000
2025-09-11 07:12:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:12:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:12:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:12:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:12:18 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.773696899414062, 'test_avg_loss': 0.7443424224853515, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:12:18 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.773696899414062, 'test_avg_loss': 0.7443424224853515, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:12:18 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.773696899414062, 'test_avg_loss': 0.7443424224853515, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:12:18 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.773696899414062, 'test_avg_loss': 0.7443424224853515, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:12:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:12:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:12:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:12:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:12:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:12:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:12:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:12:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:12:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:12:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.493805, avg_loss=0.692469, seen=200, correct=113, accuracy=0.565000
2025-09-11 07:12:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:25 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.49380493164062, 'val_avg_loss': 0.6924690246582031, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:12:25 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.49380493164062, 'val_avg_loss': 0.6924690246582031, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:12:25 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.49380493164062, 'val_avg_loss': 0.6924690246582031, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:12:25 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.49380493164062, 'val_avg_loss': 0.6924690246582031, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:12:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:12:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:12:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.057674, avg_loss=0.726442, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:28 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.057674407958984, 'test_avg_loss': 0.7264418601989746, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:12:28 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.057674407958984, 'test_avg_loss': 0.7264418601989746, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:12:28 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.057674407958984, 'test_avg_loss': 0.7264418601989746, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:12:28 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.057674407958984, 'test_avg_loss': 0.7264418601989746, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:12:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:12:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:12:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:12:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.269104, avg_loss=0.691346, seen=200, correct=105, accuracy=0.525000
2025-09-11 07:12:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:35 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.26910400390625, 'val_avg_loss': 0.6913455200195312, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-11 07:12:35 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.26910400390625, 'val_avg_loss': 0.6913455200195312, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-11 07:12:35 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.26910400390625, 'val_avg_loss': 0.6913455200195312, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-11 07:12:35 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.26910400390625, 'val_avg_loss': 0.6913455200195312, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:12:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:12:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.711515, avg_loss=0.717788, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1938MB
2025-09-11 07:12:38 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.711515426635742, 'test_avg_loss': 0.7177878856658936, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:12:38 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.711515426635742, 'test_avg_loss': 0.7177878856658936, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:12:38 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.711515426635742, 'test_avg_loss': 0.7177878856658936, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:12:38 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.711515426635742, 'test_avg_loss': 0.7177878856658936, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:12:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:12:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:12:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:12:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:12:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:12:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:12:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:12:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:12:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:12:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:12:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:12:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:12:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:12:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.308670, avg_loss=0.701543, seen=200, correct=106, accuracy=0.530000
2025-09-11 07:12:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:12:46 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 0 with val results: {'val_total': 200, 'val_loss': 140.3086700439453, 'val_avg_loss': 0.7015433502197266, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-11 07:12:46 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 0 with val results: {'val_total': 200, 'val_loss': 140.3086700439453, 'val_avg_loss': 0.7015433502197266, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-11 07:12:46 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 0 with val results: {'val_total': 200, 'val_loss': 140.3086700439453, 'val_avg_loss': 0.7015433502197266, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-11 07:12:46 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 0 with val results: {'val_total': 200, 'val_loss': 140.3086700439453, 'val_avg_loss': 0.7015433502197266, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:12:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:12:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:12:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:12:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.585297, avg_loss=0.764632, seen=40, correct=15, accuracy=0.375000
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:12:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:12:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:12:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:12:48 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.585296630859375, 'test_avg_loss': 0.7646324157714843, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:12:48 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.585296630859375, 'test_avg_loss': 0.7646324157714843, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:12:48 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.585296630859375, 'test_avg_loss': 0.7646324157714843, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:12:48 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.585296630859375, 'test_avg_loss': 0.7646324157714843, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:12:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:12:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:12:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:12:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:12:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:12:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:12:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.874924, avg_loss=0.704375, seen=200, correct=97, accuracy=0.485000
2025-09-11 07:12:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:12:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:12:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:12:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:12:55 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.8749237060547, 'val_avg_loss': 0.7043746185302734, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-11 07:12:55 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.8749237060547, 'val_avg_loss': 0.7043746185302734, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-11 07:12:55 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.8749237060547, 'val_avg_loss': 0.7043746185302734, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-11 07:12:55 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.8749237060547, 'val_avg_loss': 0.7043746185302734, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:12:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:12:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:12:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:12:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.280476, avg_loss=0.732012, seen=40, correct=13, accuracy=0.325000
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:12:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:12:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:12:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:12:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:12:57 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.280475616455078, 'test_avg_loss': 0.7320118904113769, 'test_seen': 40, 'test_correct': 13, 'test_acc': 0.325}
2025-09-11 07:12:57 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.280475616455078, 'test_avg_loss': 0.7320118904113769, 'test_seen': 40, 'test_correct': 13, 'test_acc': 0.325}
2025-09-11 07:12:57 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.280475616455078, 'test_avg_loss': 0.7320118904113769, 'test_seen': 40, 'test_correct': 13, 'test_acc': 0.325}
2025-09-11 07:12:57 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.280475616455078, 'test_avg_loss': 0.7320118904113769, 'test_seen': 40, 'test_correct': 13, 'test_acc': 0.325}
2025-09-11 07:12:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:12:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:12:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:12:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:12:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:12:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:13:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:13:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:13:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:13:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.345840, avg_loss=0.716729, seen=200, correct=96, accuracy=0.480000
2025-09-11 07:13:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:13:06 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 2 with val results: {'val_total': 200, 'val_loss': 143.34584045410156, 'val_avg_loss': 0.7167292022705078, 'val_seen': 200, 'val_correct': 96, 'val_acc': 0.48}
2025-09-11 07:13:06 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 2 with val results: {'val_total': 200, 'val_loss': 143.34584045410156, 'val_avg_loss': 0.7167292022705078, 'val_seen': 200, 'val_correct': 96, 'val_acc': 0.48}
2025-09-11 07:13:06 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 2 with val results: {'val_total': 200, 'val_loss': 143.34584045410156, 'val_avg_loss': 0.7167292022705078, 'val_seen': 200, 'val_correct': 96, 'val_acc': 0.48}
2025-09-11 07:13:06 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 2 with val results: {'val_total': 200, 'val_loss': 143.34584045410156, 'val_avg_loss': 0.7167292022705078, 'val_seen': 200, 'val_correct': 96, 'val_acc': 0.48}
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)(20, 10, 2, 30)

(20, 10, 2, 30)
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:13:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:13:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:13:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:13:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.194038, avg_loss=0.679851, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:13:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:13:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:13:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:13:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1947MB
2025-09-11 07:13:08 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.19403839111328, 'test_avg_loss': 0.679850959777832, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:13:08 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.19403839111328, 'test_avg_loss': 0.679850959777832, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:13:08 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.19403839111328, 'test_avg_loss': 0.679850959777832, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:13:08 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.19403839111328, 'test_avg_loss': 0.679850959777832, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:13:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:13:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:13:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:13:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:13:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-11 07:13:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=40, total=161)
2025-09-11 07:13:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=40, total=161)
2025-09-11 07:13:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=40, total=161)
2025-09-11 07:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:13:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:13:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:13:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-11 07:13:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=40
2025-09-11 07:13:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=40
2025-09-11 07:13:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=40
2025-09-11 07:13:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=107.093353, avg_loss=0.665176, seen=161, correct=94, accuracy=0.583851
2025-09-11 07:13:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-11 07:13:16 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 0 with val results: {'val_total': 161, 'val_loss': 107.09335327148438, 'val_avg_loss': 0.6651761072763005, 'val_seen': 161, 'val_correct': 94, 'val_acc': 0.5838509316770186}
2025-09-11 07:13:16 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 0 with val results: {'val_total': 161, 'val_loss': 107.09335327148438, 'val_avg_loss': 0.6651761072763005, 'val_seen': 161, 'val_correct': 94, 'val_acc': 0.5838509316770186}
2025-09-11 07:13:16 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 0 with val results: {'val_total': 161, 'val_loss': 107.09335327148438, 'val_avg_loss': 0.6651761072763005, 'val_seen': 161, 'val_correct': 94, 'val_acc': 0.5838509316770186}
2025-09-11 07:13:16 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 0 with val results: {'val_total': 161, 'val_loss': 107.09335327148438, 'val_avg_loss': 0.6651761072763005, 'val_seen': 161, 'val_correct': 94, 'val_acc': 0.5838509316770186}
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:13:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:13:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:13:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:13:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.882998, avg_loss=0.672075, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:13:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-11 07:13:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-11 07:13:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-11 07:13:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1980MB
2025-09-11 07:13:18 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.882997512817383, 'test_avg_loss': 0.6720749378204346, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:13:18 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.882997512817383, 'test_avg_loss': 0.6720749378204346, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:13:18 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.882997512817383, 'test_avg_loss': 0.6720749378204346, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:13:18 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.882997512817383, 'test_avg_loss': 0.6720749378204346, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:13:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-11 07:13:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=40, total=161)
2025-09-11 07:13:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=40, total=161)
2025-09-11 07:13:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=40, total=161)
2025-09-11 07:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:13:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:13:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:13:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:13:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=40
2025-09-11 07:13:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-11 07:13:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=40
2025-09-11 07:13:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=40
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=107.964325, avg_loss=0.670586, seen=161, correct=91, accuracy=0.565217
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:25 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 1 with val results: {'val_total': 161, 'val_loss': 107.96432495117188, 'val_avg_loss': 0.6705858692619371, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-11 07:13:25 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 1 with val results: {'val_total': 161, 'val_loss': 107.96432495117188, 'val_avg_loss': 0.6705858692619371, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-11 07:13:25 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 1 with val results: {'val_total': 161, 'val_loss': 107.96432495117188, 'val_avg_loss': 0.6705858692619371, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-11 07:13:25 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 1 with val results: {'val_total': 161, 'val_loss': 107.96432495117188, 'val_avg_loss': 0.6705858692619371, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:13:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:13:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:13:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:13:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.269075, avg_loss=0.656727, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:13:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:27 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.269075393676758, 'test_avg_loss': 0.656726884841919, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:13:27 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.269075393676758, 'test_avg_loss': 0.656726884841919, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:13:27 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.269075393676758, 'test_avg_loss': 0.656726884841919, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:13:27 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.269075393676758, 'test_avg_loss': 0.656726884841919, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:13:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-11 07:13:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=40, total=161)
2025-09-11 07:13:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=40, total=161)
2025-09-11 07:13:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=40, total=161)
2025-09-11 07:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:13:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:13:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:13:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=40
2025-09-11 07:13:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-11 07:13:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=40
2025-09-11 07:13:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=40
2025-09-11 07:13:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=109.103653, avg_loss=0.677662, seen=161, correct=91, accuracy=0.565217
2025-09-11 07:13:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:34 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 2 with val results: {'val_total': 161, 'val_loss': 109.10365295410156, 'val_avg_loss': 0.6776624407087054, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-11 07:13:34 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 2 with val results: {'val_total': 161, 'val_loss': 109.10365295410156, 'val_avg_loss': 0.6776624407087054, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-11 07:13:34 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 2 with val results: {'val_total': 161, 'val_loss': 109.10365295410156, 'val_avg_loss': 0.6776624407087054, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-11 07:13:34 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 2 with val results: {'val_total': 161, 'val_loss': 109.10365295410156, 'val_avg_loss': 0.6776624407087054, 'val_seen': 161, 'val_correct': 91, 'val_acc': 0.5652173913043478}
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:13:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.642426, avg_loss=0.666061, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:13:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1955MB
2025-09-11 07:13:36 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.642425537109375, 'test_avg_loss': 0.6660606384277343, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:13:36 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.642425537109375, 'test_avg_loss': 0.6660606384277343, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:13:36 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.642425537109375, 'test_avg_loss': 0.6660606384277343, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:13:36 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.642425537109375, 'test_avg_loss': 0.6660606384277343, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:13:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:13:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:13:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:13:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:13:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-11 07:13:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=31, total=123)
2025-09-11 07:13:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=31, total=123)
2025-09-11 07:13:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=30, total=123)
2025-09-11 07:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:13:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:13:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:13:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-11 07:13:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=31
2025-09-11 07:13:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=31
2025-09-11 07:13:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=30
2025-09-11 07:13:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=84.272835, avg_loss=0.685145, seen=123, correct=68, accuracy=0.552846
2025-09-11 07:13:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:13:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:13:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:13:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:13:42 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 0 with val results: {'val_total': 123, 'val_loss': 84.27283477783203, 'val_avg_loss': 0.6851449981937564, 'val_seen': 123, 'val_correct': 68, 'val_acc': 0.5528455284552846}
2025-09-11 07:13:42 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 0 with val results: {'val_total': 123, 'val_loss': 84.27283477783203, 'val_avg_loss': 0.6851449981937564, 'val_seen': 123, 'val_correct': 68, 'val_acc': 0.5528455284552846}
2025-09-11 07:13:42 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 0 with val results: {'val_total': 123, 'val_loss': 84.27283477783203, 'val_avg_loss': 0.6851449981937564, 'val_seen': 123, 'val_correct': 68, 'val_acc': 0.5528455284552846}
2025-09-11 07:13:42 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 0 with val results: {'val_total': 123, 'val_loss': 84.27283477783203, 'val_avg_loss': 0.6851449981937564, 'val_seen': 123, 'val_correct': 68, 'val_acc': 0.5528455284552846}
2025-09-11 07:13:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:13:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:13:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:13:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:13:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:13:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:13:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:13:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:13:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:13:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.068777, avg_loss=0.726719, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:13:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:13:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:13:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:13:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:13:44 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.068777084350586, 'test_avg_loss': 0.7267194271087647, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:13:44 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.068777084350586, 'test_avg_loss': 0.7267194271087647, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:13:44 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.068777084350586, 'test_avg_loss': 0.7267194271087647, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:13:44 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.068777084350586, 'test_avg_loss': 0.7267194271087647, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:13:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=31, total=123)
2025-09-11 07:13:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-11 07:13:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=31, total=123)
2025-09-11 07:13:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=30, total=123)
2025-09-11 07:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:13:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:13:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:13:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-11 07:13:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=31
2025-09-11 07:13:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=31
2025-09-11 07:13:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=30
2025-09-11 07:13:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=85.854706, avg_loss=0.698006, seen=123, correct=61, accuracy=0.495935
2025-09-11 07:13:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:50 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 1 with val results: {'val_total': 123, 'val_loss': 85.85470581054688, 'val_avg_loss': 0.6980057382971291, 'val_seen': 123, 'val_correct': 61, 'val_acc': 0.4959349593495935}
2025-09-11 07:13:50 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 1 with val results: {'val_total': 123, 'val_loss': 85.85470581054688, 'val_avg_loss': 0.6980057382971291, 'val_seen': 123, 'val_correct': 61, 'val_acc': 0.4959349593495935}
2025-09-11 07:13:50 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 1 with val results: {'val_total': 123, 'val_loss': 85.85470581054688, 'val_avg_loss': 0.6980057382971291, 'val_seen': 123, 'val_correct': 61, 'val_acc': 0.4959349593495935}
2025-09-11 07:13:50 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 1 with val results: {'val_total': 123, 'val_loss': 85.85470581054688, 'val_avg_loss': 0.6980057382971291, 'val_seen': 123, 'val_correct': 61, 'val_acc': 0.4959349593495935}
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:13:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:13:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.348186, avg_loss=0.683705, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:52 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.348186492919922, 'test_avg_loss': 0.6837046623229981, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:13:52 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.348186492919922, 'test_avg_loss': 0.6837046623229981, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:13:52 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.348186492919922, 'test_avg_loss': 0.6837046623229981, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:13:52 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.348186492919922, 'test_avg_loss': 0.6837046623229981, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:13:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-11 07:13:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=31, total=123)
2025-09-11 07:13:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=31, total=123)
2025-09-11 07:13:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=30, total=123)
2025-09-11 07:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:13:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:13:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=31
2025-09-11 07:13:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-11 07:13:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=30
2025-09-11 07:13:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=31
2025-09-11 07:13:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=85.696899, avg_loss=0.696723, seen=123, correct=67, accuracy=0.544715
2025-09-11 07:13:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:13:57 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 2 with val results: {'val_total': 123, 'val_loss': 85.6968994140625, 'val_avg_loss': 0.6967227594639228, 'val_seen': 123, 'val_correct': 67, 'val_acc': 0.5447154471544715}
2025-09-11 07:13:57 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 2 with val results: {'val_total': 123, 'val_loss': 85.6968994140625, 'val_avg_loss': 0.6967227594639228, 'val_seen': 123, 'val_correct': 67, 'val_acc': 0.5447154471544715}
2025-09-11 07:13:57 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 2 with val results: {'val_total': 123, 'val_loss': 85.6968994140625, 'val_avg_loss': 0.6967227594639228, 'val_seen': 123, 'val_correct': 67, 'val_acc': 0.5447154471544715}
2025-09-11 07:13:57 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 2 with val results: {'val_total': 123, 'val_loss': 85.6968994140625, 'val_avg_loss': 0.6967227594639228, 'val_seen': 123, 'val_correct': 67, 'val_acc': 0.5447154471544715}
2025-09-11 07:13:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:13:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:13:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:13:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:13:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:13:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:13:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:13:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:13:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:13:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:13:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.473186, avg_loss=0.661830, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:13:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:14:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:14:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:14:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1964MB
2025-09-11 07:14:00 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.473186492919922, 'test_avg_loss': 0.661829662322998, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:14:00 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.473186492919922, 'test_avg_loss': 0.661829662322998, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:14:00 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.473186492919922, 'test_avg_loss': 0.661829662322998, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:14:00 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.473186492919922, 'test_avg_loss': 0.661829662322998, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:14:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-11 07:14:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=19, total=75)
2025-09-11 07:14:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=19, total=75)
2025-09-11 07:14:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=75)
2025-09-11 07:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:14:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=19
2025-09-11 07:14:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=19
2025-09-11 07:14:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-11 07:14:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:14:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=51.574310, avg_loss=0.687657, seen=75, correct=40, accuracy=0.533333
2025-09-11 07:14:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=1997MB
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-11 07:14:04 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 0 with val results: {'val_total': 75, 'val_loss': 51.574310302734375, 'val_avg_loss': 0.687657470703125, 'val_seen': 75, 'val_correct': 40, 'val_acc': 0.5333333333333333}
2025-09-11 07:14:04 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 0 with val results: {'val_total': 75, 'val_loss': 51.574310302734375, 'val_avg_loss': 0.687657470703125, 'val_seen': 75, 'val_correct': 40, 'val_acc': 0.5333333333333333}
2025-09-11 07:14:04 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 0 with val results: {'val_total': 75, 'val_loss': 51.574310302734375, 'val_avg_loss': 0.687657470703125, 'val_seen': 75, 'val_correct': 40, 'val_acc': 0.5333333333333333}
2025-09-11 07:14:04 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 0 with val results: {'val_total': 75, 'val_loss': 51.574310302734375, 'val_avg_loss': 0.687657470703125, 'val_seen': 75, 'val_correct': 40, 'val_acc': 0.5333333333333333}
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:14:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:14:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:14:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:14:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.546741, avg_loss=0.688669, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-11 07:14:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-11 07:14:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-11 07:14:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1997MB
2025-09-11 07:14:06 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.546741485595703, 'test_avg_loss': 0.6886685371398926, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:06 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.546741485595703, 'test_avg_loss': 0.6886685371398926, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:06 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.546741485595703, 'test_avg_loss': 0.6886685371398926, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:06 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.546741485595703, 'test_avg_loss': 0.6886685371398926, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-11 07:14:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=75)
2025-09-11 07:14:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=19, total=75)
2025-09-11 07:14:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=19, total=75)
2025-09-11 07:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:14:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=19
2025-09-11 07:14:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-11 07:14:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=19
2025-09-11 07:14:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:14:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=50.610405, avg_loss=0.674805, seen=75, correct=47, accuracy=0.626667
2025-09-11 07:14:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:10 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 1 with val results: {'val_total': 75, 'val_loss': 50.61040496826172, 'val_avg_loss': 0.674805399576823, 'val_seen': 75, 'val_correct': 47, 'val_acc': 0.6266666666666667}
2025-09-11 07:14:10 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 1 with val results: {'val_total': 75, 'val_loss': 50.61040496826172, 'val_avg_loss': 0.674805399576823, 'val_seen': 75, 'val_correct': 47, 'val_acc': 0.6266666666666667}
2025-09-11 07:14:10 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 1 with val results: {'val_total': 75, 'val_loss': 50.61040496826172, 'val_avg_loss': 0.674805399576823, 'val_seen': 75, 'val_correct': 47, 'val_acc': 0.6266666666666667}
2025-09-11 07:14:10 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 1 with val results: {'val_total': 75, 'val_loss': 50.61040496826172, 'val_avg_loss': 0.674805399576823, 'val_seen': 75, 'val_correct': 47, 'val_acc': 0.6266666666666667}
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:14:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:14:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:14:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:14:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:14:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.532156, avg_loss=0.688304, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:14:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:12 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.532155990600586, 'test_avg_loss': 0.6883038997650146, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:12 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.532155990600586, 'test_avg_loss': 0.6883038997650146, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:12 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.532155990600586, 'test_avg_loss': 0.6883038997650146, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:12 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.532155990600586, 'test_avg_loss': 0.6883038997650146, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=75)
2025-09-11 07:14:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=19, total=75)
2025-09-11 07:14:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=19, total=75)
2025-09-11 07:14:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-11 07:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:14:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-11 07:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=19
2025-09-11 07:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=19
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=51.703156, avg_loss=0.689375, seen=75, correct=42, accuracy=0.560000
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:16 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 2 with val results: {'val_total': 75, 'val_loss': 51.703155517578125, 'val_avg_loss': 0.6893754069010417, 'val_seen': 75, 'val_correct': 42, 'val_acc': 0.56}
2025-09-11 07:14:16 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 2 with val results: {'val_total': 75, 'val_loss': 51.703155517578125, 'val_avg_loss': 0.6893754069010417, 'val_seen': 75, 'val_correct': 42, 'val_acc': 0.56}
2025-09-11 07:14:16 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 2 with val results: {'val_total': 75, 'val_loss': 51.703155517578125, 'val_avg_loss': 0.6893754069010417, 'val_seen': 75, 'val_correct': 42, 'val_acc': 0.56}
2025-09-11 07:14:16 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 2 with val results: {'val_total': 75, 'val_loss': 51.703155517578125, 'val_avg_loss': 0.6893754069010417, 'val_seen': 75, 'val_correct': 42, 'val_acc': 0.56}
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:14:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:14:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:14:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:14:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.993690, avg_loss=0.699842, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:14:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1972MB
2025-09-11 07:14:18 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.993690490722656, 'test_avg_loss': 0.6998422622680665, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:14:18 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.993690490722656, 'test_avg_loss': 0.6998422622680665, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:14:18 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.993690490722656, 'test_avg_loss': 0.6998422622680665, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:14:18 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.993690490722656, 'test_avg_loss': 0.6998422622680665, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:14:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:14:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:14:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:14:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:14:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:14:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.951599, avg_loss=0.699758, seen=200, correct=102, accuracy=0.510000
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2052MB allocated=2006MB
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:14:26 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 0 with val results: {'val_total': 200, 'val_loss': 139.95159912109375, 'val_avg_loss': 0.6997579956054687, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-11 07:14:26 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 0 with val results: {'val_total': 200, 'val_loss': 139.95159912109375, 'val_avg_loss': 0.6997579956054687, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-11 07:14:26 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 0 with val results: {'val_total': 200, 'val_loss': 139.95159912109375, 'val_avg_loss': 0.6997579956054687, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-11 07:14:26 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 0 with val results: {'val_total': 200, 'val_loss': 139.95159912109375, 'val_avg_loss': 0.6997579956054687, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:14:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:14:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:14:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:14:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.323904, avg_loss=0.683098, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:14:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:14:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:14:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:14:28 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.323904037475586, 'test_avg_loss': 0.6830976009368896, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:14:28 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.323904037475586, 'test_avg_loss': 0.6830976009368896, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:14:28 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.323904037475586, 'test_avg_loss': 0.6830976009368896, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:14:28 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.323904037475586, 'test_avg_loss': 0.6830976009368896, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:14:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:14:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:14:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:14:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:14:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:14:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:14:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:14:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:14:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:14:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.487000, avg_loss=0.702435, seen=200, correct=101, accuracy=0.505000
2025-09-11 07:14:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:35 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.48699951171875, 'val_avg_loss': 0.7024349975585937, 'val_seen': 200, 'val_correct': 101, 'val_acc': 0.505}
2025-09-11 07:14:35 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.48699951171875, 'val_avg_loss': 0.7024349975585937, 'val_seen': 200, 'val_correct': 101, 'val_acc': 0.505}
2025-09-11 07:14:35 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.48699951171875, 'val_avg_loss': 0.7024349975585937, 'val_seen': 200, 'val_correct': 101, 'val_acc': 0.505}
2025-09-11 07:14:35 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.48699951171875, 'val_avg_loss': 0.7024349975585937, 'val_seen': 200, 'val_correct': 101, 'val_acc': 0.505}
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:14:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:14:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:14:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:14:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.339119, avg_loss=0.683478, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:14:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:37 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.33911895751953, 'test_avg_loss': 0.6834779739379883, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:37 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.33911895751953, 'test_avg_loss': 0.6834779739379883, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:37 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.33911895751953, 'test_avg_loss': 0.6834779739379883, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:37 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.33911895751953, 'test_avg_loss': 0.6834779739379883, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:14:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:14:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:14:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:14:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:14:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:14:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:14:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:14:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:14:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:14:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.635986, avg_loss=0.713180, seen=200, correct=100, accuracy=0.500000
2025-09-11 07:14:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:45 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.635986328125, 'val_avg_loss': 0.713179931640625, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-11 07:14:45 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.635986328125, 'val_avg_loss': 0.713179931640625, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-11 07:14:45 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.635986328125, 'val_avg_loss': 0.713179931640625, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-11 07:14:45 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.635986328125, 'val_avg_loss': 0.713179931640625, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:14:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:14:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:14:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.358049, avg_loss=0.733951, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1981MB
2025-09-11 07:14:47 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.358049392700195, 'test_avg_loss': 0.7339512348175049, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:14:47 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.358049392700195, 'test_avg_loss': 0.7339512348175049, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:14:47 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.358049392700195, 'test_avg_loss': 0.7339512348175049, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:14:47 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.358049392700195, 'test_avg_loss': 0.7339512348175049, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:14:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-11 07:14:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=42, total=170)
2025-09-11 07:14:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=43, total=170)
2025-09-11 07:14:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=42, total=170)
2025-09-11 07:14:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:14:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=42
2025-09-11 07:14:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=43
2025-09-11 07:14:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=42
2025-09-11 07:14:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-11 07:14:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=115.705246, avg_loss=0.680619, seen=170, correct=96, accuracy=0.564706
2025-09-11 07:14:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-11 07:14:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-11 07:14:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-11 07:14:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-11 07:14:53 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 0 with val results: {'val_total': 170, 'val_loss': 115.70524597167969, 'val_avg_loss': 0.680619093951057, 'val_seen': 170, 'val_correct': 96, 'val_acc': 0.5647058823529412}
2025-09-11 07:14:53 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 0 with val results: {'val_total': 170, 'val_loss': 115.70524597167969, 'val_avg_loss': 0.680619093951057, 'val_seen': 170, 'val_correct': 96, 'val_acc': 0.5647058823529412}
2025-09-11 07:14:53 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 0 with val results: {'val_total': 170, 'val_loss': 115.70524597167969, 'val_avg_loss': 0.680619093951057, 'val_seen': 170, 'val_correct': 96, 'val_acc': 0.5647058823529412}
2025-09-11 07:14:53 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 0 with val results: {'val_total': 170, 'val_loss': 115.70524597167969, 'val_avg_loss': 0.680619093951057, 'val_seen': 170, 'val_correct': 96, 'val_acc': 0.5647058823529412}
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:14:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:14:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:14:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.286074, avg_loss=0.682152, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2014MB
2025-09-11 07:14:56 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.286073684692383, 'test_avg_loss': 0.6821518421173096, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:14:56 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.286073684692383, 'test_avg_loss': 0.6821518421173096, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:14:56 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.286073684692383, 'test_avg_loss': 0.6821518421173096, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:14:56 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.286073684692383, 'test_avg_loss': 0.6821518421173096, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=42, total=170)
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=42, total=170)
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=43, total=170)
2025-09-11 07:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:14:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=42
2025-09-11 07:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=43
2025-09-11 07:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-11 07:15:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=42
2025-09-11 07:15:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=116.979660, avg_loss=0.688116, seen=170, correct=96, accuracy=0.564706
2025-09-11 07:15:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:02 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 1 with val results: {'val_total': 170, 'val_loss': 116.97966003417969, 'val_avg_loss': 0.6881156472598805, 'val_seen': 170, 'val_correct': 96, 'val_acc': 0.5647058823529412}
2025-09-11 07:15:02 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 1 with val results: {'val_total': 170, 'val_loss': 116.97966003417969, 'val_avg_loss': 0.6881156472598805, 'val_seen': 170, 'val_correct': 96, 'val_acc': 0.5647058823529412}
2025-09-11 07:15:02 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 1 with val results: {'val_total': 170, 'val_loss': 116.97966003417969, 'val_avg_loss': 0.6881156472598805, 'val_seen': 170, 'val_correct': 96, 'val_acc': 0.5647058823529412}
2025-09-11 07:15:02 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 1 with val results: {'val_total': 170, 'val_loss': 116.97966003417969, 'val_avg_loss': 0.6881156472598805, 'val_seen': 170, 'val_correct': 96, 'val_acc': 0.5647058823529412}
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:15:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:15:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:15:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:15:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.172911, avg_loss=0.679323, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:15:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:04 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.172910690307617, 'test_avg_loss': 0.6793227672576905, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:15:04 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.172910690307617, 'test_avg_loss': 0.6793227672576905, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:15:04 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.172910690307617, 'test_avg_loss': 0.6793227672576905, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:15:04 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.172910690307617, 'test_avg_loss': 0.6793227672576905, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:15:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-11 07:15:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=42, total=170)
2025-09-11 07:15:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=42, total=170)
2025-09-11 07:15:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=43, total=170)
2025-09-11 07:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:15:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=42
2025-09-11 07:15:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=43
2025-09-11 07:15:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-11 07:15:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=42
2025-09-11 07:15:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=118.787796, avg_loss=0.698752, seen=170, correct=85, accuracy=0.500000
2025-09-11 07:15:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:11 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 2 with val results: {'val_total': 170, 'val_loss': 118.78779602050781, 'val_avg_loss': 0.6987517412971048, 'val_seen': 170, 'val_correct': 85, 'val_acc': 0.5}
2025-09-11 07:15:11 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 2 with val results: {'val_total': 170, 'val_loss': 118.78779602050781, 'val_avg_loss': 0.6987517412971048, 'val_seen': 170, 'val_correct': 85, 'val_acc': 0.5}
2025-09-11 07:15:11 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 2 with val results: {'val_total': 170, 'val_loss': 118.78779602050781, 'val_avg_loss': 0.6987517412971048, 'val_seen': 170, 'val_correct': 85, 'val_acc': 0.5}
2025-09-11 07:15:11 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 2 with val results: {'val_total': 170, 'val_loss': 118.78779602050781, 'val_avg_loss': 0.6987517412971048, 'val_seen': 170, 'val_correct': 85, 'val_acc': 0.5}
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:15:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:15:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:15:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:15:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.896259, avg_loss=0.672406, seen=40, correct=26, accuracy=0.650000
2025-09-11 07:15:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1989MB
2025-09-11 07:15:14 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.896259307861328, 'test_avg_loss': 0.6724064826965332, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:15:14 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.896259307861328, 'test_avg_loss': 0.6724064826965332, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:15:14 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.896259307861328, 'test_avg_loss': 0.6724064826965332, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:15:14 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.896259307861328, 'test_avg_loss': 0.6724064826965332, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:15:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:15:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:15:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:15:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:15:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-11 07:15:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=48, total=193)
2025-09-11 07:15:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=48, total=193)
2025-09-11 07:15:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=48, total=193)
2025-09-11 07:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:15:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=48
2025-09-11 07:15:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=48
2025-09-11 07:15:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=48
2025-09-11 07:15:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-11 07:15:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=135.693115, avg_loss=0.703073, seen=193, correct=94, accuracy=0.487047
2025-09-11 07:15:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-11 07:15:22 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 0 with val results: {'val_total': 193, 'val_loss': 135.693115234375, 'val_avg_loss': 0.7030731359294041, 'val_seen': 193, 'val_correct': 94, 'val_acc': 0.48704663212435234}
2025-09-11 07:15:22 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 0 with val results: {'val_total': 193, 'val_loss': 135.693115234375, 'val_avg_loss': 0.7030731359294041, 'val_seen': 193, 'val_correct': 94, 'val_acc': 0.48704663212435234}
2025-09-11 07:15:22 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 0 with val results: {'val_total': 193, 'val_loss': 135.693115234375, 'val_avg_loss': 0.7030731359294041, 'val_seen': 193, 'val_correct': 94, 'val_acc': 0.48704663212435234}
2025-09-11 07:15:22 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 0 with val results: {'val_total': 193, 'val_loss': 135.693115234375, 'val_avg_loss': 0.7030731359294041, 'val_seen': 193, 'val_correct': 94, 'val_acc': 0.48704663212435234}
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:15:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:15:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:15:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:15:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.496401, avg_loss=0.662410, seen=40, correct=24, accuracy=0.600000
2025-09-11 07:15:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-11 07:15:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-11 07:15:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-11 07:15:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2023MB
2025-09-11 07:15:24 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.496400833129883, 'test_avg_loss': 0.6624100208282471, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:15:24 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.496400833129883, 'test_avg_loss': 0.6624100208282471, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:15:24 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.496400833129883, 'test_avg_loss': 0.6624100208282471, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:15:24 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.496400833129883, 'test_avg_loss': 0.6624100208282471, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:15:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=48, total=193)
2025-09-11 07:15:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-11 07:15:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=48, total=193)
2025-09-11 07:15:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=48, total=193)
2025-09-11 07:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:15:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:15:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=48
2025-09-11 07:15:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-11 07:15:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=48
2025-09-11 07:15:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=48
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=136.509369, avg_loss=0.707302, seen=193, correct=91, accuracy=0.471503
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:32 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 1 with val results: {'val_total': 193, 'val_loss': 136.50936889648438, 'val_avg_loss': 0.7073024295154631, 'val_seen': 193, 'val_correct': 91, 'val_acc': 0.47150259067357514}
2025-09-11 07:15:32 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 1 with val results: {'val_total': 193, 'val_loss': 136.50936889648438, 'val_avg_loss': 0.7073024295154631, 'val_seen': 193, 'val_correct': 91, 'val_acc': 0.47150259067357514}
2025-09-11 07:15:32 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 1 with val results: {'val_total': 193, 'val_loss': 136.50936889648438, 'val_avg_loss': 0.7073024295154631, 'val_seen': 193, 'val_correct': 91, 'val_acc': 0.47150259067357514}
2025-09-11 07:15:32 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 1 with val results: {'val_total': 193, 'val_loss': 136.50936889648438, 'val_avg_loss': 0.7073024295154631, 'val_seen': 193, 'val_correct': 91, 'val_acc': 0.47150259067357514}
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:15:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:15:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:15:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:15:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.582954, avg_loss=0.664574, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:15:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:34 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.58295440673828, 'test_avg_loss': 0.664573860168457, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:15:34 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.58295440673828, 'test_avg_loss': 0.664573860168457, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:15:34 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.58295440673828, 'test_avg_loss': 0.664573860168457, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:15:34 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.58295440673828, 'test_avg_loss': 0.664573860168457, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:15:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-11 07:15:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=48, total=193)
2025-09-11 07:15:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=48, total=193)
2025-09-11 07:15:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=48, total=193)
2025-09-11 07:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:15:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)2025-09-11 07:15:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:15:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=48
2025-09-11 07:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=48
2025-09-11 07:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-11 07:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=48
2025-09-11 07:15:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=140.843567, avg_loss=0.729759, seen=193, correct=78, accuracy=0.404145
2025-09-11 07:15:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:42 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 2 with val results: {'val_total': 193, 'val_loss': 140.84356689453125, 'val_avg_loss': 0.7297594139613018, 'val_seen': 193, 'val_correct': 78, 'val_acc': 0.40414507772020725}
2025-09-11 07:15:42 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 2 with val results: {'val_total': 193, 'val_loss': 140.84356689453125, 'val_avg_loss': 0.7297594139613018, 'val_seen': 193, 'val_correct': 78, 'val_acc': 0.40414507772020725}
2025-09-11 07:15:42 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 2 with val results: {'val_total': 193, 'val_loss': 140.84356689453125, 'val_avg_loss': 0.7297594139613018, 'val_seen': 193, 'val_correct': 78, 'val_acc': 0.40414507772020725}
2025-09-11 07:15:42 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 2 with val results: {'val_total': 193, 'val_loss': 140.84356689453125, 'val_avg_loss': 0.7297594139613018, 'val_seen': 193, 'val_correct': 78, 'val_acc': 0.40414507772020725}
2025-09-11 07:15:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:15:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:15:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:15:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:15:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:15:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:15:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:15:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:15:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:15:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.235592, avg_loss=0.680890, seen=40, correct=24, accuracy=0.600000
2025-09-11 07:15:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=1998MB
2025-09-11 07:15:45 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.235591888427734, 'test_avg_loss': 0.6808897972106933, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:15:45 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.235591888427734, 'test_avg_loss': 0.6808897972106933, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:15:45 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.235591888427734, 'test_avg_loss': 0.6808897972106933, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:15:45 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.235591888427734, 'test_avg_loss': 0.6808897972106933, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:15:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:15:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:15:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:15:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:15:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-11 07:15:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=112)
2025-09-11 07:15:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=28, total=112)
2025-09-11 07:15:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=28, total=112)
2025-09-11 07:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:15:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=28
2025-09-11 07:15:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:15:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=28
2025-09-11 07:15:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=73.127350, avg_loss=0.652923, seen=112, correct=70, accuracy=0.625000
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-11 07:15:50 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 0 with val results: {'val_total': 112, 'val_loss': 73.12734985351562, 'val_avg_loss': 0.6529227665492466, 'val_seen': 112, 'val_correct': 70, 'val_acc': 0.625}
2025-09-11 07:15:50 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 0 with val results: {'val_total': 112, 'val_loss': 73.12734985351562, 'val_avg_loss': 0.6529227665492466, 'val_seen': 112, 'val_correct': 70, 'val_acc': 0.625}
2025-09-11 07:15:50 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 0 with val results: {'val_total': 112, 'val_loss': 73.12734985351562, 'val_avg_loss': 0.6529227665492466, 'val_seen': 112, 'val_correct': 70, 'val_acc': 0.625}
2025-09-11 07:15:50 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 0 with val results: {'val_total': 112, 'val_loss': 73.12734985351562, 'val_avg_loss': 0.6529227665492466, 'val_seen': 112, 'val_correct': 70, 'val_acc': 0.625}
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.671236, avg_loss=0.691781, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-11 07:15:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-11 07:15:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-11 07:15:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2031MB
2025-09-11 07:15:52 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.671236038208008, 'test_avg_loss': 0.6917809009552002, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:15:52 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.671236038208008, 'test_avg_loss': 0.6917809009552002, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:15:52 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.671236038208008, 'test_avg_loss': 0.6917809009552002, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:15:52 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.671236038208008, 'test_avg_loss': 0.6917809009552002, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:15:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-11 07:15:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=28, total=112)
2025-09-11 07:15:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=28, total=112)
2025-09-11 07:15:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=112)
2025-09-11 07:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:15:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:15:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:15:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:15:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:15:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=28
2025-09-11 07:15:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=28
2025-09-11 07:15:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=74.802757, avg_loss=0.667882, seen=112, correct=62, accuracy=0.553571
2025-09-11 07:15:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:15:57 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 1 with val results: {'val_total': 112, 'val_loss': 74.8027572631836, 'val_avg_loss': 0.6678817612784249, 'val_seen': 112, 'val_correct': 62, 'val_acc': 0.5535714285714286}
2025-09-11 07:15:57 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 1 with val results: {'val_total': 112, 'val_loss': 74.8027572631836, 'val_avg_loss': 0.6678817612784249, 'val_seen': 112, 'val_correct': 62, 'val_acc': 0.5535714285714286}
2025-09-11 07:15:57 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 1 with val results: {'val_total': 112, 'val_loss': 74.8027572631836, 'val_avg_loss': 0.6678817612784249, 'val_seen': 112, 'val_correct': 62, 'val_acc': 0.5535714285714286}
2025-09-11 07:15:57 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 1 with val results: {'val_total': 112, 'val_loss': 74.8027572631836, 'val_avg_loss': 0.6678817612784249, 'val_seen': 112, 'val_correct': 62, 'val_acc': 0.5535714285714286}
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:15:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:15:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:15:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:15:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:15:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:15:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.175299, avg_loss=0.704382, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:15:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:15:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:00 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.1752986907959, 'test_avg_loss': 0.7043824672698975, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:00 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.1752986907959, 'test_avg_loss': 0.7043824672698975, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:00 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.1752986907959, 'test_avg_loss': 0.7043824672698975, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:00 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.1752986907959, 'test_avg_loss': 0.7043824672698975, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-11 07:16:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=28, total=112)
2025-09-11 07:16:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=112)
2025-09-11 07:16:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=28, total=112)
2025-09-11 07:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:16:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:16:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=28
2025-09-11 07:16:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:16:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=28
2025-09-11 07:16:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:16:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=77.812515, avg_loss=0.694755, seen=112, correct=54, accuracy=0.482143
2025-09-11 07:16:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:04 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 2 with val results: {'val_total': 112, 'val_loss': 77.81251525878906, 'val_avg_loss': 0.6947546005249023, 'val_seen': 112, 'val_correct': 54, 'val_acc': 0.48214285714285715}
2025-09-11 07:16:04 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 2 with val results: {'val_total': 112, 'val_loss': 77.81251525878906, 'val_avg_loss': 0.6947546005249023, 'val_seen': 112, 'val_correct': 54, 'val_acc': 0.48214285714285715}
2025-09-11 07:16:04 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 2 with val results: {'val_total': 112, 'val_loss': 77.81251525878906, 'val_avg_loss': 0.6947546005249023, 'val_seen': 112, 'val_correct': 54, 'val_acc': 0.48214285714285715}
2025-09-11 07:16:04 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 2 with val results: {'val_total': 112, 'val_loss': 77.81251525878906, 'val_avg_loss': 0.6947546005249023, 'val_seen': 112, 'val_correct': 54, 'val_acc': 0.48214285714285715}
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:16:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:16:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:16:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:16:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:16:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.345327, avg_loss=0.733633, seen=40, correct=15, accuracy=0.375000
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2006MB
2025-09-11 07:16:07 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.345327377319336, 'test_avg_loss': 0.7336331844329834, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:16:07 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.345327377319336, 'test_avg_loss': 0.7336331844329834, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:16:07 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.345327377319336, 'test_avg_loss': 0.7336331844329834, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:16:07 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.345327377319336, 'test_avg_loss': 0.7336331844329834, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:16:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-11 07:16:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=18, total=74)
2025-09-11 07:16:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=74)
2025-09-11 07:16:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=19, total=74)
2025-09-11 07:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:16:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-11 07:16:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=19
2025-09-11 07:16:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:16:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=18
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.918137, avg_loss=0.701596, seen=74, correct=39, accuracy=0.527027
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-11 07:16:11 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 0 with val results: {'val_total': 74, 'val_loss': 51.91813659667969, 'val_avg_loss': 0.7015964404956715, 'val_seen': 74, 'val_correct': 39, 'val_acc': 0.527027027027027}
2025-09-11 07:16:11 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 0 with val results: {'val_total': 74, 'val_loss': 51.91813659667969, 'val_avg_loss': 0.7015964404956715, 'val_seen': 74, 'val_correct': 39, 'val_acc': 0.527027027027027}
2025-09-11 07:16:11 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 0 with val results: {'val_total': 74, 'val_loss': 51.91813659667969, 'val_avg_loss': 0.7015964404956715, 'val_seen': 74, 'val_correct': 39, 'val_acc': 0.527027027027027}
2025-09-11 07:16:11 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 0 with val results: {'val_total': 74, 'val_loss': 51.91813659667969, 'val_avg_loss': 0.7015964404956715, 'val_seen': 74, 'val_correct': 39, 'val_acc': 0.527027027027027}
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:16:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.863705, avg_loss=0.671593, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-11 07:16:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-11 07:16:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-11 07:16:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2040MB
2025-09-11 07:16:13 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.863704681396484, 'test_avg_loss': 0.6715926170349121, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:16:13 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.863704681396484, 'test_avg_loss': 0.6715926170349121, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:16:13 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.863704681396484, 'test_avg_loss': 0.6715926170349121, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:16:13 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.863704681396484, 'test_avg_loss': 0.6715926170349121, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:16:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-11 07:16:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=19, total=74)
2025-09-11 07:16:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=18, total=74)
2025-09-11 07:16:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=74)
2025-09-11 07:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:16:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=19
2025-09-11 07:16:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:16:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-11 07:16:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=18
2025-09-11 07:16:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.732185, avg_loss=0.699084, seen=74, correct=40, accuracy=0.540541
2025-09-11 07:16:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:17 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 1 with val results: {'val_total': 74, 'val_loss': 51.73218536376953, 'val_avg_loss': 0.6990835859968856, 'val_seen': 74, 'val_correct': 40, 'val_acc': 0.5405405405405406}
2025-09-11 07:16:17 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 1 with val results: {'val_total': 74, 'val_loss': 51.73218536376953, 'val_avg_loss': 0.6990835859968856, 'val_seen': 74, 'val_correct': 40, 'val_acc': 0.5405405405405406}
2025-09-11 07:16:17 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 1 with val results: {'val_total': 74, 'val_loss': 51.73218536376953, 'val_avg_loss': 0.6990835859968856, 'val_seen': 74, 'val_correct': 40, 'val_acc': 0.5405405405405406}
2025-09-11 07:16:17 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 1 with val results: {'val_total': 74, 'val_loss': 51.73218536376953, 'val_avg_loss': 0.6990835859968856, 'val_seen': 74, 'val_correct': 40, 'val_acc': 0.5405405405405406}
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:16:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:16:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:16:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:16:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.664066, avg_loss=0.691602, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:16:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:19 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.664066314697266, 'test_avg_loss': 0.6916016578674317, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:16:19 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.664066314697266, 'test_avg_loss': 0.6916016578674317, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:16:19 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.664066314697266, 'test_avg_loss': 0.6916016578674317, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:16:19 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.664066314697266, 'test_avg_loss': 0.6916016578674317, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:16:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-11 07:16:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=18, total=74)
2025-09-11 07:16:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=18, total=74)
2025-09-11 07:16:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=19, total=74)
2025-09-11 07:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:16:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=18
2025-09-11 07:16:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=19
2025-09-11 07:16:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-11 07:16:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=18
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.004700, avg_loss=0.702766, seen=74, correct=34, accuracy=0.459459
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:23 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 2 with val results: {'val_total': 74, 'val_loss': 52.00469970703125, 'val_avg_loss': 0.7027662122571791, 'val_seen': 74, 'val_correct': 34, 'val_acc': 0.4594594594594595}
2025-09-11 07:16:23 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 2 with val results: {'val_total': 74, 'val_loss': 52.00469970703125, 'val_avg_loss': 0.7027662122571791, 'val_seen': 74, 'val_correct': 34, 'val_acc': 0.4594594594594595}
2025-09-11 07:16:23 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 2 with val results: {'val_total': 74, 'val_loss': 52.00469970703125, 'val_avg_loss': 0.7027662122571791, 'val_seen': 74, 'val_correct': 34, 'val_acc': 0.4594594594594595}
2025-09-11 07:16:23 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 2 with val results: {'val_total': 74, 'val_loss': 52.00469970703125, 'val_avg_loss': 0.7027662122571791, 'val_seen': 74, 'val_correct': 34, 'val_acc': 0.4594594594594595}
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:16:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:16:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:16:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:16:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.803291, avg_loss=0.695082, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:16:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2050MB allocated=2015MB
2025-09-11 07:16:25 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.80329132080078, 'test_avg_loss': 0.6950822830200195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:25 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.80329132080078, 'test_avg_loss': 0.6950822830200195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:25 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.80329132080078, 'test_avg_loss': 0.6950822830200195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:25 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.80329132080078, 'test_avg_loss': 0.6950822830200195, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:16:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:16:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:16:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:16:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:16:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.310379, avg_loss=0.676552, seen=200, correct=121, accuracy=0.605000
2025-09-11 07:16:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-11 07:16:34 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 0 with val results: {'val_total': 200, 'val_loss': 135.3103790283203, 'val_avg_loss': 0.6765518951416015, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-11 07:16:34 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 0 with val results: {'val_total': 200, 'val_loss': 135.3103790283203, 'val_avg_loss': 0.6765518951416015, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-11 07:16:34 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 0 with val results: {'val_total': 200, 'val_loss': 135.3103790283203, 'val_avg_loss': 0.6765518951416015, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-11 07:16:34 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 0 with val results: {'val_total': 200, 'val_loss': 135.3103790283203, 'val_avg_loss': 0.6765518951416015, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:16:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:16:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:16:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.867416, avg_loss=0.671685, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-11 07:16:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-11 07:16:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-11 07:16:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2048MB
2025-09-11 07:16:37 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.867416381835938, 'test_avg_loss': 0.6716854095458984, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:16:37 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.867416381835938, 'test_avg_loss': 0.6716854095458984, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:16:37 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.867416381835938, 'test_avg_loss': 0.6716854095458984, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:16:37 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.867416381835938, 'test_avg_loss': 0.6716854095458984, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:16:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:16:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:16:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:16:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:16:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:16:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:16:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.087021, avg_loss=0.685435, seen=200, correct=113, accuracy=0.565000
2025-09-11 07:16:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:45 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.08702087402344, 'val_avg_loss': 0.6854351043701172, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:16:45 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.08702087402344, 'val_avg_loss': 0.6854351043701172, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:16:45 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.08702087402344, 'val_avg_loss': 0.6854351043701172, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:16:45 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.08702087402344, 'val_avg_loss': 0.6854351043701172, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:16:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:16:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:16:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:16:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:16:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:16:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.821384, avg_loss=0.645535, seen=40, correct=27, accuracy=0.675000
2025-09-11 07:16:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:48 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.82138442993164, 'test_avg_loss': 0.645534610748291, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:16:48 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.82138442993164, 'test_avg_loss': 0.645534610748291, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:16:48 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.82138442993164, 'test_avg_loss': 0.645534610748291, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:16:48 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.82138442993164, 'test_avg_loss': 0.645534610748291, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:16:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:16:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:16:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:16:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:16:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:16:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:16:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:16:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:16:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:16:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.277130, avg_loss=0.696386, seen=200, correct=107, accuracy=0.535000
2025-09-11 07:16:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:55 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.27713012695312, 'val_avg_loss': 0.6963856506347657, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-11 07:16:55 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.27713012695312, 'val_avg_loss': 0.6963856506347657, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-11 07:16:55 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.27713012695312, 'val_avg_loss': 0.6963856506347657, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-11 07:16:55 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.27713012695312, 'val_avg_loss': 0.6963856506347657, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-11 07:16:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:16:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:16:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:16:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:16:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:16:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:16:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:16:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:16:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:16:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:16:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:16:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:16:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.029640, avg_loss=0.700741, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:16:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:16:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:16:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2023MB
2025-09-11 07:16:58 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.029640197753906, 'test_avg_loss': 0.7007410049438476, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:58 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.029640197753906, 'test_avg_loss': 0.7007410049438476, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:58 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.029640197753906, 'test_avg_loss': 0.7007410049438476, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:58 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.029640197753906, 'test_avg_loss': 0.7007410049438476, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:16:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:16:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:16:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:16:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:16:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:16:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:16:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:17:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:17:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:17:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:17:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.885071, avg_loss=0.689425, seen=200, correct=109, accuracy=0.545000
2025-09-11 07:17:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2057MB
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:17:05 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.88507080078125, 'val_avg_loss': 0.6894253540039063, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:17:05 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.88507080078125, 'val_avg_loss': 0.6894253540039063, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:17:05 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.88507080078125, 'val_avg_loss': 0.6894253540039063, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:17:05 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.88507080078125, 'val_avg_loss': 0.6894253540039063, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:17:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:17:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:17:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.311680, avg_loss=0.682792, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:17:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:17:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:17:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:17:08 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.31167984008789, 'test_avg_loss': 0.6827919960021973, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:17:08 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.31167984008789, 'test_avg_loss': 0.6827919960021973, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:17:08 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.31167984008789, 'test_avg_loss': 0.6827919960021973, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:17:08 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.31167984008789, 'test_avg_loss': 0.6827919960021973, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:17:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:17:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:17:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:17:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:17:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:17:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:17:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:17:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:17:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:17:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.031433, avg_loss=0.690157, seen=200, correct=105, accuracy=0.525000
2025-09-11 07:17:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:15 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.03143310546875, 'val_avg_loss': 0.6901571655273437, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-11 07:17:15 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.03143310546875, 'val_avg_loss': 0.6901571655273437, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-11 07:17:15 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.03143310546875, 'val_avg_loss': 0.6901571655273437, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-11 07:17:15 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.03143310546875, 'val_avg_loss': 0.6901571655273437, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:17:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:17:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:17:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.288645, avg_loss=0.707216, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:17 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.288644790649414, 'test_avg_loss': 0.7072161197662353, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:17:17 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.288644790649414, 'test_avg_loss': 0.7072161197662353, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:17:17 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.288644790649414, 'test_avg_loss': 0.7072161197662353, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:17:17 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.288644790649414, 'test_avg_loss': 0.7072161197662353, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:17:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:17:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:17:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:17:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:17:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.664185, avg_loss=0.678321, seen=200, correct=115, accuracy=0.575000
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:24 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.6641845703125, 'val_avg_loss': 0.6783209228515625, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-11 07:17:24 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.6641845703125, 'val_avg_loss': 0.6783209228515625, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-11 07:17:24 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.6641845703125, 'val_avg_loss': 0.6783209228515625, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-11 07:17:24 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.6641845703125, 'val_avg_loss': 0.6783209228515625, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:17:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:17:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:17:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:17:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.410992, avg_loss=0.685275, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2032MB
2025-09-11 07:17:26 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.410991668701172, 'test_avg_loss': 0.6852747917175293, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:17:26 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.410991668701172, 'test_avg_loss': 0.6852747917175293, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:17:26 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.410991668701172, 'test_avg_loss': 0.6852747917175293, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:17:26 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.410991668701172, 'test_avg_loss': 0.6852747917175293, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:17:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:17:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:17:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:17:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:17:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-11 07:17:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=14, total=54)
2025-09-11 07:17:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=13, total=54)
2025-09-11 07:17:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=13, total=54)
2025-09-11 07:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(27, 3, 2, 30)
2025-09-11 07:17:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-11 07:17:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=13
2025-09-11 07:17:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=14
2025-09-11 07:17:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=13
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.616444, avg_loss=0.678082, seen=54, correct=26, accuracy=0.481481
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:17:30 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 0 with val results: {'val_total': 54, 'val_loss': 36.6164436340332, 'val_avg_loss': 0.6780822895191334, 'val_seen': 54, 'val_correct': 26, 'val_acc': 0.48148148148148145}
2025-09-11 07:17:30 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 0 with val results: {'val_total': 54, 'val_loss': 36.6164436340332, 'val_avg_loss': 0.6780822895191334, 'val_seen': 54, 'val_correct': 26, 'val_acc': 0.48148148148148145}
2025-09-11 07:17:30 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 0 with val results: {'val_total': 54, 'val_loss': 36.6164436340332, 'val_avg_loss': 0.6780822895191334, 'val_seen': 54, 'val_correct': 26, 'val_acc': 0.48148148148148145}
2025-09-11 07:17:30 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 0 with val results: {'val_total': 54, 'val_loss': 36.6164436340332, 'val_avg_loss': 0.6780822895191334, 'val_seen': 54, 'val_correct': 26, 'val_acc': 0.48148148148148145}
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:17:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:17:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:17:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:17:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.288725, avg_loss=0.657218, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:17:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:17:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:17:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:17:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:17:32 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.288724899291992, 'test_avg_loss': 0.6572181224822998, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:17:32 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.288724899291992, 'test_avg_loss': 0.6572181224822998, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:17:32 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.288724899291992, 'test_avg_loss': 0.6572181224822998, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:17:32 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.288724899291992, 'test_avg_loss': 0.6572181224822998, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:17:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-11 07:17:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=14, total=54)
2025-09-11 07:17:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=13, total=54)
2025-09-11 07:17:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=13, total=54)
2025-09-11 07:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(27, 3, 2, 30)
2025-09-11 07:17:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=13
2025-09-11 07:17:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=13
2025-09-11 07:17:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=14
2025-09-11 07:17:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.048290, avg_loss=0.667561, seen=54, correct=35, accuracy=0.648148
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:36 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 1 with val results: {'val_total': 54, 'val_loss': 36.04829025268555, 'val_avg_loss': 0.6675609306052879, 'val_seen': 54, 'val_correct': 35, 'val_acc': 0.6481481481481481}
2025-09-11 07:17:36 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 1 with val results: {'val_total': 54, 'val_loss': 36.04829025268555, 'val_avg_loss': 0.6675609306052879, 'val_seen': 54, 'val_correct': 35, 'val_acc': 0.6481481481481481}
2025-09-11 07:17:36 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 1 with val results: {'val_total': 54, 'val_loss': 36.04829025268555, 'val_avg_loss': 0.6675609306052879, 'val_seen': 54, 'val_correct': 35, 'val_acc': 0.6481481481481481}
2025-09-11 07:17:36 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 1 with val results: {'val_total': 54, 'val_loss': 36.04829025268555, 'val_avg_loss': 0.6675609306052879, 'val_seen': 54, 'val_correct': 35, 'val_acc': 0.6481481481481481}
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:17:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.510731, avg_loss=0.662768, seen=40, correct=26, accuracy=0.650000
2025-09-11 07:17:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:38 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.510730743408203, 'test_avg_loss': 0.6627682685852051, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:17:38 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.510730743408203, 'test_avg_loss': 0.6627682685852051, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:17:38 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.510730743408203, 'test_avg_loss': 0.6627682685852051, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:17:38 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.510730743408203, 'test_avg_loss': 0.6627682685852051, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:17:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=14, total=54)
2025-09-11 07:17:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=13, total=54)
2025-09-11 07:17:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-11 07:17:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=13, total=54)
2025-09-11 07:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(27, 3, 2, 30)
2025-09-11 07:17:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(27, 3, 2, 30)
2025-09-11 07:17:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=14
2025-09-11 07:17:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=13
2025-09-11 07:17:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=13
2025-09-11 07:17:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-11 07:17:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.120777, avg_loss=0.668903, seen=54, correct=30, accuracy=0.555556
2025-09-11 07:17:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:42 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 2 with val results: {'val_total': 54, 'val_loss': 36.12077713012695, 'val_avg_loss': 0.6689032801875362, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-11 07:17:42 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 2 with val results: {'val_total': 54, 'val_loss': 36.12077713012695, 'val_avg_loss': 0.6689032801875362, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-11 07:17:42 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 2 with val results: {'val_total': 54, 'val_loss': 36.12077713012695, 'val_avg_loss': 0.6689032801875362, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-11 07:17:42 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 2 with val results: {'val_total': 54, 'val_loss': 36.12077713012695, 'val_avg_loss': 0.6689032801875362, 'val_seen': 54, 'val_correct': 30, 'val_acc': 0.5555555555555556}
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:17:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:17:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:17:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:17:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.328217, avg_loss=0.683205, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:17:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2040MB
2025-09-11 07:17:44 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.328216552734375, 'test_avg_loss': 0.6832054138183594, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:17:44 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.328216552734375, 'test_avg_loss': 0.6832054138183594, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:17:44 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.328216552734375, 'test_avg_loss': 0.6832054138183594, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:17:44 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.328216552734375, 'test_avg_loss': 0.6832054138183594, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:17:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:17:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:17:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:17:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:17:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:17:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:17:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:17:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:17:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:17:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:17:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:17:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:17:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.417496, avg_loss=0.692087, seen=200, correct=114, accuracy=0.570000
2025-09-11 07:17:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:17:53 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.41749572753906, 'val_avg_loss': 0.6920874786376953, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-11 07:17:53 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.41749572753906, 'val_avg_loss': 0.6920874786376953, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-11 07:17:53 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.41749572753906, 'val_avg_loss': 0.6920874786376953, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-11 07:17:53 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.41749572753906, 'val_avg_loss': 0.6920874786376953, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:17:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:17:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:17:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:17:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:17:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.225697, avg_loss=0.705642, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:17:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:17:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:17:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:17:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:17:56 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.225696563720703, 'test_avg_loss': 0.7056424140930175, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:17:56 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.225696563720703, 'test_avg_loss': 0.7056424140930175, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:17:56 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.225696563720703, 'test_avg_loss': 0.7056424140930175, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:17:56 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.225696563720703, 'test_avg_loss': 0.7056424140930175, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:17:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:17:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:17:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:17:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:17:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:17:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:18:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:18:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:18:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:18:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.138702, avg_loss=0.690694, seen=200, correct=107, accuracy=0.535000
2025-09-11 07:18:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:04 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.13870239257812, 'val_avg_loss': 0.6906935119628906, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-11 07:18:04 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.13870239257812, 'val_avg_loss': 0.6906935119628906, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-11 07:18:04 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.13870239257812, 'val_avg_loss': 0.6906935119628906, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-11 07:18:04 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.13870239257812, 'val_avg_loss': 0.6906935119628906, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:18:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:18:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:18:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:18:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.211872, avg_loss=0.680297, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:07 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.211872100830078, 'test_avg_loss': 0.680296802520752, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:18:07 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.211872100830078, 'test_avg_loss': 0.680296802520752, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:18:07 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.211872100830078, 'test_avg_loss': 0.680296802520752, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:18:07 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.211872100830078, 'test_avg_loss': 0.680296802520752, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:18:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:18:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:18:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:18:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:18:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:18:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:18:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:18:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:18:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:18:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.908600, avg_loss=0.699543, seen=200, correct=102, accuracy=0.510000
2025-09-11 07:18:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:15 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.90859985351562, 'val_avg_loss': 0.6995429992675781, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-11 07:18:15 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.90859985351562, 'val_avg_loss': 0.6995429992675781, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-11 07:18:15 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.90859985351562, 'val_avg_loss': 0.6995429992675781, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-11 07:18:15 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.90859985351562, 'val_avg_loss': 0.6995429992675781, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:18:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:18:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:18:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:18:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.869123, avg_loss=0.721728, seen=40, correct=14, accuracy=0.350000
2025-09-11 07:18:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2049MB
2025-09-11 07:18:18 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.869123458862305, 'test_avg_loss': 0.7217280864715576, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-09-11 07:18:18 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.869123458862305, 'test_avg_loss': 0.7217280864715576, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-09-11 07:18:18 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.869123458862305, 'test_avg_loss': 0.7217280864715576, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-09-11 07:18:18 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.869123458862305, 'test_avg_loss': 0.7217280864715576, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-09-11 07:18:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:18:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:18:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:18:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:18:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:18:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:18:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:18:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:18:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:18:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:18:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:18:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:18:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.393112, avg_loss=0.671966, seen=200, correct=123, accuracy=0.615000
2025-09-11 07:18:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-11 07:18:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-11 07:18:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-11 07:18:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-11 07:18:26 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 0 with val results: {'val_total': 200, 'val_loss': 134.3931121826172, 'val_avg_loss': 0.6719655609130859, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-11 07:18:26 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 0 with val results: {'val_total': 200, 'val_loss': 134.3931121826172, 'val_avg_loss': 0.6719655609130859, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-11 07:18:26 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 0 with val results: {'val_total': 200, 'val_loss': 134.3931121826172, 'val_avg_loss': 0.6719655609130859, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-11 07:18:26 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 0 with val results: {'val_total': 200, 'val_loss': 134.3931121826172, 'val_avg_loss': 0.6719655609130859, 'val_seen': 200, 'val_correct': 123, 'val_acc': 0.615}
2025-09-11 07:18:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:18:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:18:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:18:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:18:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:18:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:18:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:18:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:18:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.824697, avg_loss=0.695617, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:18:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-11 07:18:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-11 07:18:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-11 07:18:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2082MB
2025-09-11 07:18:29 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.824697494506836, 'test_avg_loss': 0.6956174373626709, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:18:29 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.824697494506836, 'test_avg_loss': 0.6956174373626709, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:18:29 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.824697494506836, 'test_avg_loss': 0.6956174373626709, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:18:29 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.824697494506836, 'test_avg_loss': 0.6956174373626709, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:18:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:18:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:18:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:18:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:18:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:18:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:18:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:18:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:18:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.296387, avg_loss=0.686482, seen=200, correct=110, accuracy=0.550000
2025-09-11 07:18:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:37 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.29638671875, 'val_avg_loss': 0.68648193359375, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-11 07:18:37 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.29638671875, 'val_avg_loss': 0.68648193359375, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-11 07:18:37 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.29638671875, 'val_avg_loss': 0.68648193359375, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-11 07:18:37 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.29638671875, 'val_avg_loss': 0.68648193359375, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.391985, avg_loss=0.684800, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:40 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.391984939575195, 'test_avg_loss': 0.6847996234893798, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:18:40 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.391984939575195, 'test_avg_loss': 0.6847996234893798, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:18:40 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.391984939575195, 'test_avg_loss': 0.6847996234893798, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:18:40 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.391984939575195, 'test_avg_loss': 0.6847996234893798, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:18:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:18:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:18:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:18:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:18:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:18:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:18:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:18:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:18:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.474838, avg_loss=0.687374, seen=200, correct=113, accuracy=0.565000
2025-09-11 07:18:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:47 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 2 with val results: {'val_total': 200, 'val_loss': 137.47483825683594, 'val_avg_loss': 0.6873741912841796, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:18:47 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 2 with val results: {'val_total': 200, 'val_loss': 137.47483825683594, 'val_avg_loss': 0.6873741912841796, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:18:47 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 2 with val results: {'val_total': 200, 'val_loss': 137.47483825683594, 'val_avg_loss': 0.6873741912841796, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:18:47 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 2 with val results: {'val_total': 200, 'val_loss': 137.47483825683594, 'val_avg_loss': 0.6873741912841796, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:18:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.103176, avg_loss=0.677579, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:18:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2057MB
2025-09-11 07:18:49 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.10317611694336, 'test_avg_loss': 0.677579402923584, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:18:49 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.10317611694336, 'test_avg_loss': 0.677579402923584, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:18:49 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.10317611694336, 'test_avg_loss': 0.677579402923584, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:18:49 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.10317611694336, 'test_avg_loss': 0.677579402923584, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:18:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:18:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:18:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:18:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:18:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-11 07:18:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=21, total=83)
2025-09-11 07:18:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=20, total=83)
2025-09-11 07:18:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=21, total=83)
2025-09-11 07:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:18:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=21
2025-09-11 07:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=21
2025-09-11 07:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=20
2025-09-11 07:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-11 07:18:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.068390, avg_loss=0.687571, seen=83, correct=45, accuracy=0.542169
2025-09-11 07:18:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:18:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:18:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:18:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:18:53 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 0 with val results: {'val_total': 83, 'val_loss': 57.068389892578125, 'val_avg_loss': 0.6875709625611822, 'val_seen': 83, 'val_correct': 45, 'val_acc': 0.5421686746987951}
2025-09-11 07:18:53 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 0 with val results: {'val_total': 83, 'val_loss': 57.068389892578125, 'val_avg_loss': 0.6875709625611822, 'val_seen': 83, 'val_correct': 45, 'val_acc': 0.5421686746987951}
2025-09-11 07:18:53 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 0 with val results: {'val_total': 83, 'val_loss': 57.068389892578125, 'val_avg_loss': 0.6875709625611822, 'val_seen': 83, 'val_correct': 45, 'val_acc': 0.5421686746987951}
2025-09-11 07:18:53 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 0 with val results: {'val_total': 83, 'val_loss': 57.068389892578125, 'val_avg_loss': 0.6875709625611822, 'val_seen': 83, 'val_correct': 45, 'val_acc': 0.5421686746987951}
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:18:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:18:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:18:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.402132, avg_loss=0.710053, seen=40, correct=18, accuracy=0.450000
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:18:56 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.402132034301758, 'test_avg_loss': 0.7100533008575439, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:18:56 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.402132034301758, 'test_avg_loss': 0.7100533008575439, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:18:56 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.402132034301758, 'test_avg_loss': 0.7100533008575439, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:18:56 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.402132034301758, 'test_avg_loss': 0.7100533008575439, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=21, total=83)
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=21, total=83)
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=20, total=83)
2025-09-11 07:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:18:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:18:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=21
2025-09-11 07:18:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=21
2025-09-11 07:18:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-11 07:18:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=20
2025-09-11 07:18:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.760429, avg_loss=0.707957, seen=83, correct=37, accuracy=0.445783
2025-09-11 07:18:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:18:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:18:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:18:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:00 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 1 with val results: {'val_total': 83, 'val_loss': 58.76042938232422, 'val_avg_loss': 0.7079569805099304, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}
2025-09-11 07:19:00 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 1 with val results: {'val_total': 83, 'val_loss': 58.76042938232422, 'val_avg_loss': 0.7079569805099304, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}
2025-09-11 07:19:00 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 1 with val results: {'val_total': 83, 'val_loss': 58.76042938232422, 'val_avg_loss': 0.7079569805099304, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}
2025-09-11 07:19:00 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 1 with val results: {'val_total': 83, 'val_loss': 58.76042938232422, 'val_avg_loss': 0.7079569805099304, 'val_seen': 83, 'val_correct': 37, 'val_acc': 0.4457831325301205}
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:19:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:19:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:19:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.780087, avg_loss=0.719502, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:02 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.780086517333984, 'test_avg_loss': 0.7195021629333496, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:19:02 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.780086517333984, 'test_avg_loss': 0.7195021629333496, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:19:02 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.780086517333984, 'test_avg_loss': 0.7195021629333496, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:19:02 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.780086517333984, 'test_avg_loss': 0.7195021629333496, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:19:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-11 07:19:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=21, total=83)
2025-09-11 07:19:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=21, total=83)
2025-09-11 07:19:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=20, total=83)
2025-09-11 07:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:19:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:19:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-11 07:19:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=21
2025-09-11 07:19:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=20
2025-09-11 07:19:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=21
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.112572, avg_loss=0.700151, seen=83, correct=44, accuracy=0.530120
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:06 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 2 with val results: {'val_total': 83, 'val_loss': 58.112571716308594, 'val_avg_loss': 0.7001514664615494, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-11 07:19:06 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 2 with val results: {'val_total': 83, 'val_loss': 58.112571716308594, 'val_avg_loss': 0.7001514664615494, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-11 07:19:06 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 2 with val results: {'val_total': 83, 'val_loss': 58.112571716308594, 'val_avg_loss': 0.7001514664615494, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-11 07:19:06 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 2 with val results: {'val_total': 83, 'val_loss': 58.112571716308594, 'val_avg_loss': 0.7001514664615494, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:19:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:19:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:19:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.042393, avg_loss=0.726060, seen=40, correct=15, accuracy=0.375000
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2065MB
2025-09-11 07:19:08 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.04239273071289, 'test_avg_loss': 0.7260598182678223, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:19:08 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.04239273071289, 'test_avg_loss': 0.7260598182678223, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:19:08 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.04239273071289, 'test_avg_loss': 0.7260598182678223, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:19:08 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.04239273071289, 'test_avg_loss': 0.7260598182678223, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-11 07:19:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:19:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:19:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:19:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:19:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:19:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:19:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:19:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:19:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:19:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:19:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:19:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:19:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.748779, avg_loss=0.668744, seen=200, correct=126, accuracy=0.630000
2025-09-11 07:19:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:19:16 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.748779296875, 'val_avg_loss': 0.668743896484375, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-11 07:19:16 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.748779296875, 'val_avg_loss': 0.668743896484375, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-11 07:19:16 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.748779296875, 'val_avg_loss': 0.668743896484375, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-11 07:19:16 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 0 with val results: {'val_total': 200, 'val_loss': 133.748779296875, 'val_avg_loss': 0.668743896484375, 'val_seen': 200, 'val_correct': 126, 'val_acc': 0.63}
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.522106, avg_loss=0.713053, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:19:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:19:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:19:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:19:19 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.522106170654297, 'test_avg_loss': 0.7130526542663574, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:19:19 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.522106170654297, 'test_avg_loss': 0.7130526542663574, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:19:19 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.522106170654297, 'test_avg_loss': 0.7130526542663574, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:19:19 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.522106170654297, 'test_avg_loss': 0.7130526542663574, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:19:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:19:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:19:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:19:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:19:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)2025-09-11 07:19:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:19:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:19:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:19:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:19:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:19:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.084824, avg_loss=0.670424, seen=200, correct=122, accuracy=0.610000
2025-09-11 07:19:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:27 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 1 with val results: {'val_total': 200, 'val_loss': 134.08482360839844, 'val_avg_loss': 0.6704241180419922, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-11 07:19:27 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 1 with val results: {'val_total': 200, 'val_loss': 134.08482360839844, 'val_avg_loss': 0.6704241180419922, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-11 07:19:27 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 1 with val results: {'val_total': 200, 'val_loss': 134.08482360839844, 'val_avg_loss': 0.6704241180419922, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-11 07:19:27 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 1 with val results: {'val_total': 200, 'val_loss': 134.08482360839844, 'val_avg_loss': 0.6704241180419922, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:19:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.465597, avg_loss=0.686640, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:19:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:29 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.46559715270996, 'test_avg_loss': 0.686639928817749, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:19:29 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.46559715270996, 'test_avg_loss': 0.686639928817749, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:19:29 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.46559715270996, 'test_avg_loss': 0.686639928817749, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:19:29 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.46559715270996, 'test_avg_loss': 0.686639928817749, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:19:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:19:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:19:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:19:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:19:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:19:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:19:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:19:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:19:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.062653, avg_loss=0.690313, seen=200, correct=109, accuracy=0.545000
2025-09-11 07:19:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:38 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.06265258789062, 'val_avg_loss': 0.6903132629394532, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:19:38 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.06265258789062, 'val_avg_loss': 0.6903132629394532, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:19:38 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.06265258789062, 'val_avg_loss': 0.6903132629394532, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:19:38 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.06265258789062, 'val_avg_loss': 0.6903132629394532, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:19:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.921711, avg_loss=0.698043, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:19:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2074MB
2025-09-11 07:19:41 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.921710968017578, 'test_avg_loss': 0.6980427742004395, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:19:41 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.921710968017578, 'test_avg_loss': 0.6980427742004395, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:19:41 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.921710968017578, 'test_avg_loss': 0.6980427742004395, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:19:41 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.921710968017578, 'test_avg_loss': 0.6980427742004395, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:19:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:19:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:19:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:19:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:19:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=30, total=119)
2025-09-11 07:19:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-11 07:19:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=30, total=119)
2025-09-11 07:19:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=29, total=119)
2025-09-11 07:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:19:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:19:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-11 07:19:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=30
2025-09-11 07:19:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=29
2025-09-11 07:19:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=30
2025-09-11 07:19:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=83.640030, avg_loss=0.702857, seen=119, correct=59, accuracy=0.495798
2025-09-11 07:19:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:19:46 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 0 with val results: {'val_total': 119, 'val_loss': 83.64002990722656, 'val_avg_loss': 0.7028573941783745, 'val_seen': 119, 'val_correct': 59, 'val_acc': 0.4957983193277311}
2025-09-11 07:19:46 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 0 with val results: {'val_total': 119, 'val_loss': 83.64002990722656, 'val_avg_loss': 0.7028573941783745, 'val_seen': 119, 'val_correct': 59, 'val_acc': 0.4957983193277311}
2025-09-11 07:19:46 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 0 with val results: {'val_total': 119, 'val_loss': 83.64002990722656, 'val_avg_loss': 0.7028573941783745, 'val_seen': 119, 'val_correct': 59, 'val_acc': 0.4957983193277311}
2025-09-11 07:19:46 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 0 with val results: {'val_total': 119, 'val_loss': 83.64002990722656, 'val_avg_loss': 0.7028573941783745, 'val_seen': 119, 'val_correct': 59, 'val_acc': 0.4957983193277311}
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:19:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:19:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:19:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:19:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.892441, avg_loss=0.672311, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:19:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:19:49 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.892440795898438, 'test_avg_loss': 0.6723110198974609, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:19:49 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.892440795898438, 'test_avg_loss': 0.6723110198974609, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:19:49 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.892440795898438, 'test_avg_loss': 0.6723110198974609, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:19:49 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.892440795898438, 'test_avg_loss': 0.6723110198974609, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=30, total=119)
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=30, total=119)
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=29, total=119)
2025-09-11 07:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:19:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-11 07:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=30
2025-09-11 07:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=29
2025-09-11 07:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=30
2025-09-11 07:19:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.746956, avg_loss=0.695353, seen=119, correct=63, accuracy=0.529412
2025-09-11 07:19:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:19:54 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 1 with val results: {'val_total': 119, 'val_loss': 82.74695587158203, 'val_avg_loss': 0.6953525703494289, 'val_seen': 119, 'val_correct': 63, 'val_acc': 0.5294117647058824}
2025-09-11 07:19:54 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 1 with val results: {'val_total': 119, 'val_loss': 82.74695587158203, 'val_avg_loss': 0.6953525703494289, 'val_seen': 119, 'val_correct': 63, 'val_acc': 0.5294117647058824}
2025-09-11 07:19:54 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 1 with val results: {'val_total': 119, 'val_loss': 82.74695587158203, 'val_avg_loss': 0.6953525703494289, 'val_seen': 119, 'val_correct': 63, 'val_acc': 0.5294117647058824}
2025-09-11 07:19:54 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 1 with val results: {'val_total': 119, 'val_loss': 82.74695587158203, 'val_avg_loss': 0.6953525703494289, 'val_seen': 119, 'val_correct': 63, 'val_acc': 0.5294117647058824}
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:19:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:19:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:19:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:19:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:19:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:19:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:19:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.872555, avg_loss=0.696814, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:19:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:19:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:19:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:19:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:19:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:19:57 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.872554779052734, 'test_avg_loss': 0.6968138694763184, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:19:57 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.872554779052734, 'test_avg_loss': 0.6968138694763184, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:19:57 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.872554779052734, 'test_avg_loss': 0.6968138694763184, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:19:57 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.872554779052734, 'test_avg_loss': 0.6968138694763184, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:19:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=30, total=119)
2025-09-11 07:19:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=30, total=119)
2025-09-11 07:19:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-11 07:19:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=29, total=119)
2025-09-11 07:19:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:19:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:19:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:19:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:19:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:19:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=30
2025-09-11 07:20:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=30
2025-09-11 07:20:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=29
2025-09-11 07:20:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-11 07:20:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=83.004349, avg_loss=0.697516, seen=119, correct=65, accuracy=0.546218
2025-09-11 07:20:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:20:02 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 2 with val results: {'val_total': 119, 'val_loss': 83.00434875488281, 'val_avg_loss': 0.6975155357553178, 'val_seen': 119, 'val_correct': 65, 'val_acc': 0.5462184873949579}
2025-09-11 07:20:02 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 2 with val results: {'val_total': 119, 'val_loss': 83.00434875488281, 'val_avg_loss': 0.6975155357553178, 'val_seen': 119, 'val_correct': 65, 'val_acc': 0.5462184873949579}
2025-09-11 07:20:02 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 2 with val results: {'val_total': 119, 'val_loss': 83.00434875488281, 'val_avg_loss': 0.6975155357553178, 'val_seen': 119, 'val_correct': 65, 'val_acc': 0.5462184873949579}
2025-09-11 07:20:02 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 2 with val results: {'val_total': 119, 'val_loss': 83.00434875488281, 'val_avg_loss': 0.6975155357553178, 'val_seen': 119, 'val_correct': 65, 'val_acc': 0.5462184873949579}
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.465071, avg_loss=0.711627, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:20:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:20:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:20:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2083MB
2025-09-11 07:20:04 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.465070724487305, 'test_avg_loss': 0.7116267681121826, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:20:04 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.465070724487305, 'test_avg_loss': 0.7116267681121826, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:20:04 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.465070724487305, 'test_avg_loss': 0.7116267681121826, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:20:04 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.465070724487305, 'test_avg_loss': 0.7116267681121826, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:20:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:20:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:20:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:20:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:20:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:20:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:20:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:20:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:20:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.633545, avg_loss=0.693168, seen=200, correct=103, accuracy=0.515000
2025-09-11 07:20:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:20:12 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.633544921875, 'val_avg_loss': 0.693167724609375, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-11 07:20:12 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.633544921875, 'val_avg_loss': 0.693167724609375, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-11 07:20:12 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.633544921875, 'val_avg_loss': 0.693167724609375, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-11 07:20:12 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.633544921875, 'val_avg_loss': 0.693167724609375, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:20:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:20:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:20:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.000866, avg_loss=0.675022, seen=40, correct=24, accuracy=0.600000
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:20:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:20:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:20:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:20:15 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.000865936279297, 'test_avg_loss': 0.6750216484069824, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:20:15 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.000865936279297, 'test_avg_loss': 0.6750216484069824, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:20:15 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.000865936279297, 'test_avg_loss': 0.6750216484069824, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:20:15 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.000865936279297, 'test_avg_loss': 0.6750216484069824, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:20:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:20:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:20:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:20:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:20:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:20:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:20:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:20:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:20:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:20:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.276505, avg_loss=0.691383, seen=200, correct=97, accuracy=0.485000
2025-09-11 07:20:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:23 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.27650451660156, 'val_avg_loss': 0.6913825225830078, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-11 07:20:23 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.27650451660156, 'val_avg_loss': 0.6913825225830078, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-11 07:20:23 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.27650451660156, 'val_avg_loss': 0.6913825225830078, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-11 07:20:23 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 1 with val results: {'val_total': 200, 'val_loss': 138.27650451660156, 'val_avg_loss': 0.6913825225830078, 'val_seen': 200, 'val_correct': 97, 'val_acc': 0.485}
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:20:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:20:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:20:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.887377, avg_loss=0.697184, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:25 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88737678527832, 'test_avg_loss': 0.697184419631958, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:20:25 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88737678527832, 'test_avg_loss': 0.697184419631958, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:20:25 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88737678527832, 'test_avg_loss': 0.697184419631958, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:20:25 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.88737678527832, 'test_avg_loss': 0.697184419631958, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:20:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:20:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:20:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:20:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:20:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)2025-09-11 07:20:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:20:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:20:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:20:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:20:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:20:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.027771, avg_loss=0.710139, seen=200, correct=95, accuracy=0.475000
2025-09-11 07:20:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:33 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.02777099609375, 'val_avg_loss': 0.7101388549804688, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-11 07:20:33 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.02777099609375, 'val_avg_loss': 0.7101388549804688, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-11 07:20:33 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.02777099609375, 'val_avg_loss': 0.7101388549804688, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-11 07:20:33 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.02777099609375, 'val_avg_loss': 0.7101388549804688, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:20:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:20:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:20:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:20:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.253441, avg_loss=0.681336, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:20:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2091MB
2025-09-11 07:20:35 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.253440856933594, 'test_avg_loss': 0.6813360214233398, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:20:35 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.253440856933594, 'test_avg_loss': 0.6813360214233398, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:20:35 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.253440856933594, 'test_avg_loss': 0.6813360214233398, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:20:35 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.253440856933594, 'test_avg_loss': 0.6813360214233398, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:20:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-11 07:20:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=22, total=89)
2025-09-11 07:20:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=22, total=89)
2025-09-11 07:20:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=22, total=89)
2025-09-11 07:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:20:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:20:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=22
2025-09-11 07:20:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=22
2025-09-11 07:20:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-11 07:20:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=22
2025-09-11 07:20:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.158318, avg_loss=0.687172, seen=89, correct=48, accuracy=0.539326
2025-09-11 07:20:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:20:40 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 0 with val results: {'val_total': 89, 'val_loss': 61.15831756591797, 'val_avg_loss': 0.6871721074822243, 'val_seen': 89, 'val_correct': 48, 'val_acc': 0.5393258426966292}
2025-09-11 07:20:40 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 0 with val results: {'val_total': 89, 'val_loss': 61.15831756591797, 'val_avg_loss': 0.6871721074822243, 'val_seen': 89, 'val_correct': 48, 'val_acc': 0.5393258426966292}
2025-09-11 07:20:40 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 0 with val results: {'val_total': 89, 'val_loss': 61.15831756591797, 'val_avg_loss': 0.6871721074822243, 'val_seen': 89, 'val_correct': 48, 'val_acc': 0.5393258426966292}
2025-09-11 07:20:40 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 0 with val results: {'val_total': 89, 'val_loss': 61.15831756591797, 'val_avg_loss': 0.6871721074822243, 'val_seen': 89, 'val_correct': 48, 'val_acc': 0.5393258426966292}
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:20:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:20:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:20:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:20:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.451782, avg_loss=0.686295, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:20:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:20:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:20:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:20:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:20:42 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.4517822265625, 'test_avg_loss': 0.6862945556640625, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:20:42 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.4517822265625, 'test_avg_loss': 0.6862945556640625, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:20:42 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.4517822265625, 'test_avg_loss': 0.6862945556640625, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:20:42 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.4517822265625, 'test_avg_loss': 0.6862945556640625, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:20:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-11 07:20:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=22, total=89)
2025-09-11 07:20:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=22, total=89)
2025-09-11 07:20:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=22, total=89)
2025-09-11 07:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:20:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=22
2025-09-11 07:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=22
2025-09-11 07:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=22
2025-09-11 07:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-11 07:20:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.863926, avg_loss=0.695100, seen=89, correct=42, accuracy=0.471910
2025-09-11 07:20:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:46 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 1 with val results: {'val_total': 89, 'val_loss': 61.86392593383789, 'val_avg_loss': 0.6951002913914369, 'val_seen': 89, 'val_correct': 42, 'val_acc': 0.47191011235955055}
2025-09-11 07:20:46 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 1 with val results: {'val_total': 89, 'val_loss': 61.86392593383789, 'val_avg_loss': 0.6951002913914369, 'val_seen': 89, 'val_correct': 42, 'val_acc': 0.47191011235955055}
2025-09-11 07:20:46 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 1 with val results: {'val_total': 89, 'val_loss': 61.86392593383789, 'val_avg_loss': 0.6951002913914369, 'val_seen': 89, 'val_correct': 42, 'val_acc': 0.47191011235955055}
2025-09-11 07:20:46 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 1 with val results: {'val_total': 89, 'val_loss': 61.86392593383789, 'val_avg_loss': 0.6951002913914369, 'val_seen': 89, 'val_correct': 42, 'val_acc': 0.47191011235955055}
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:20:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.905882, avg_loss=0.672647, seen=40, correct=25, accuracy=0.625000
2025-09-11 07:20:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:48 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.905881881713867, 'test_avg_loss': 0.6726470470428467, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:20:48 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.905881881713867, 'test_avg_loss': 0.6726470470428467, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:20:48 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.905881881713867, 'test_avg_loss': 0.6726470470428467, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:20:48 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.905881881713867, 'test_avg_loss': 0.6726470470428467, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-11 07:20:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=22, total=89)
2025-09-11 07:20:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=22, total=89)
2025-09-11 07:20:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-11 07:20:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=22, total=89)
2025-09-11 07:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:20:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:20:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-11 07:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=22
2025-09-11 07:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=22
2025-09-11 07:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=22
2025-09-11 07:20:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.795895, avg_loss=0.694336, seen=89, correct=44, accuracy=0.494382
2025-09-11 07:20:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:52 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 2 with val results: {'val_total': 89, 'val_loss': 61.795894622802734, 'val_avg_loss': 0.694335894638233, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}
2025-09-11 07:20:52 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 2 with val results: {'val_total': 89, 'val_loss': 61.795894622802734, 'val_avg_loss': 0.694335894638233, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}
2025-09-11 07:20:52 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 2 with val results: {'val_total': 89, 'val_loss': 61.795894622802734, 'val_avg_loss': 0.694335894638233, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}
2025-09-11 07:20:52 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 2 with val results: {'val_total': 89, 'val_loss': 61.795894622802734, 'val_avg_loss': 0.694335894638233, 'val_seen': 89, 'val_correct': 44, 'val_acc': 0.4943820224719101}
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:20:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:20:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:20:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:20:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:20:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:20:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.961937, avg_loss=0.699048, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:20:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2099MB
2025-09-11 07:20:54 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.961936950683594, 'test_avg_loss': 0.6990484237670899, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:20:54 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.961936950683594, 'test_avg_loss': 0.6990484237670899, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:20:54 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.961936950683594, 'test_avg_loss': 0.6990484237670899, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:20:54 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.961936950683594, 'test_avg_loss': 0.6990484237670899, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:20:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:20:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:20:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:20:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:20:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:20:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:20:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:21:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:21:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:21:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:21:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.433228, avg_loss=0.712166, seen=200, correct=95, accuracy=0.475000
2025-09-11 07:21:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:21:01 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 0 with val results: {'val_total': 200, 'val_loss': 142.4332275390625, 'val_avg_loss': 0.7121661376953125, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-11 07:21:01 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 0 with val results: {'val_total': 200, 'val_loss': 142.4332275390625, 'val_avg_loss': 0.7121661376953125, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-11 07:21:01 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 0 with val results: {'val_total': 200, 'val_loss': 142.4332275390625, 'val_avg_loss': 0.7121661376953125, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-11 07:21:01 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 0 with val results: {'val_total': 200, 'val_loss': 142.4332275390625, 'val_avg_loss': 0.7121661376953125, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:21:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:21:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:21:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:21:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.285145, avg_loss=0.682129, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:21:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:21:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:21:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:21:03 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.285144805908203, 'test_avg_loss': 0.682128620147705, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:21:03 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.285144805908203, 'test_avg_loss': 0.682128620147705, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:21:03 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.285144805908203, 'test_avg_loss': 0.682128620147705, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:21:03 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.285144805908203, 'test_avg_loss': 0.682128620147705, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:21:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:21:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:21:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:21:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:21:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:21:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:21:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:21:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.982132, avg_loss=0.709911, seen=200, correct=89, accuracy=0.445000
2025-09-11 07:21:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:11 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 1 with val results: {'val_total': 200, 'val_loss': 141.9821319580078, 'val_avg_loss': 0.7099106597900391, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445}
2025-09-11 07:21:11 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 1 with val results: {'val_total': 200, 'val_loss': 141.9821319580078, 'val_avg_loss': 0.7099106597900391, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445}
2025-09-11 07:21:11 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 1 with val results: {'val_total': 200, 'val_loss': 141.9821319580078, 'val_avg_loss': 0.7099106597900391, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445}
2025-09-11 07:21:11 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 1 with val results: {'val_total': 200, 'val_loss': 141.9821319580078, 'val_avg_loss': 0.7099106597900391, 'val_seen': 200, 'val_correct': 89, 'val_acc': 0.445}
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:21:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:21:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:21:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:21:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.490772, avg_loss=0.687269, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:21:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:13 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.490772247314453, 'test_avg_loss': 0.6872693061828613, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:21:13 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.490772247314453, 'test_avg_loss': 0.6872693061828613, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:21:13 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.490772247314453, 'test_avg_loss': 0.6872693061828613, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:21:13 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.490772247314453, 'test_avg_loss': 0.6872693061828613, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:21:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:21:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:21:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:21:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:21:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)2025-09-11 07:21:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:21:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:21:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.927063, avg_loss=0.709635, seen=200, correct=103, accuracy=0.515000
2025-09-11 07:21:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:21 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 2 with val results: {'val_total': 200, 'val_loss': 141.92706298828125, 'val_avg_loss': 0.7096353149414063, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-11 07:21:21 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 2 with val results: {'val_total': 200, 'val_loss': 141.92706298828125, 'val_avg_loss': 0.7096353149414063, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-11 07:21:21 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 2 with val results: {'val_total': 200, 'val_loss': 141.92706298828125, 'val_avg_loss': 0.7096353149414063, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-11 07:21:21 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 2 with val results: {'val_total': 200, 'val_loss': 141.92706298828125, 'val_avg_loss': 0.7096353149414063, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.674604, avg_loss=0.666865, seen=40, correct=24, accuracy=0.600000
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2108MB
2025-09-11 07:21:24 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.674604415893555, 'test_avg_loss': 0.6668651103973389, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:21:24 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.674604415893555, 'test_avg_loss': 0.6668651103973389, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:21:24 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.674604415893555, 'test_avg_loss': 0.6668651103973389, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:21:24 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.674604415893555, 'test_avg_loss': 0.6668651103973389, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:21:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:21:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:21:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:21:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:21:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=25, total=100)
2025-09-11 07:21:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-11 07:21:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=25, total=100)
2025-09-11 07:21:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=25, total=100)
2025-09-11 07:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:21:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=25
2025-09-11 07:21:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-11 07:21:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=25
2025-09-11 07:21:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=25
2025-09-11 07:21:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=70.423355, avg_loss=0.704234, seen=100, correct=47, accuracy=0.470000
2025-09-11 07:21:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:21:29 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 0 with val results: {'val_total': 100, 'val_loss': 70.42335510253906, 'val_avg_loss': 0.7042335510253906, 'val_seen': 100, 'val_correct': 47, 'val_acc': 0.47}
2025-09-11 07:21:29 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 0 with val results: {'val_total': 100, 'val_loss': 70.42335510253906, 'val_avg_loss': 0.7042335510253906, 'val_seen': 100, 'val_correct': 47, 'val_acc': 0.47}
2025-09-11 07:21:29 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 0 with val results: {'val_total': 100, 'val_loss': 70.42335510253906, 'val_avg_loss': 0.7042335510253906, 'val_seen': 100, 'val_correct': 47, 'val_acc': 0.47}
2025-09-11 07:21:29 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 0 with val results: {'val_total': 100, 'val_loss': 70.42335510253906, 'val_avg_loss': 0.7042335510253906, 'val_seen': 100, 'val_correct': 47, 'val_acc': 0.47}
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:21:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:21:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:21:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.791054, avg_loss=0.694776, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:21:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:21:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:21:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:21:31 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.791053771972656, 'test_avg_loss': 0.6947763442993165, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:21:31 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.791053771972656, 'test_avg_loss': 0.6947763442993165, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:21:31 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.791053771972656, 'test_avg_loss': 0.6947763442993165, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:21:31 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.791053771972656, 'test_avg_loss': 0.6947763442993165, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:21:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-11 07:21:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=25, total=100)
2025-09-11 07:21:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=25, total=100)
2025-09-11 07:21:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=25, total=100)
2025-09-11 07:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:21:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:21:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=25
2025-09-11 07:21:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=25
2025-09-11 07:21:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=25
2025-09-11 07:21:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-11 07:21:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=72.357224, avg_loss=0.723572, seen=100, correct=42, accuracy=0.420000
2025-09-11 07:21:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:36 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 1 with val results: {'val_total': 100, 'val_loss': 72.35722351074219, 'val_avg_loss': 0.7235722351074219, 'val_seen': 100, 'val_correct': 42, 'val_acc': 0.42}
2025-09-11 07:21:36 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 1 with val results: {'val_total': 100, 'val_loss': 72.35722351074219, 'val_avg_loss': 0.7235722351074219, 'val_seen': 100, 'val_correct': 42, 'val_acc': 0.42}
2025-09-11 07:21:36 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 1 with val results: {'val_total': 100, 'val_loss': 72.35722351074219, 'val_avg_loss': 0.7235722351074219, 'val_seen': 100, 'val_correct': 42, 'val_acc': 0.42}
2025-09-11 07:21:36 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 1 with val results: {'val_total': 100, 'val_loss': 72.35722351074219, 'val_avg_loss': 0.7235722351074219, 'val_seen': 100, 'val_correct': 42, 'val_acc': 0.42}
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:21:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:21:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:21:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:21:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.488594, avg_loss=0.737215, seen=40, correct=18, accuracy=0.450000
2025-09-11 07:21:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:39 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.48859405517578, 'test_avg_loss': 0.7372148513793946, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:21:39 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.48859405517578, 'test_avg_loss': 0.7372148513793946, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:21:39 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.48859405517578, 'test_avg_loss': 0.7372148513793946, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:21:39 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.48859405517578, 'test_avg_loss': 0.7372148513793946, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=25, total=100)
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=25, total=100)
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=25, total=100)
2025-09-11 07:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=25
2025-09-11 07:21:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-11 07:21:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=25
2025-09-11 07:21:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=25
2025-09-11 07:21:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=67.798309, avg_loss=0.677983, seen=100, correct=55, accuracy=0.550000
2025-09-11 07:21:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:43 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 2 with val results: {'val_total': 100, 'val_loss': 67.79830932617188, 'val_avg_loss': 0.6779830932617188, 'val_seen': 100, 'val_correct': 55, 'val_acc': 0.55}
2025-09-11 07:21:43 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 2 with val results: {'val_total': 100, 'val_loss': 67.79830932617188, 'val_avg_loss': 0.6779830932617188, 'val_seen': 100, 'val_correct': 55, 'val_acc': 0.55}
2025-09-11 07:21:43 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 2 with val results: {'val_total': 100, 'val_loss': 67.79830932617188, 'val_avg_loss': 0.6779830932617188, 'val_seen': 100, 'val_correct': 55, 'val_acc': 0.55}
2025-09-11 07:21:43 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 2 with val results: {'val_total': 100, 'val_loss': 67.79830932617188, 'val_avg_loss': 0.6779830932617188, 'val_seen': 100, 'val_correct': 55, 'val_acc': 0.55}
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:21:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:21:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:21:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:21:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.757347, avg_loss=0.718934, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:21:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2116MB
2025-09-11 07:21:45 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.757347106933594, 'test_avg_loss': 0.7189336776733398, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:21:45 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.757347106933594, 'test_avg_loss': 0.7189336776733398, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:21:45 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.757347106933594, 'test_avg_loss': 0.7189336776733398, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:21:45 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.757347106933594, 'test_avg_loss': 0.7189336776733398, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:21:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:21:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:21:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:21:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:21:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-11 07:21:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=110)
2025-09-11 07:21:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=27, total=110)
2025-09-11 07:21:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=27, total=110)
2025-09-11 07:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:21:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=27
2025-09-11 07:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=27
2025-09-11 07:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:21:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.614792, avg_loss=0.678316, seen=110, correct=60, accuracy=0.545455
2025-09-11 07:21:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:21:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:21:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:21:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:21:50 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 0 with val results: {'val_total': 110, 'val_loss': 74.61479187011719, 'val_avg_loss': 0.678316289728338, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-11 07:21:50 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 0 with val results: {'val_total': 110, 'val_loss': 74.61479187011719, 'val_avg_loss': 0.678316289728338, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-11 07:21:50 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 0 with val results: {'val_total': 110, 'val_loss': 74.61479187011719, 'val_avg_loss': 0.678316289728338, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-11 07:21:50 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 0 with val results: {'val_total': 110, 'val_loss': 74.61479187011719, 'val_avg_loss': 0.678316289728338, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-11 07:21:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:21:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:21:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:21:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:21:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

(20, 10, 2, 30)
2025-09-11 07:21:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:21:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:21:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:21:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:21:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.353767, avg_loss=0.683844, seen=40, correct=20, accuracy=0.500000
2025-09-11 07:21:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:21:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:21:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:21:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:21:53 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.35376739501953, 'test_avg_loss': 0.6838441848754883, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:21:53 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.35376739501953, 'test_avg_loss': 0.6838441848754883, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:21:53 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.35376739501953, 'test_avg_loss': 0.6838441848754883, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:21:53 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.35376739501953, 'test_avg_loss': 0.6838441848754883, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-11 07:21:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-11 07:21:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=27, total=110)
2025-09-11 07:21:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=110)
2025-09-11 07:21:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=27, total=110)
2025-09-11 07:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:21:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:21:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=27
2025-09-11 07:21:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:21:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:21:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=27
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.087173, avg_loss=0.682611, seen=110, correct=59, accuracy=0.536364
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:21:58 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 1 with val results: {'val_total': 110, 'val_loss': 75.08717346191406, 'val_avg_loss': 0.6826106678355824, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-11 07:21:58 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 1 with val results: {'val_total': 110, 'val_loss': 75.08717346191406, 'val_avg_loss': 0.6826106678355824, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-11 07:21:58 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 1 with val results: {'val_total': 110, 'val_loss': 75.08717346191406, 'val_avg_loss': 0.6826106678355824, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-11 07:21:58 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 1 with val results: {'val_total': 110, 'val_loss': 75.08717346191406, 'val_avg_loss': 0.6826106678355824, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:21:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:21:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:21:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:21:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:21:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.632305, avg_loss=0.690808, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:21:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:00 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.632305145263672, 'test_avg_loss': 0.6908076286315918, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:22:00 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.632305145263672, 'test_avg_loss': 0.6908076286315918, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:22:00 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.632305145263672, 'test_avg_loss': 0.6908076286315918, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:22:00 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.632305145263672, 'test_avg_loss': 0.6908076286315918, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:22:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-11 07:22:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=28, total=110)
2025-09-11 07:22:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=27, total=110)
2025-09-11 07:22:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=27, total=110)
2025-09-11 07:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:22:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:22:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-11 07:22:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=28
2025-09-11 07:22:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=27
2025-09-11 07:22:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=27
2025-09-11 07:22:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=77.645691, avg_loss=0.705870, seen=110, correct=56, accuracy=0.509091
2025-09-11 07:22:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2124MB
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:05 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 2 with val results: {'val_total': 110, 'val_loss': 77.64569091796875, 'val_avg_loss': 0.7058699174360795, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909}
2025-09-11 07:22:05 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 2 with val results: {'val_total': 110, 'val_loss': 77.64569091796875, 'val_avg_loss': 0.7058699174360795, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909}
2025-09-11 07:22:05 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 2 with val results: {'val_total': 110, 'val_loss': 77.64569091796875, 'val_avg_loss': 0.7058699174360795, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909}
2025-09-11 07:22:05 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 2 with val results: {'val_total': 110, 'val_loss': 77.64569091796875, 'val_avg_loss': 0.7058699174360795, 'val_seen': 110, 'val_correct': 56, 'val_acc': 0.509090909090909}
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:22:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:22:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:22:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.830742, avg_loss=0.670769, seen=40, correct=26, accuracy=0.650000
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2124MB
2025-09-11 07:22:08 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.83074188232422, 'test_avg_loss': 0.6707685470581055, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:22:08 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.83074188232422, 'test_avg_loss': 0.6707685470581055, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:22:08 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.83074188232422, 'test_avg_loss': 0.6707685470581055, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:22:08 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.83074188232422, 'test_avg_loss': 0.6707685470581055, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-11 07:22:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-11 07:22:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=37, total=147)
2025-09-11 07:22:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=36, total=147)
2025-09-11 07:22:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=37, total=147)
2025-09-11 07:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:22:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-11 07:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=37
2025-09-11 07:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=36
2025-09-11 07:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=37
2025-09-11 07:22:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.601280, avg_loss=0.704771, seen=147, correct=71, accuracy=0.482993
2025-09-11 07:22:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:22:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:22:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:22:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:22:14 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 0 with val results: {'val_total': 147, 'val_loss': 103.60128021240234, 'val_avg_loss': 0.7047706136898119, 'val_seen': 147, 'val_correct': 71, 'val_acc': 0.48299319727891155}
2025-09-11 07:22:14 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 0 with val results: {'val_total': 147, 'val_loss': 103.60128021240234, 'val_avg_loss': 0.7047706136898119, 'val_seen': 147, 'val_correct': 71, 'val_acc': 0.48299319727891155}
2025-09-11 07:22:14 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 0 with val results: {'val_total': 147, 'val_loss': 103.60128021240234, 'val_avg_loss': 0.7047706136898119, 'val_seen': 147, 'val_correct': 71, 'val_acc': 0.48299319727891155}
2025-09-11 07:22:14 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 0 with val results: {'val_total': 147, 'val_loss': 103.60128021240234, 'val_avg_loss': 0.7047706136898119, 'val_seen': 147, 'val_correct': 71, 'val_acc': 0.48299319727891155}
2025-09-11 07:22:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:22:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:22:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:22:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:22:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:22:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:22:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:22:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:22:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.583046, avg_loss=0.664576, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:22:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:22:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:22:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:22:17 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.583045959472656, 'test_avg_loss': 0.6645761489868164, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:22:17 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.583045959472656, 'test_avg_loss': 0.6645761489868164, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:22:17 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.583045959472656, 'test_avg_loss': 0.6645761489868164, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:22:17 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.583045959472656, 'test_avg_loss': 0.6645761489868164, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:22:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=37, total=147)
2025-09-11 07:22:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-11 07:22:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=37, total=147)
2025-09-11 07:22:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=36, total=147)
2025-09-11 07:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:22:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=37
2025-09-11 07:22:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-11 07:22:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=36
2025-09-11 07:22:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=37
2025-09-11 07:22:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=104.706062, avg_loss=0.712286, seen=147, correct=70, accuracy=0.476190
2025-09-11 07:22:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:23 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 1 with val results: {'val_total': 147, 'val_loss': 104.70606231689453, 'val_avg_loss': 0.7122861382101668, 'val_seen': 147, 'val_correct': 70, 'val_acc': 0.47619047619047616}
2025-09-11 07:22:23 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 1 with val results: {'val_total': 147, 'val_loss': 104.70606231689453, 'val_avg_loss': 0.7122861382101668, 'val_seen': 147, 'val_correct': 70, 'val_acc': 0.47619047619047616}
2025-09-11 07:22:23 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 1 with val results: {'val_total': 147, 'val_loss': 104.70606231689453, 'val_avg_loss': 0.7122861382101668, 'val_seen': 147, 'val_correct': 70, 'val_acc': 0.47619047619047616}
2025-09-11 07:22:23 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 1 with val results: {'val_total': 147, 'val_loss': 104.70606231689453, 'val_avg_loss': 0.7122861382101668, 'val_seen': 147, 'val_correct': 70, 'val_acc': 0.47619047619047616}
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:22:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:22:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:22:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:22:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:22:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.726547, avg_loss=0.668164, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:22:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:26 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.726547241210938, 'test_avg_loss': 0.6681636810302735, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:22:26 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.726547241210938, 'test_avg_loss': 0.6681636810302735, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:22:26 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.726547241210938, 'test_avg_loss': 0.6681636810302735, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:22:26 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.726547241210938, 'test_avg_loss': 0.6681636810302735, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=37, total=147)
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=36, total=147)
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=37, total=147)
2025-09-11 07:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-11 07:22:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=37
2025-09-11 07:22:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=36
2025-09-11 07:22:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=37
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=105.547989, avg_loss=0.718014, seen=147, correct=70, accuracy=0.476190
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2133MB
2025-09-11 07:22:32 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 2 with val results: {'val_total': 147, 'val_loss': 105.54798889160156, 'val_avg_loss': 0.7180135298748406, 'val_seen': 147, 'val_correct': 70, 'val_acc': 0.47619047619047616}
2025-09-11 07:22:32 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 2 with val results: {'val_total': 147, 'val_loss': 105.54798889160156, 'val_avg_loss': 0.7180135298748406, 'val_seen': 147, 'val_correct': 70, 'val_acc': 0.47619047619047616}
2025-09-11 07:22:32 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 2 with val results: {'val_total': 147, 'val_loss': 105.54798889160156, 'val_avg_loss': 0.7180135298748406, 'val_seen': 147, 'val_correct': 70, 'val_acc': 0.47619047619047616}
2025-09-11 07:22:32 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 2 with val results: {'val_total': 147, 'val_loss': 105.54798889160156, 'val_avg_loss': 0.7180135298748406, 'val_seen': 147, 'val_correct': 70, 'val_acc': 0.47619047619047616}
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:22:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.336002, avg_loss=0.683400, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:22:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2133MB
2025-09-11 07:22:34 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.336002349853516, 'test_avg_loss': 0.6834000587463379, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:22:34 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.336002349853516, 'test_avg_loss': 0.6834000587463379, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:22:34 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.336002349853516, 'test_avg_loss': 0.6834000587463379, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:22:34 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.336002349853516, 'test_avg_loss': 0.6834000587463379, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:22:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-11 07:22:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=11, total=46)
2025-09-11 07:22:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=12, total=46)
2025-09-11 07:22:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=11, total=46)
2025-09-11 07:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(23, 7, 2, 30)
2025-09-11 07:22:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=11
2025-09-11 07:22:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-11 07:22:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=11
2025-09-11 07:22:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=12
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.929436, avg_loss=0.715857, seen=46, correct=23, accuracy=0.500000
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:22:38 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 0 with val results: {'val_total': 46, 'val_loss': 32.92943572998047, 'val_avg_loss': 0.7158572984778363, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5}
2025-09-11 07:22:38 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 0 with val results: {'val_total': 46, 'val_loss': 32.92943572998047, 'val_avg_loss': 0.7158572984778363, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5}
2025-09-11 07:22:38 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 0 with val results: {'val_total': 46, 'val_loss': 32.92943572998047, 'val_avg_loss': 0.7158572984778363, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5}
2025-09-11 07:22:38 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 0 with val results: {'val_total': 46, 'val_loss': 32.92943572998047, 'val_avg_loss': 0.7158572984778363, 'val_seen': 46, 'val_correct': 23, 'val_acc': 0.5}
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:22:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:22:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:22:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:22:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.587473, avg_loss=0.689687, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:22:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:22:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:22:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:22:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:22:41 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.587472915649414, 'test_avg_loss': 0.6896868228912354, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:22:41 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.587472915649414, 'test_avg_loss': 0.6896868228912354, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:22:41 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.587472915649414, 'test_avg_loss': 0.6896868228912354, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:22:41 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.587472915649414, 'test_avg_loss': 0.6896868228912354, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:22:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=11, total=46)
2025-09-11 07:22:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=11, total=46)
2025-09-11 07:22:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-11 07:22:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=12, total=46)
2025-09-11 07:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(23, 7, 2, 30)
2025-09-11 07:22:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-11 07:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=11
2025-09-11 07:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=12
2025-09-11 07:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=11
2025-09-11 07:22:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.263004, avg_loss=0.679631, seen=46, correct=21, accuracy=0.456522
2025-09-11 07:22:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2141MB
2025-09-11 07:22:44 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 1 with val results: {'val_total': 46, 'val_loss': 31.263004302978516, 'val_avg_loss': 0.6796305283256199, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-11 07:22:44 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 1 with val results: {'val_total': 46, 'val_loss': 31.263004302978516, 'val_avg_loss': 0.6796305283256199, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-11 07:22:44 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 1 with val results: {'val_total': 46, 'val_loss': 31.263004302978516, 'val_avg_loss': 0.6796305283256199, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-11 07:22:44 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 1 with val results: {'val_total': 46, 'val_loss': 31.263004302978516, 'val_avg_loss': 0.6796305283256199, 'val_seen': 46, 'val_correct': 21, 'val_acc': 0.45652173913043476}
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:22:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:22:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.329939, avg_loss=0.683248, seen=40, correct=24, accuracy=0.600000
2025-09-11 07:22:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:46 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.329938888549805, 'test_avg_loss': 0.6832484722137451, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:22:46 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.329938888549805, 'test_avg_loss': 0.6832484722137451, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:22:46 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.329938888549805, 'test_avg_loss': 0.6832484722137451, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:22:46 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.329938888549805, 'test_avg_loss': 0.6832484722137451, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-11 07:22:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-11 07:22:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=11, total=46)
2025-09-11 07:22:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=12, total=46)
2025-09-11 07:22:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=11, total=46)
2025-09-11 07:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(23, 7, 2, 30)
2025-09-11 07:22:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(23, 7, 2, 30)
2025-09-11 07:22:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=7, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-11 07:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=12
2025-09-11 07:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=11
2025-09-11 07:22:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=11
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.593945, avg_loss=0.686825, seen=46, correct=26, accuracy=0.565217
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:49 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 2 with val results: {'val_total': 46, 'val_loss': 31.593944549560547, 'val_avg_loss': 0.6868248815121858, 'val_seen': 46, 'val_correct': 26, 'val_acc': 0.5652173913043478}
2025-09-11 07:22:49 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 2 with val results: {'val_total': 46, 'val_loss': 31.593944549560547, 'val_avg_loss': 0.6868248815121858, 'val_seen': 46, 'val_correct': 26, 'val_acc': 0.5652173913043478}
2025-09-11 07:22:49 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 2 with val results: {'val_total': 46, 'val_loss': 31.593944549560547, 'val_avg_loss': 0.6868248815121858, 'val_seen': 46, 'val_correct': 26, 'val_acc': 0.5652173913043478}
2025-09-11 07:22:49 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 2 with val results: {'val_total': 46, 'val_loss': 31.593944549560547, 'val_avg_loss': 0.6868248815121858, 'val_seen': 46, 'val_correct': 26, 'val_acc': 0.5652173913043478}
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:22:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:22:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:22:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.049343, avg_loss=0.751234, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2141MB
2025-09-11 07:22:51 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.04934310913086, 'test_avg_loss': 0.7512335777282715, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:22:51 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.04934310913086, 'test_avg_loss': 0.7512335777282715, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:22:51 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.04934310913086, 'test_avg_loss': 0.7512335777282715, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:22:51 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.04934310913086, 'test_avg_loss': 0.7512335777282715, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:22:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:22:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=33, total=132)
2025-09-11 07:22:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-11 07:22:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=132)
2025-09-11 07:22:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=132)
2025-09-11 07:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:22:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:22:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-11 07:22:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:22:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=33
2025-09-11 07:22:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:22:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.326721, avg_loss=0.669142, seen=132, correct=85, accuracy=0.643939
2025-09-11 07:22:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:22:57 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 0 with val results: {'val_total': 132, 'val_loss': 88.32672119140625, 'val_avg_loss': 0.669141827207623, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-11 07:22:57 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 0 with val results: {'val_total': 132, 'val_loss': 88.32672119140625, 'val_avg_loss': 0.669141827207623, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-11 07:22:57 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 0 with val results: {'val_total': 132, 'val_loss': 88.32672119140625, 'val_avg_loss': 0.669141827207623, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-11 07:22:57 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 0 with val results: {'val_total': 132, 'val_loss': 88.32672119140625, 'val_avg_loss': 0.669141827207623, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:22:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:22:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:22:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.181904, avg_loss=0.729548, seen=40, correct=17, accuracy=0.425000
2025-09-11 07:22:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:22:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:22:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:23:00 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.181903839111328, 'test_avg_loss': 0.7295475959777832, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:23:00 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.181903839111328, 'test_avg_loss': 0.7295475959777832, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:23:00 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.181903839111328, 'test_avg_loss': 0.7295475959777832, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:23:00 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.181903839111328, 'test_avg_loss': 0.7295475959777832, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=132)
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=132)
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=33, total=132)
2025-09-11 07:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-11 07:23:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:23:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=33
2025-09-11 07:23:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:23:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.215492, avg_loss=0.668299, seen=132, correct=79, accuracy=0.598485
2025-09-11 07:23:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:06 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 1 with val results: {'val_total': 132, 'val_loss': 88.21549224853516, 'val_avg_loss': 0.6682991837010239, 'val_seen': 132, 'val_correct': 79, 'val_acc': 0.5984848484848485}
2025-09-11 07:23:06 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 1 with val results: {'val_total': 132, 'val_loss': 88.21549224853516, 'val_avg_loss': 0.6682991837010239, 'val_seen': 132, 'val_correct': 79, 'val_acc': 0.5984848484848485}
2025-09-11 07:23:06 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 1 with val results: {'val_total': 132, 'val_loss': 88.21549224853516, 'val_avg_loss': 0.6682991837010239, 'val_seen': 132, 'val_correct': 79, 'val_acc': 0.5984848484848485}
2025-09-11 07:23:06 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 1 with val results: {'val_total': 132, 'val_loss': 88.21549224853516, 'val_avg_loss': 0.6682991837010239, 'val_seen': 132, 'val_correct': 79, 'val_acc': 0.5984848484848485}
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:23:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:23:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:23:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:23:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.330727, avg_loss=0.733268, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:23:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:08 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.330726623535156, 'test_avg_loss': 0.7332681655883789, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:08 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.330726623535156, 'test_avg_loss': 0.7332681655883789, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:08 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.330726623535156, 'test_avg_loss': 0.7332681655883789, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:08 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.330726623535156, 'test_avg_loss': 0.7332681655883789, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=132)
2025-09-11 07:23:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-11 07:23:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=132)
2025-09-11 07:23:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=33, total=132)
2025-09-11 07:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:23:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:23:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-11 07:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=33
2025-09-11 07:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:23:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=90.192505, avg_loss=0.683277, seen=132, correct=75, accuracy=0.568182
2025-09-11 07:23:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:14 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 2 with val results: {'val_total': 132, 'val_loss': 90.1925048828125, 'val_avg_loss': 0.6832765521425189, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182}
2025-09-11 07:23:14 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 2 with val results: {'val_total': 132, 'val_loss': 90.1925048828125, 'val_avg_loss': 0.6832765521425189, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182}
2025-09-11 07:23:14 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 2 with val results: {'val_total': 132, 'val_loss': 90.1925048828125, 'val_avg_loss': 0.6832765521425189, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182}
2025-09-11 07:23:14 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 2 with val results: {'val_total': 132, 'val_loss': 90.1925048828125, 'val_avg_loss': 0.6832765521425189, 'val_seen': 132, 'val_correct': 75, 'val_acc': 0.5681818181818182}
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

(20, 10, 2, 30)
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:23:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:23:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:23:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:23:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.909754, avg_loss=0.722744, seen=40, correct=17, accuracy=0.425000
2025-09-11 07:23:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2150MB
2025-09-11 07:23:17 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.909753799438477, 'test_avg_loss': 0.7227438449859619, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:23:17 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.909753799438477, 'test_avg_loss': 0.7227438449859619, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:23:17 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.909753799438477, 'test_avg_loss': 0.7227438449859619, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:23:17 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.909753799438477, 'test_avg_loss': 0.7227438449859619, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-11 07:23:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:23:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:23:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:23:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:23:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-11 07:23:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=33, total=133)
2025-09-11 07:23:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=133)
2025-09-11 07:23:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=133)
2025-09-11 07:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:23:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:23:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:23:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:23:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=33
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.059418, avg_loss=0.692176, seen=133, correct=76, accuracy=0.571429
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:23:23 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 0 with val results: {'val_total': 133, 'val_loss': 92.05941772460938, 'val_avg_loss': 0.6921760731173637, 'val_seen': 133, 'val_correct': 76, 'val_acc': 0.5714285714285714}
2025-09-11 07:23:23 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 0 with val results: {'val_total': 133, 'val_loss': 92.05941772460938, 'val_avg_loss': 0.6921760731173637, 'val_seen': 133, 'val_correct': 76, 'val_acc': 0.5714285714285714}
2025-09-11 07:23:23 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 0 with val results: {'val_total': 133, 'val_loss': 92.05941772460938, 'val_avg_loss': 0.6921760731173637, 'val_seen': 133, 'val_correct': 76, 'val_acc': 0.5714285714285714}
2025-09-11 07:23:23 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 0 with val results: {'val_total': 133, 'val_loss': 92.05941772460938, 'val_avg_loss': 0.6921760731173637, 'val_seen': 133, 'val_correct': 76, 'val_acc': 0.5714285714285714}
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:23:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:23:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:23:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:23:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.626808, avg_loss=0.715670, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:23:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:23:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:23:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:23:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:23:25 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.626808166503906, 'test_avg_loss': 0.7156702041625976, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:25 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.626808166503906, 'test_avg_loss': 0.7156702041625976, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:25 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.626808166503906, 'test_avg_loss': 0.7156702041625976, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:25 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.626808166503906, 'test_avg_loss': 0.7156702041625976, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-11 07:23:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=133)
2025-09-11 07:23:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=133)
2025-09-11 07:23:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=33, total=133)
2025-09-11 07:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:23:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:23:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:23:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:23:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:23:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=33
2025-09-11 07:23:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.394424, avg_loss=0.687176, seen=133, correct=70, accuracy=0.526316
2025-09-11 07:23:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:32 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 1 with val results: {'val_total': 133, 'val_loss': 91.39442443847656, 'val_avg_loss': 0.6871761235975682, 'val_seen': 133, 'val_correct': 70, 'val_acc': 0.5263157894736842}
2025-09-11 07:23:32 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 1 with val results: {'val_total': 133, 'val_loss': 91.39442443847656, 'val_avg_loss': 0.6871761235975682, 'val_seen': 133, 'val_correct': 70, 'val_acc': 0.5263157894736842}
2025-09-11 07:23:32 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 1 with val results: {'val_total': 133, 'val_loss': 91.39442443847656, 'val_avg_loss': 0.6871761235975682, 'val_seen': 133, 'val_correct': 70, 'val_acc': 0.5263157894736842}
2025-09-11 07:23:32 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 1 with val results: {'val_total': 133, 'val_loss': 91.39442443847656, 'val_avg_loss': 0.6871761235975682, 'val_seen': 133, 'val_correct': 70, 'val_acc': 0.5263157894736842}
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:23:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:23:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:23:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:23:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:23:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.620750, avg_loss=0.715519, seen=40, correct=16, accuracy=0.400000
2025-09-11 07:23:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:34 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.620750427246094, 'test_avg_loss': 0.7155187606811524, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:34 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.620750427246094, 'test_avg_loss': 0.7155187606811524, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:34 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.620750427246094, 'test_avg_loss': 0.7155187606811524, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:34 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.620750427246094, 'test_avg_loss': 0.7155187606811524, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-11 07:23:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-11 07:23:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=33, total=133)
2025-09-11 07:23:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=33, total=133)
2025-09-11 07:23:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=33, total=133)
2025-09-11 07:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:23:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=33
2025-09-11 07:23:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-11 07:23:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=33
2025-09-11 07:23:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=33
2025-09-11 07:23:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.938713, avg_loss=0.691269, seen=133, correct=69, accuracy=0.518797
2025-09-11 07:23:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:40 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 2 with val results: {'val_total': 133, 'val_loss': 91.93871307373047, 'val_avg_loss': 0.6912685193513569, 'val_seen': 133, 'val_correct': 69, 'val_acc': 0.518796992481203}
2025-09-11 07:23:40 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 2 with val results: {'val_total': 133, 'val_loss': 91.93871307373047, 'val_avg_loss': 0.6912685193513569, 'val_seen': 133, 'val_correct': 69, 'val_acc': 0.518796992481203}
2025-09-11 07:23:40 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 2 with val results: {'val_total': 133, 'val_loss': 91.93871307373047, 'val_avg_loss': 0.6912685193513569, 'val_seen': 133, 'val_correct': 69, 'val_acc': 0.518796992481203}
2025-09-11 07:23:40 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 2 with val results: {'val_total': 133, 'val_loss': 91.93871307373047, 'val_avg_loss': 0.6912685193513569, 'val_seen': 133, 'val_correct': 69, 'val_acc': 0.518796992481203}
2025-09-11 07:23:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:23:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:23:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:23:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:23:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:23:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:23:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:23:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:23:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.432529, avg_loss=0.710813, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2158MB
2025-09-11 07:23:43 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.43252944946289, 'test_avg_loss': 0.7108132362365722, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:23:43 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.43252944946289, 'test_avg_loss': 0.7108132362365722, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:23:43 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.43252944946289, 'test_avg_loss': 0.7108132362365722, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:23:43 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.43252944946289, 'test_avg_loss': 0.7108132362365722, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:23:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:23:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:23:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:23:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:23:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=21, total=83)
2025-09-11 07:23:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=21, total=83)
2025-09-11 07:23:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-11 07:23:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=20, total=83)
2025-09-11 07:23:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:23:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=21
2025-09-11 07:23:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-11 07:23:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=20
2025-09-11 07:23:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=21
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.465031, avg_loss=0.704398, seen=83, correct=44, accuracy=0.530120
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-11 07:23:47 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 0 with val results: {'val_total': 83, 'val_loss': 58.465030670166016, 'val_avg_loss': 0.7043979598815182, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-11 07:23:47 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 0 with val results: {'val_total': 83, 'val_loss': 58.465030670166016, 'val_avg_loss': 0.7043979598815182, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-11 07:23:47 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 0 with val results: {'val_total': 83, 'val_loss': 58.465030670166016, 'val_avg_loss': 0.7043979598815182, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-11 07:23:47 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 0 with val results: {'val_total': 83, 'val_loss': 58.465030670166016, 'val_avg_loss': 0.7043979598815182, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2

2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:23:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:23:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:23:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:23:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.297331, avg_loss=0.682433, seen=40, correct=27, accuracy=0.675000
2025-09-11 07:23:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-11 07:23:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-11 07:23:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-11 07:23:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2192MB
2025-09-11 07:23:49 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.297330856323242, 'test_avg_loss': 0.6824332714080811, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:23:49 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.297330856323242, 'test_avg_loss': 0.6824332714080811, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:23:49 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.297330856323242, 'test_avg_loss': 0.6824332714080811, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:23:49 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.297330856323242, 'test_avg_loss': 0.6824332714080811, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-11 07:23:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=20, total=83)
2025-09-11 07:23:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=21, total=83)
2025-09-11 07:23:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-11 07:23:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=21, total=83)
2025-09-11 07:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:23:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-11 07:23:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=21
2025-09-11 07:23:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=20
2025-09-11 07:23:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=21
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.494129, avg_loss=0.680652, seen=83, correct=48, accuracy=0.578313
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:53 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 1 with val results: {'val_total': 83, 'val_loss': 56.4941291809082, 'val_avg_loss': 0.680652158806123, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-11 07:23:53 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 1 with val results: {'val_total': 83, 'val_loss': 56.4941291809082, 'val_avg_loss': 0.680652158806123, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-11 07:23:53 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 1 with val results: {'val_total': 83, 'val_loss': 56.4941291809082, 'val_avg_loss': 0.680652158806123, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-11 07:23:53 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 1 with val results: {'val_total': 83, 'val_loss': 56.4941291809082, 'val_avg_loss': 0.680652158806123, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:23:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:23:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:23:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:23:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.982117, avg_loss=0.699553, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:23:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:55 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.98211669921875, 'test_avg_loss': 0.6995529174804688, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:23:55 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.98211669921875, 'test_avg_loss': 0.6995529174804688, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:23:55 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.98211669921875, 'test_avg_loss': 0.6995529174804688, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:23:55 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.98211669921875, 'test_avg_loss': 0.6995529174804688, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:23:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-11 07:23:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=21, total=83)
2025-09-11 07:23:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=20, total=83)
2025-09-11 07:23:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=21, total=83)
2025-09-11 07:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:23:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:23:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=21
2025-09-11 07:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-11 07:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=21
2025-09-11 07:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=20
2025-09-11 07:23:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.282684, avg_loss=0.678105, seen=83, correct=47, accuracy=0.566265
2025-09-11 07:23:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:23:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:23:58 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 2 with val results: {'val_total': 83, 'val_loss': 56.282684326171875, 'val_avg_loss': 0.6781046304358057, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-11 07:23:58 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 2 with val results: {'val_total': 83, 'val_loss': 56.282684326171875, 'val_avg_loss': 0.6781046304358057, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-11 07:23:58 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 2 with val results: {'val_total': 83, 'val_loss': 56.282684326171875, 'val_avg_loss': 0.6781046304358057, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-11 07:23:58 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 2 with val results: {'val_total': 83, 'val_loss': 56.282684326171875, 'val_avg_loss': 0.6781046304358057, 'val_seen': 83, 'val_correct': 47, 'val_acc': 0.5662650602409639}
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:23:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:23:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:23:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:23:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.714863, avg_loss=0.717872, seen=40, correct=18, accuracy=0.450000
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:24:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:24:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:24:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2166MB
2025-09-11 07:24:01 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.714862823486328, 'test_avg_loss': 0.7178715705871582, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:24:01 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.714862823486328, 'test_avg_loss': 0.7178715705871582, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:24:01 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.714862823486328, 'test_avg_loss': 0.7178715705871582, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:24:01 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.714862823486328, 'test_avg_loss': 0.7178715705871582, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-11 07:24:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:24:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:24:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:24:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:24:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=47, total=188)
2025-09-11 07:24:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=47, total=188)
2025-09-11 07:24:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=47, total=188)
2025-09-11 07:24:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-11 07:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:24:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=47
2025-09-11 07:24:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=47
2025-09-11 07:24:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-11 07:24:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=47
2025-09-11 07:24:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=130.726166, avg_loss=0.695352, seen=188, correct=97, accuracy=0.515957
2025-09-11 07:24:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-11 07:24:10 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 0 with val results: {'val_total': 188, 'val_loss': 130.72616577148438, 'val_avg_loss': 0.695351945593002, 'val_seen': 188, 'val_correct': 97, 'val_acc': 0.5159574468085106}
2025-09-11 07:24:10 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 0 with val results: {'val_total': 188, 'val_loss': 130.72616577148438, 'val_avg_loss': 0.695351945593002, 'val_seen': 188, 'val_correct': 97, 'val_acc': 0.5159574468085106}
2025-09-11 07:24:10 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 0 with val results: {'val_total': 188, 'val_loss': 130.72616577148438, 'val_avg_loss': 0.695351945593002, 'val_seen': 188, 'val_correct': 97, 'val_acc': 0.5159574468085106}
2025-09-11 07:24:10 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 0 with val results: {'val_total': 188, 'val_loss': 130.72616577148438, 'val_avg_loss': 0.695351945593002, 'val_seen': 188, 'val_correct': 97, 'val_acc': 0.5159574468085106}
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)(20, 10, 2, 30)

2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:24:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:24:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:24:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:24:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.652817, avg_loss=0.691320, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:24:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-11 07:24:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-11 07:24:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-11 07:24:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2200MB
2025-09-11 07:24:12 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.652816772460938, 'test_avg_loss': 0.6913204193115234, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:24:12 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.652816772460938, 'test_avg_loss': 0.6913204193115234, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:24:12 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.652816772460938, 'test_avg_loss': 0.6913204193115234, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:24:12 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.652816772460938, 'test_avg_loss': 0.6913204193115234, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:24:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=47, total=188)
2025-09-11 07:24:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-11 07:24:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=47, total=188)
2025-09-11 07:24:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=47, total=188)
2025-09-11 07:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:24:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=47
2025-09-11 07:24:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-11 07:24:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=47
2025-09-11 07:24:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=47
2025-09-11 07:24:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=130.056015, avg_loss=0.691787, seen=188, correct=101, accuracy=0.537234
2025-09-11 07:24:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:21 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 1 with val results: {'val_total': 188, 'val_loss': 130.05601501464844, 'val_avg_loss': 0.6917873139077044, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-11 07:24:21 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 1 with val results: {'val_total': 188, 'val_loss': 130.05601501464844, 'val_avg_loss': 0.6917873139077044, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-11 07:24:21 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 1 with val results: {'val_total': 188, 'val_loss': 130.05601501464844, 'val_avg_loss': 0.6917873139077044, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-11 07:24:21 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 1 with val results: {'val_total': 188, 'val_loss': 130.05601501464844, 'val_avg_loss': 0.6917873139077044, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:24:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:24:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:24:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:24:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:24:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.108313, avg_loss=0.677708, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:24:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:23 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.108312606811523, 'test_avg_loss': 0.6777078151702881, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:24:23 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.108312606811523, 'test_avg_loss': 0.6777078151702881, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:24:23 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.108312606811523, 'test_avg_loss': 0.6777078151702881, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:24:23 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.108312606811523, 'test_avg_loss': 0.6777078151702881, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:24:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-11 07:24:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=47, total=188)
2025-09-11 07:24:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=47, total=188)
2025-09-11 07:24:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=47, total=188)
2025-09-11 07:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:24:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:24:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=47
2025-09-11 07:24:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=47
2025-09-11 07:24:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-11 07:24:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=47
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=131.832565, avg_loss=0.701237, seen=188, correct=101, accuracy=0.537234
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:31 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 2 with val results: {'val_total': 188, 'val_loss': 131.8325653076172, 'val_avg_loss': 0.7012370495086021, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-11 07:24:31 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 2 with val results: {'val_total': 188, 'val_loss': 131.8325653076172, 'val_avg_loss': 0.7012370495086021, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-11 07:24:31 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 2 with val results: {'val_total': 188, 'val_loss': 131.8325653076172, 'val_avg_loss': 0.7012370495086021, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-11 07:24:31 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 2 with val results: {'val_total': 188, 'val_loss': 131.8325653076172, 'val_avg_loss': 0.7012370495086021, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:24:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:24:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:24:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:24:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.861570, avg_loss=0.696539, seen=40, correct=21, accuracy=0.525000
2025-09-11 07:24:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2175MB
2025-09-11 07:24:33 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.861570358276367, 'test_avg_loss': 0.6965392589569092, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:24:33 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.861570358276367, 'test_avg_loss': 0.6965392589569092, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:24:33 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.861570358276367, 'test_avg_loss': 0.6965392589569092, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:24:33 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.861570358276367, 'test_avg_loss': 0.6965392589569092, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-11 07:24:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:24:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:24:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:24:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:24:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:24:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:24:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:24:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:24:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:24:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:24:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:24:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:24:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.390305, avg_loss=0.686952, seen=200, correct=106, accuracy=0.530000
2025-09-11 07:24:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-11 07:24:41 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.3903045654297, 'val_avg_loss': 0.6869515228271484, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-11 07:24:41 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.3903045654297, 'val_avg_loss': 0.6869515228271484, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-11 07:24:41 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.3903045654297, 'val_avg_loss': 0.6869515228271484, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-11 07:24:41 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.3903045654297, 'val_avg_loss': 0.6869515228271484, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:24:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:24:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:24:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.417561, avg_loss=0.685439, seen=40, correct=23, accuracy=0.575000
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-11 07:24:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-11 07:24:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-11 07:24:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2208MB
2025-09-11 07:24:44 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.417560577392578, 'test_avg_loss': 0.6854390144348145, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:24:44 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.417560577392578, 'test_avg_loss': 0.6854390144348145, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:24:44 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.417560577392578, 'test_avg_loss': 0.6854390144348145, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:24:44 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.417560577392578, 'test_avg_loss': 0.6854390144348145, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-11 07:24:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:24:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:24:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:24:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:24:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:24:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:24:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:24:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:24:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:24:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:24:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.124634, avg_loss=0.685623, seen=200, correct=111, accuracy=0.555000
2025-09-11 07:24:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:24:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:24:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:24:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:24:51 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.1246337890625, 'val_avg_loss': 0.6856231689453125, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-11 07:24:51 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.1246337890625, 'val_avg_loss': 0.6856231689453125, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-11 07:24:51 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.1246337890625, 'val_avg_loss': 0.6856231689453125, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-11 07:24:51 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 1 with val results: {'val_total': 200, 'val_loss': 137.1246337890625, 'val_avg_loss': 0.6856231689453125, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:24:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:24:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:24:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.535789, avg_loss=0.688395, seen=40, correct=22, accuracy=0.550000
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:24:54 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.535789489746094, 'test_avg_loss': 0.6883947372436523, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:24:54 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.535789489746094, 'test_avg_loss': 0.6883947372436523, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:24:54 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.535789489746094, 'test_avg_loss': 0.6883947372436523, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:24:54 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.535789489746094, 'test_avg_loss': 0.6883947372436523, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=50, total=200)
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=50, total=200)
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=50, total=200)
2025-09-11 07:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:24:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(30, 30, 1, 30)
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
(30, 30, 1, 30)(30, 30, 1, 30)

2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:24:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-11 07:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=50
2025-09-11 07:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=50
2025-09-11 07:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=50
2025-09-11 07:25:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.200897, avg_loss=0.696004, seen=200, correct=100, accuracy=0.500000
2025-09-11 07:25:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:25:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:25:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:25:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:25:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:25:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:25:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:25:01 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.20089721679688, 'val_avg_loss': 0.6960044860839844, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-11 07:25:01 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.20089721679688, 'val_avg_loss': 0.6960044860839844, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-11 07:25:01 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.20089721679688, 'val_avg_loss': 0.6960044860839844, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-11 07:25:01 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 2 with val results: {'val_total': 200, 'val_loss': 139.20089721679688, 'val_avg_loss': 0.6960044860839844, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-11 07:25:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=10, total=40)
2025-09-11 07:25:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-11 07:25:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=10, total=40)
2025-09-11 07:25:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=10, total=40)
2025-09-11 07:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
(20, 10, 2, 30)
2025-09-11 07:25:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
(20, 10, 2, 30)
(20, 10, 2, 30)
(20, 10, 2, 30)
2025-09-11 07:25:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:25:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:25:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=10, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:25:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=10
2025-09-11 07:25:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-11 07:25:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=10
2025-09-11 07:25:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=10
2025-09-11 07:25:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.873661, avg_loss=0.721842, seen=40, correct=19, accuracy=0.475000
2025-09-11 07:25:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:25:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:25:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:25:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-11 07:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:25:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:25:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:25:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2302MB allocated=2183MB
2025-09-11 07:25:04 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.873661041259766, 'test_avg_loss': 0.7218415260314941, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:25:04 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.873661041259766, 'test_avg_loss': 0.7218415260314941, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:25:04 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.873661041259766, 'test_avg_loss': 0.7218415260314941, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:25:04 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.873661041259766, 'test_avg_loss': 0.7218415260314941, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-11 07:25:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:25:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:25:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:25:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:25:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-11 07:25:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-11 07:25:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-11 07:25:05 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-11 07:25:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:25:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:25:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:25:05 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:25:05 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:25:05 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:25:05 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:25:05 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=1979, total=7916)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=1979, total=7916)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=1979, total=7916)
2025-09-11 07:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:25:06 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-09-11 07:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:36 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=120
2025-09-11 07:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-09-11 07:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=120
2025-09-11 07:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=120
2025-09-11 07:25:36 (federatedscope.llm.trainer.trainer:1217) INFO: [train|final] total=480, loss_sum=332.563995, avg_loss=0.692842, seen=480, correct=263, accuracy=0.547917
2025-09-11 07:25:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:25:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:25:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:25:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2398MB allocated=2217MB
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2342MB allocated=2217MB
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2392MB allocated=2217MB
2025-09-11 07:25:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '3/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.26350170373917, 'train_avg_loss': 0.7021958475311597, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-09-11 07:25:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.35621166229248, 'train_avg_loss': 0.677968430519104, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-09-11 07:25:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 332.5639953613281, 'train_avg_loss': 0.6928416570027669, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-09-11 07:25:38 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 332.5639953613281, 'train_avg_loss': 0.6928416570027669, 'train_seen': 480, 'train_correct': 263, 'train_acc': 0.5479166666666667}}
2025-09-11 07:25:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '1/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.75924879312515, 'train_avg_loss': 0.7063270732760429, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-09-11 07:25:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '2/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.18503051996231, 'train_avg_loss': 0.6848752543330192, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-09-11 07:25:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:25:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:25:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:25:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:25:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-11 07:25:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-11 07:25:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-11 07:25:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:25:38 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=353, total=1409)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=353, total=1409)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=353, total=1409)
2025-09-11 07:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:46 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:47 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:25:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=120
2025-09-11 07:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-09-11 07:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=120
2025-09-11 07:26:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=120
2025-09-11 07:26:09 (federatedscope.llm.trainer.trainer:1217) INFO: [train|final] total=480, loss_sum=334.222473, avg_loss=0.696297, seen=480, correct=265, accuracy=0.552083
2025-09-11 07:26:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:26:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:26:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:26:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:26:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:26:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:26:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2388MB allocated=2217MB
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2376MB allocated=2217MB
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-11 07:26:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '1/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.10646486282349, 'train_avg_loss': 0.6842205405235291, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-09-11 07:26:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '3/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.27644836902618, 'train_avg_loss': 0.6939704030752182, 'train_seen': 120, 'train_correct': 66, 'train_acc': 0.55}}
2025-09-11 07:26:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '2/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.64814746379852, 'train_avg_loss': 0.7054012288649877, 'train_seen': 120, 'train_correct': 67, 'train_acc': 0.5583333333333333}}
2025-09-11 07:26:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.19141566753387, 'train_avg_loss': 0.7015951305627823, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-09-11 07:26:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 334.22247314453125, 'train_avg_loss': 0.6962968190511067, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-09-11 07:26:12 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 334.22247314453125, 'train_avg_loss': 0.6962968190511067, 'train_seen': 480, 'train_correct': 265, 'train_acc': 0.5520833333333334}}
2025-09-11 07:26:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:26:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:26:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:26:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:26:12 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:26:12 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:26:12 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:26:12 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=399, total=1594)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=399, total=1594)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=399, total=1594)
2025-09-11 07:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:26:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-09-11 07:26:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:20 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:21 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:22 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:23 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:26 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:27 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:28 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:29 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:30 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:31 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:32 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:33 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:34 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:35 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:37 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:38 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:39 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:40 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:41 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:42 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:43 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:44 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-09-11 07:26:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=120
2025-09-11 07:26:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=120
2025-09-11 07:26:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=120
2025-09-11 07:26:44 (federatedscope.llm.trainer.trainer:1217) INFO: [train|final] total=480, loss_sum=333.068298, avg_loss=0.693892, seen=480, correct=254, accuracy=0.529167
2025-09-11 07:26:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:26:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:26:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:26:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:26:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:26:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:26:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:26:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2322MB allocated=2217MB
2025-09-11 07:26:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-11 07:26:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-11 07:26:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-11 07:26:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '3/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.37437903881073, 'train_avg_loss': 0.6947864919900895, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-09-11 07:26:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '1/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 83.37774932384491, 'train_avg_loss': 0.6948145776987076, 'train_seen': 120, 'train_correct': 70, 'train_acc': 0.5833333333333334}}
2025-09-11 07:26:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.39084935188293, 'train_avg_loss': 0.6782570779323578, 'train_seen': 120, 'train_correct': 64, 'train_acc': 0.5333333333333333}}
2025-09-11 07:26:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 333.06829833984375, 'train_avg_loss': 0.6938922882080079, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-09-11 07:26:46 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 333.06829833984375, 'train_avg_loss': 0.6938922882080079, 'train_seen': 480, 'train_correct': 254, 'train_acc': 0.5291666666666667}}
2025-09-11 07:26:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '2/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.92534184455872, 'train_avg_loss': 0.7077111820379893, 'train_seen': 120, 'train_correct': 58, 'train_acc': 0.48333333333333334}}
2025-09-11 07:26:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:26:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:26:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:26:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:26:46 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:26:46 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:26:46 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:26:46 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=1122, total=4486)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=1122, total=4486)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=1122, total=4486)
2025-09-11 07:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
(30, 30, 1, 30)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:26:47 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-09-11 07:26:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:48 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:49 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:50 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:51 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=4 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:52 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=5 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:53 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=6 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:54 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=7 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=8 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:55 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:56 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=9 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:57 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=10 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=11 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:58 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:26:59 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=12 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:00 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=13 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:01 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=14 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:02 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=15 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:03 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=16 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:04 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=17 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:05 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=18 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:06 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=19 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:07 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=20 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:08 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=21 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:09 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:10 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=22 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:11 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=23 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:12 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=24 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:13 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:14 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=25 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=26 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:15 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=27 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:16 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:17 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=28 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:18 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=29 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:19 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=30 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=3, local_total=120
2025-09-11 07:27:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=1, local_total=120
2025-09-11 07:27:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=2, local_total=120
2025-09-11 07:27:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=120
2025-09-11 07:27:19 (federatedscope.llm.trainer.trainer:1217) INFO: [train|final] total=480, loss_sum=335.076050, avg_loss=0.698075, seen=480, correct=245, accuracy=0.510417
2025-09-11 07:27:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:27:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:27:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:27:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-09-11 07:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:27:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:27:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:27:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:27:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-11 07:27:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-11 07:27:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-11 07:27:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-11 07:27:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-11 07:27:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '1/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 81.90829533338547, 'train_avg_loss': 0.6825691277782122, 'train_seen': 120, 'train_correct': 65, 'train_acc': 0.5416666666666666}}
2025-09-11 07:27:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '3/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 82.11826986074448, 'train_avg_loss': 0.6843189155062039, 'train_seen': 120, 'train_correct': 63, 'train_acc': 0.525}}
2025-09-11 07:27:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 84.27509045600891, 'train_avg_loss': 0.7022924204667409, 'train_seen': 120, 'train_correct': 62, 'train_acc': 0.5166666666666667}}
2025-09-11 07:27:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 480, 'train_loss': 335.0760498046875, 'train_avg_loss': 0.6980751037597657, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-09-11 07:27:22 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {'train_total': 480, 'train_loss': 335.0760498046875, 'train_avg_loss': 0.6980751037597657, 'train_seen': 480, 'train_correct': 245, 'train_acc': 0.5104166666666666}}
2025-09-11 07:27:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '2/4', 'Local': True, 'Results': {'train_total': 120, 'train_loss': 86.77438974380493, 'train_avg_loss': 0.7231199145317078, 'train_seen': 120, 'train_correct': 55, 'train_acc': 0.4583333333333333}}
2025-09-11 07:27:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:27:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:27:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:27:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=1344 skipped=0 missing=291 unexpected=0
2025-09-11 07:27:22 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:27:22 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:27:22 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:27:22 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=-1
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=3, local_count=652, total=2605)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=1, local_count=652, total=2605)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=2, local_count=652, total=2605)
2025-09-11 07:27:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140269672464384 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:27:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=3] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140448718913536 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:27:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=1] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139969762951168 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:27:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=2] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=140109886259200 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
(30, 30, 1, 30)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=30, num_train_batch_last_epoch=30, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
(30, 30, 1, 30)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
(30, 30, 1, 30)
2025-09-11 07:27:23 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=30, grad_accum_step=2, will_run_step(loops)=60
2025-09-11 07:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=1 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:24 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=2 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
2025-09-11 07:27:25 (federatedscope.llm.trainer.trainer:1055) INFO: [UPD] optimizer_step=3 (sync_gradients=True), num_train_batch_last_epoch=60
