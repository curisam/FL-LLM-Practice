# federatedscope/core/workers/client.py
# -*- coding: utf-8 -*-

import copy
import logging
import sys
import pickle

from federatedscope.core.message import Message
from federatedscope.core.communication import StandaloneCommManager, \
    StandaloneDDPCommManager, gRPCCommManager
from federatedscope.core.monitors.early_stopper import EarlyStopper
from federatedscope.core.auxiliaries.trainer_builder import get_trainer
from federatedscope.core.secret_sharing import AdditiveSecretSharing
from federatedscope.core.auxiliaries.utils import merge_dict_of_results, \
    calculate_time_cost, add_prefix_to_path, get_ds_rank
from federatedscope.core.workers.base_client import BaseClient

logger = logging.getLogger(__name__)
if get_ds_rank() == 0:
    logger.setLevel(logging.INFO)

import os
import json


class Client(BaseClient):
    """
    The Client class, which describes the behaviors of client in an FL \
    course. The behaviors are described by the handling functions (named as \
    ``callback_funcs_for_xxx``)

    Arguments:
        ID: The unique ID of the client, which is assigned by the server
        when joining the FL course
        server_id: (Default) 0
        state: The training round
        config: The configuration
        data: The data owned by the client
        model: The model maintained locally
        device: The device to run local training and evaluation

    Attributes:
        ID: ID of worker
        state: the training round index
        model: the model maintained locally
        cfg: the configuration of FL course, \
            see ``federatedscope.core.configs``
        mode: the run mode for FL, ``distributed`` or ``standalone``
        monitor: monite FL course and record metrics, \
            see ``federatedscope.core.monitors.monitor.Monitor``
        trainer: instantiated trainer, see ``federatedscope.core.trainers``
        best_results: best results ever seen
        history_results: all evaluation results
        early_stopper: determine when to early stop, \
            see ``federatedscope.core.monitors.early_stopper.EarlyStopper``
        ss_manager: secret sharing manager
        msg_buffer: dict buffer for storing message
        comm_manager: manager for communication, \
            see ``federatedscope.core.communication``

    ---------------------------------------------------------------
    ğŸ”§ ì´ ë²„ì „ì—ì„œì˜ ì£¼ìš” ì»¤ìŠ¤í„°ë§ˆì´ì§•
    - ë©”ì¸ í”„ë¡œì„¸ìŠ¤ì—ì„œë§Œ íŒŒì¼ ê¸°ë¡ / ì„œë²„ ì „ì†¡ â†’ ì¤‘ë³µ ë°©ì§€
    - eval_results.raw: test_*ì™€ val_*ê°€ ëª¨ë‘ ìˆì„ ë•Œë§Œ 1íšŒ ê¸°ë¡
    - evaluate()ëŠ” í•­ìƒ metricsë¥¼ ì •ì˜(ì´ˆê¸°í™”)í•˜ê³  ì˜ˆì™¸ ë³´í˜¸
    - train()/evaluate() ì‹œì‘ ì „ í•´ë‹¹ split ìºì‹œë¥¼ ë¦¬ì…‹(ë¼ìš´ë“œ ê°„ ëˆ„ì  ë°©ì§€)
    - ë¡œê¹…: ë¡œì»¬(per-rank), per-proc ë¦¬ìŠ¤íŠ¸(rank0), ì§‘ê³„ë³¸(rank0) 3ë‹¨ ì¶œë ¥
    ---------------------------------------------------------------
    """
    def __init__(self,
                 ID=-1,
                 server_id=None,
                 state=-1,
                 config=None,
                 data=None,  # ClientData í´ë˜ìŠ¤
                 model=None,
                 device='cpu',
                 strategy=None,
                 is_unseen_client=False,
                 *args,
                 **kwargs):
        super(Client, self).__init__(ID, state, config, model, strategy)

        self.data = data  # ClientData í´ë˜ìŠ¤

        # Register message handlers
        self._register_default_handlers()

        # Un-configured worker
        if config is None:
            return

        # unseen client ì—¬ë¶€
        self.is_unseen_client = is_unseen_client

        # ê³µê²©ì ì—¬ë¶€ íŒŒì‹±
        parsed_attack_ids = list()
        if isinstance(config.attack.attacker_id, int):
            parsed_attack_ids.append(config.attack.attacker_id)
        elif isinstance(config.attack.attacker_id, list):
            parsed_attack_ids = config.attack.attacker_id
        else:
            raise TypeError(f"The expected types of config.attack.attacker_id "
                            f"include 'int' and 'list', but we got "
                            f"{type(config.attack.attacker_id)}")

        self.is_attacker = ID in parsed_attack_ids and \
            config.attack.attack_method != '' and \
            config.federate.mode == 'standalone'

        # Build Trainer
        self.trainer = get_trainer(model=model,
                                   data=data,
                                   device=device,
                                   config=self._cfg,
                                   is_attacker=self.is_attacker,
                                   monitor=self._monitor)
        self.device = device

        # ğŸ”’ outdir ê°•ì œ: sub_exp/rank-* ì œê±°í•˜ê³  ìƒìœ„ exp í´ë” í•˜ë‚˜ë§Œ ì“°ê¸°
        self._force_single_outdir()

        # For client-side evaluation
        self.best_results = dict()
        self.history_results = dict()

        # EarlyStopper
        patience = self._cfg.early_stop.patience if \
            self._cfg.federate.method in ["local", "global"] else 0
        self.early_stopper = EarlyStopper(
            patience, self._cfg.early_stop.delta,
            self._cfg.early_stop.improve_indicator_mode,
            self._monitor.the_larger_the_better)

        # Secret Sharing Manager and message buffer
        self.ss_manager = AdditiveSecretSharing(
            shared_party_num=int(self._cfg.federate.sample_client_num
                                 )) if self._cfg.federate.use_ss else None

        self.msg_buffer = {'train': dict(), 'eval': dict()}

        # Communication and communication ability
        if 'resource_info' in kwargs and kwargs['resource_info'] is not None:
            self.comp_speed = float(kwargs['resource_info']['computation']) / 1000.  # (s/sample)
            self.comm_bandwidth = float(kwargs['resource_info']['communication'])  # (kbit/s)
        else:
            self.comp_speed = None
            self.comm_bandwidth = None

        if self._cfg.backend == 'torch':
            try:
                self.model_size = sys.getsizeof(pickle.dumps(self.model)) / 1024.0 * 8.  # kbits
            except Exception as error:
                self.model_size = 1.0
                logger.warning(f'{error} in calculate model size.')
        else:
            self.model_size = 1.0
            logger.warning(f'The calculation of model size in backend:{self._cfg.backend} is not provided.')

        # Initialize communication manager
        self.server_id = server_id

        comm_queue = kwargs.get('shared_comm_queue')
        if self._cfg.federate.process_num <= 1:
            self.comm_manager = StandaloneCommManager(
                comm_queue=comm_queue, monitor=self._monitor)
        else:
            self.comm_manager = StandaloneDDPCommManager(
                comm_queue=comm_queue, monitor=self._monitor)
        self.local_address = None

        self.logger = logger

        # === ì¤‘ë³µ ê¸°ë¡ ë°©ì§€ ë§ˆì»¤ë“¤ ===
        self._eval_written_marker = set()   # {(client_id, round)}
        self._train_written_marker = set()

        # ì´ë¯¸ ìˆëŠ” ì´ˆê¸°í™”ë“¤ ë’¤ì— ì¶”ê°€
        self._handled_rounds = {
            'model_para': set(),
            'evaluate': set(),
        }

        # --- ë¹„-ë©”ì¸(rank>0) íŒŒì¼ ê¸°ë¡ ë¹„í™œì„±í™” + outdir í†µì¼ ---
        is_main = True
        try:
            if hasattr(self, "trainer") and hasattr(self.trainer, "accelerator") and self.trainer.accelerator is not None:
                is_main = self.trainer.accelerator.is_main_process
            else:
                _rank = int(os.environ.get("RANK", os.environ.get("LOCAL_RANK", "0") or 0))
                is_main = (_rank == 0)
        except Exception:
            is_main = True

        if not is_main:
            try:
                if hasattr(self._monitor, "exp_print_output_stream") and self._monitor.exp_print_output_stream:
                    try:
                        self._monitor.exp_print_output_stream.close()
                    except Exception:
                        pass
                    self._monitor.exp_print_output_stream = None
                if hasattr(self._monitor, "to_file"):
                    self._monitor.to_file = False
            except Exception as e:
                self.logger.debug(f"[monitor-disable-nonmain] skip due to: {e}")

        # outdirì´ sub_exp/... ë¼ë©´ ìƒìœ„ë¡œ ê°•ì œ ê³ ì • (ìµœì¢… í•œ ë²ˆ ë”)
        self._force_single_outdir()

    def _force_single_outdir(self):
        """
        self._monitor.outdirê°€ sub_exp/rank-*/... ë¡œ ë‚´ë ¤ê°€ë©´ ìƒìœ„ exp í´ë”ë¡œ ëŒì–´ì˜¬ë¦½ë‹ˆë‹¤.
        ì´ë ‡ê²Œ í•˜ë©´ ê²°ê³¼ íŒŒì¼ì€ í•­ìƒ exp/<expname>/<run>/ ì—ë§Œ 1ë²Œ ìƒê¹ë‹ˆë‹¤.
        """
        try:
            outdir = getattr(self._monitor, "outdir", None)
            if not outdir:
                return
            norm = os.path.normpath(outdir)
            parts = norm.split(os.sep)
            if "sub_exp" in parts:
                idx = parts.index("sub_exp")
                fixed = os.sep.join(parts[:idx])  # sub_exp ì•ê¹Œì§€ë§Œ
                if fixed:
                    self._monitor.outdir = fixed
                    os.makedirs(self._monitor.outdir, exist_ok=True)
                    self.logger.info(f"[outdir-fixed] monitor.outdir -> {self._monitor.outdir}")
        except Exception as e:
            self.logger.debug(f"[outdir-fixed] skip due to: {e}")

    # ------------------------------------------------------------------
    # íŒŒì¼ ì•„ì›ƒë””ë ‰í† ë¦¬ ë³´ì¥ / JSONL append ìœ í‹¸ / (rankë³„) ë¡œê¹… ìœ í‹¸
    # ------------------------------------------------------------------

    def _ensure_outdir(self):
        if hasattr(self._monitor, "outdir") and self._monitor.outdir:
            # âœ… outdir ìƒì„± ì§ì „ í•œ ë²ˆ ë” ìƒìœ„ í´ë”ë¡œ ê³ ì •
            self._force_single_outdir()
            os.makedirs(self._monitor.outdir, exist_ok=True)

    def _append_raw_line(self, role_str: str, round_idx, results_dict: dict, filename: str):
        """
        outdir (self._monitor.outdir)/filename ì— JSON ë¼ì¸ í•œ ì¤„ append
        results_dict: {'train_*'...} ë˜ëŠ” {'test_*','val_*'...} ê°™ì€ ì§‘ê³„ í‚¤ë§Œ ë„˜ê¸°ì„¸ìš”.
        """
        if not self._is_main_process():
            return

        try:
            outdir = getattr(self._monitor, "outdir", None)
            if not outdir:
                outdir = os.path.join("exp", "default")
            os.makedirs(outdir, exist_ok=True)
            outpath = os.path.join(outdir, filename)

            line = {
                "Role": role_str,
                "Round": round_idx,
                "Results_raw": results_dict
            }
            with open(outpath, "a", encoding="utf-8") as f:
                f.write(json.dumps(line, ensure_ascii=False) + "\n")
        except Exception as e:
            # íŒŒì¼ ë¬¸ì œë¡œ í•™ìŠµì´ ì£½ì§€ ì•Šë„ë¡ ë°©ì–´
            self.logger.warning(f"[append_raw_line] failed to write {filename}: {e}")

    def _log_split_metrics(self, role_str, round_idx, split, trainer_ctx):
        """
        split: 'train' | 'val' | 'test'
        trainer_ctx: self.trainer.ctx

        â‘  ê° rank ë¡œì»¬ ìŠ¤ëƒ…ìƒ·
        â‘¡ rank0ì—ì„œ per-proc ë¦¬ìŠ¤íŠ¸ (ëª¨ë“  rankì˜ local ìŠ¤ëƒ…ìƒ· ëª¨ìŒ)
        â‘¢ rank0ì—ì„œ í•´ë‹¹ splitì˜ ì§‘ê³„ ê²°ê³¼(ì ‘ë‘ì‚¬ 'split_')
        """
        # accelerator ì°¾ê¸° (client, trainer.ctx, trainer ìˆœìœ¼ë¡œ ì‹œë„)
        accel = getattr(self, "accelerator", None)
        if accel is None:
            accel = getattr(trainer_ctx, "accelerator", None)
        if accel is None and hasattr(self, "trainer") and hasattr(self.trainer, "accelerator"):
            accel = self.trainer.accelerator

        # ë­í¬/ì›”ë“œ ì¶”ì • (accelerator ì—†ì„ ë•Œ envë¡œ ë³´ì™„)
        if accel is not None:
            world = getattr(accel, "num_processes", 1)
            rank = getattr(accel, "process_index", 0)
        else:
            world = int(os.getenv("WORLD_SIZE", "1"))
            rank = int(os.getenv("LOCAL_RANK", "0"))

        # â‘  ê° rank ë¡œì»¬ ìŠ¤ëƒ…ìƒ·
        local = getattr(trainer_ctx, "local_results_for_log", {}) or {}
        self.logger.info({
            'Role': role_str, 'Round': round_idx, 'Split': split,
            'Rank': f'{rank}/{world}', 'Local': True,
            'Results': local
        })

        # â‘¡ rank0: per-proc ë¦¬ìŠ¤íŠ¸
        if world > 1 and rank == 0:
            per_procs = getattr(trainer_ctx, "per_proc_local_results", None)
            if per_procs:
                self.logger.info({
                    'Role': role_str, 'Round': round_idx, 'Split': split,
                    'Local': False, 'Results_procs': per_procs
                })

        # â‘¢ ì§‘ê³„ ê²°ê³¼(í•´ë‹¹ split ì ‘ë‘ì‚¬ 4í‚¤ë§Œ; alias ê¸ˆì§€)
        agg = getattr(trainer_ctx, "eval_metrics", {}) or {}
        sp = f"{split}_"
        agg_clean = {k: v for k, v in agg.items() if k.startswith(sp)}

        # ì§‘ê³„ ê²°ê³¼ëŠ” ì˜¤ì§ rank0ì—ì„œë§Œ ì¶œë ¥
        if agg_clean and rank == 0:
            self.logger.info({
                'Role': role_str, 'Round': round_idx, 'Split': split,
                'Aggregated': True, 'Results_raw': agg_clean
            })

    def _is_main_process(self) -> bool:
        # accelerateê°€ ìˆìœ¼ë©´ ê·¸ê±¸ë¡œ, ì—†ìœ¼ë©´ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ ê°€ì •
        trainer = getattr(self, 'trainer', None)
        if trainer is not None and hasattr(trainer, 'accelerator') and trainer.accelerator is not None:
            return trainer.accelerator.is_main_process
        return True  # accelerator ë¯¸ì‚¬ìš© ì‹œ ë‹¨ì¼ í”„ë¡œì„¸ìŠ¤ì´ë¯€ë¡œ True

    def _gen_timestamp(self, init_timestamp, instance_number):
        if init_timestamp is None:
            return None

        comp_cost, comm_cost = calculate_time_cost(
            instance_number=instance_number,
            comm_size=self.model_size,
            comp_speed=self.comp_speed,
            comm_bandwidth=self.comm_bandwidth)
        return init_timestamp + comp_cost + comm_cost

    def _calculate_model_delta(self, init_model, updated_model):
        if not isinstance(init_model, list):
            init_model = [init_model]
            updated_model = [updated_model]

        model_deltas = list()
        for model_index in range(len(init_model)):
            model_delta = copy.deepcopy(init_model[model_index])
            for key in init_model[model_index].keys():
                model_delta[key] = updated_model[model_index][key] - init_model[model_index][key]
            model_deltas.append(model_delta)

        if len(model_deltas) > 1:
            return model_deltas
        else:
            return model_deltas[0]

    def _reset_ctx_split_metrics(self, split: str):
        """
        âœ… (ì¤‘ìš”) ë¼ìš´ë“œ ì‹œì‘ ì „ì— í•´ë‹¹ splitì˜ ìºì‹œë¥¼ ë¦¬ì…‹
        - trainer.ctx.eval_metrics, trainer.ctx.local_results_for_log,
          trainer.ctx.per_proc_local_results ë“±ì—ì„œ split ì ‘ë‘ì‚¬ì˜ í‚¤ë¥¼ ì œê±°.
        - DDP ìƒí™©ì—ì„œ ë¼ìš´ë“œ ê°„ ëˆ„ì ë˜ì–´ train_total ì´ ë¶ˆì–´ë‚˜ëŠ” í˜„ìƒ ë°©ì§€.
        """
        try:
            ctx = getattr(self.trainer, "ctx", None)
            if ctx is None:
                return

            # 1) ì§‘ê³„ë³¸ì—ì„œ split ì ‘ë‘ì‚¬ í‚¤ ì‚­ì œ
            em = getattr(ctx, "eval_metrics", None)
            if isinstance(em, dict):
                for k in list(em.keys()):
                    if k.startswith(f"{split}_"):
                        del em[k]

            # 2) ë¡œì»¬ ìŠ¤ëƒ…ìƒ·ì—ì„œ split ì ‘ë‘ì‚¬ í‚¤ ì‚­ì œ
            lr = getattr(ctx, "local_results_for_log", None)
            if isinstance(lr, dict):
                for k in list(lr.keys()):
                    if k.startswith(f"{split}_"):
                        del lr[k]

            # 3) per-proc ìºì‹œ ì •ë¦¬
            if hasattr(ctx, "per_proc_local_results"):
                pplr = getattr(ctx, "per_proc_local_results")
                if isinstance(pplr, dict):
                    for k in list(pplr.keys()):
                        if k.startswith(f"{split}_") or k == "rank":
                            del pplr[k]
                else:
                    ctx.per_proc_local_results = None

            # 4) ì„ì‹œ í…ì„œ ìºì‹œ ì œê±°
            if hasattr(ctx, "ddp_tmp_tensors"):
                ctx.ddp_tmp_tensors = {}

        except Exception as e:
            self.logger.debug(f"[reset split={split}] skip due to: {e}")

    # ------------------------------------------------------------------
    # ëŸ¬ë‹ ë£¨í”„
    # ------------------------------------------------------------------

    def join_in(self):
        """To send ``join_in`` message to the server for joining in the FL course."""
        self.comm_manager.send(
            Message(msg_type='join_in',
                    sender=self.ID,
                    receiver=[self.server_id],
                    timestamp=0,
                    content=self.local_address))

    def run(self):
        """To listen to the message and handle them accordingly (used for distributed mode)"""
        while True:
            msg = self.comm_manager.receive()
            if self.state <= msg.state:
                self.msg_handlers[msg.msg_type](msg)

            if msg.msg_type == 'finish':
                break

    def run_standalone(self):
        """Run in standalone mode"""
        self.join_in()
        self.run()

    # ------------------------------------------------------------------
    # ì„œë²„ë¡œë¶€í„° ë°›ì€ ê¸€ë¡œë²Œ ëª¨ë¸ íŒŒë¼ë¯¸í„°(í˜¹ì€ ì‹œí¬ë¦¿ ì…°ì–´ ì¡°ê°)ë¥¼ ì²˜ë¦¬í•˜ê³ ,
    # ë¡œì»¬ í•™ìŠµì„ íŠ¸ë¦¬ê±°í•œ ë’¤ ê²°ê³¼ë¥¼ ì„œë²„ì— ë‹¤ì‹œ ë³´ëƒ…ë‹ˆë‹¤.
    # ------------------------------------------------------------------
    def callback_funcs_for_model_para(self, message: Message):
        """
        The handling function for receiving model parameters, which triggers the local training process.
        """
        if 'ss' in message.msg_type:
            # A fragment of the shared secret
            state, content, timestamp = message.state, message.content, message.timestamp
            self.msg_buffer['train'][state].append(content)

            if len(self.msg_buffer['train'][state]) == self._cfg.federate.client_num:
                # Check whether the received fragments are enough
                model_list = self.msg_buffer['train'][state]
                sample_size, first_aggregate_model_para = model_list[0]
                single_model_case = True
                if isinstance(first_aggregate_model_para, list):
                    assert isinstance(first_aggregate_model_para[0], dict), \
                        "aggregate_model_para should a list of multiple state_dict for multiple models"
                    single_model_case = False
                else:
                    assert isinstance(first_aggregate_model_para, dict), \
                        "aggregate_model_para should a state_dict for single model case"
                    first_aggregate_model_para = [first_aggregate_model_para]
                    model_list = [[model] for model in model_list]

                for sub_model_idx, aggregate_single_model_para in enumerate(first_aggregate_model_para):
                    for key in aggregate_single_model_para:
                        for i in range(1, len(model_list)):
                            aggregate_single_model_para[key] += model_list[i][sub_model_idx][key]

                self.comm_manager.send(
                    Message(msg_type='model_para',
                            sender=self.ID,
                            receiver=[self.server_id],
                            state=self.state,
                            timestamp=timestamp,
                            content=(sample_size, first_aggregate_model_para[0]
                                     if single_model_case else first_aggregate_model_para)))
            return

        # ---- ì—¬ê¸°ë¶€í„° ì¼ë°˜(ë¹„-SS) ì¼€ì´ìŠ¤ ----
        round = message.state
        sender = message.sender
        timestamp = message.timestamp
        content = message.content

        # dequantization
        if self._cfg.quantization.method == 'uniform':
            from federatedscope.core.compression import symmetric_uniform_dequantization
            if isinstance(content, list):  # multiple model
                content = [symmetric_uniform_dequantization(x) for x in content]
            else:
                content = symmetric_uniform_dequantization(content)

        if self._cfg.federate.process_num > 1:
            for k, v in content.items():
                content[k] = v.to(self.device)

        self.trainer.update(content, strict=self._cfg.federate.share_local_model)

        self.state = round
        skip_train_isolated_or_global_mode = \
            self.early_stopper.early_stopped and self._cfg.federate.method in ["local", "global"]

        if self.is_unseen_client or skip_train_isolated_or_global_mode:
            sample_size, model_para_all, results = 0, self.trainer.get_model_para(), {}
            if skip_train_isolated_or_global_mode:
                logger.info(
                    f"[Local/Global mode] Client #{self.ID} has been early stopped, "
                    f"we will skip the local training")
                self._monitor.local_converged()
        else:
            if self.early_stopper.early_stopped and self._monitor.local_convergence_round == 0:
                logger.info(
                    f"[Normal FL Mode] Client #{self.ID} has been locally early stopped. "
                    f"The next FL update may result in negative effect")
                self._monitor.local_converged()

            # âœ… (ì¤‘ìš”) ì´ë²ˆ ë¼ìš´ë“œ train ì „ì— split ìºì‹œ ë¦¬ì…‹ â€” ëˆ„ì  ë°©ì§€
            self._reset_ctx_split_metrics('train')

            sample_size, model_para_all, results = self.trainer.train()

            if self._cfg.federate.share_local_model and not self._cfg.federate.online_aggr:
                model_para_all = copy.deepcopy(model_para_all)

            rank, world = 0, 1
            if hasattr(self.trainer, 'accelerator') and self.trainer.accelerator is not None:
                rank = self.trainer.accelerator.process_index
                world = self.trainer.accelerator.num_processes

            # âœ… ì›í•˜ëŠ” í¬ë§·ì˜ ë¡œê·¸ 3ì¢…(ë¡œì»¬ / per-proc / ì§‘ê³„) ì¶œë ¥
            self._log_split_metrics(
                role_str=f'Client #{self.ID}',
                round_idx=self.state,
                split='train',
                trainer_ctx=self.trainer.ctx
            )

            # âœ… rank0ì—ì„œë§Œ ì§‘ê³„ë³¸ì„ íŒŒì¼ë¡œ ê¸°ë¡
            if self._is_main_process():
                ctx = getattr(self.trainer, "ctx", None)
                agg = getattr(ctx, "eval_metrics", {}) if ctx is not None else {}
                train_agg = {k: v for k, v in (agg or {}).items() if k.startswith("train_")}
                if train_agg:
                    self._append_raw_line(
                        role_str=f"Client #{self.ID}",
                        round_idx=self.state,
                        results_dict=train_agg,
                        filename="train_results.raw"
                    )

            # âœ… exp_printìš©: train ì§‘ê³„ë³¸ë§Œ í•œ ì¤„ (rank0ì—ì„œë§Œ)
            if self._is_main_process():
                train_agg = {
                    k: v for k, v in getattr(self.trainer.ctx, "eval_metrics", {}).items()
                    if k.startswith("train_")
                }
                if train_agg:
                    self.logger.info({
                        'Role': f'Client #{self.ID}',
                        'Round': self.state,
                        'Results_raw': train_agg
                    })

        # Return the feedbacks to the server after local update
        if self._cfg.federate.use_ss:
            assert not self.is_unseen_client, \
                "Un-support using secret sharing for unseen clients." \
                "i.e., you set cfg.federate.use_ss=True and " \
                "cfg.federate.unseen_clients_rate in (0, 1)"
            single_model_case = True
            if isinstance(model_para_all, list):
                assert isinstance(model_para_all[0], dict), \
                    "model_para should a list of multiple state_dict for multiple models"
                single_model_case = False
            else:
                assert isinstance(model_para_all, dict), \
                    "model_para should a state_dict for single model case"
                model_para_all = [model_para_all]
            model_para_list_all = []
            for model_para in model_para_all:
                for key in model_para:
                    model_para[key] = model_para[key] * sample_size
                model_para_list = self.ss_manager.secret_split(model_para)
                model_para_list_all.append(model_para_list)
            frame_idx = 0
            for neighbor in self.comm_manager.neighbors:
                if neighbor != self.server_id:
                    content_frame = model_para_list_all[0][frame_idx] if \
                        single_model_case else \
                        [model_para_list[frame_idx] for model_para_list in model_para_list_all]
                    self.comm_manager.send(
                        Message(msg_type='ss_model_para',
                                sender=self.ID,
                                receiver=[neighbor],
                                state=self.state,
                                timestamp=self._gen_timestamp(
                                    init_timestamp=timestamp,
                                    instance_number=sample_size),
                                content=content_frame))
                    frame_idx += 1
            content_frame = model_para_list_all[0][frame_idx] if \
                single_model_case else \
                [model_para_list[frame_idx] for model_para_list in model_para_list_all]
            self.msg_buffer['train'][self.state] = [(sample_size, content_frame)]
        else:
            if self._cfg.asyn.use or self._cfg.aggregator.robust_rule in \
                    ['krum', 'normbounding', 'median', 'trimmedmean', 'bulyan']:
                shared_model_para = self._calculate_model_delta(
                    init_model=content, updated_model=model_para_all)
            else:
                shared_model_para = model_para_all

            # quantization
            if self._cfg.quantization.method == 'uniform':
                from federatedscope.core.compression import symmetric_uniform_quantization
                nbits = self._cfg.quantization.nbits
                if isinstance(shared_model_para, list):
                    shared_model_para = [symmetric_uniform_quantization(x, nbits) for x in shared_model_para]
                else:
                    shared_model_para = symmetric_uniform_quantization(shared_model_para, nbits)

            # âœ… rank0ë§Œ ì„œë²„ë¡œ ì—…ë¡œë“œ (ì¤‘ë³µ ë°©ì§€)
            if self._is_main_process():
                self.comm_manager.send(
                    Message(msg_type='model_para',
                            sender=self.ID,
                            receiver=[sender],
                            state=self.state,
                            timestamp=self._gen_timestamp(
                                init_timestamp=timestamp,
                                instance_number=sample_size),
                            content=(sample_size, shared_model_para)))

    def callback_funcs_for_assign_id(self, message: Message):
        """
        The handling function for receiving the client_ID assigned by the server (distributed mode).
        """
        content = message.content
        self.ID = int(content)
        logger.info('Client (address {}:{}) is assigned with #{:d}.'.format(
            self.comm_manager.host, self.comm_manager.port, self.ID))

    def callback_funcs_for_join_in_info(self, message: Message):
        """
        The handling function for receiving the request of join in information.
        """
        requirements = message.content
        timestamp = message.timestamp
        join_in_info = dict()
        for requirement in requirements:
            if requirement.lower() == 'num_sample':
                if self._cfg.train.batch_or_epoch == 'batch':
                    num_sample = self._cfg.train.local_update_steps * self._cfg.dataloader.batch_size
                else:
                    num_sample = self._cfg.train.local_update_steps * len(self.trainer.data.train_data)
                join_in_info['num_sample'] = num_sample
                if self._cfg.trainer.type == 'nodefullbatch_trainer':
                    join_in_info['num_sample'] = self.trainer.data.train_data.x.shape[0]
            elif requirement.lower() == 'client_resource':
                assert self.comm_bandwidth is not None and self.comp_speed is not None, \
                    "The requirement join_in_info 'client_resource' does not exist."
                join_in_info['client_resource'] = self.model_size / self.comm_bandwidth + self.comp_speed
            else:
                raise ValueError('Fail to get the join in information with type {}'.format(requirement))
        self.comm_manager.send(
            Message(msg_type='join_in_info',
                    sender=self.ID,
                    receiver=[self.server_id],
                    state=self.state,
                    timestamp=timestamp,
                    content=join_in_info))

    def callback_funcs_for_address(self, message: Message):
        """
        The handling function for receiving other clients' IP addresses,
        which is used for constructing a complex topology
        """
        content = message.content
        for neighbor_id, address in content.items():
            if int(neighbor_id) != self.ID:
                self.comm_manager.add_neighbors(neighbor_id, address)

    # ------------------------------------------------------------------
    # ì„œë²„ê°€ â€œí‰ê°€ ìš”ì²­(evaluate)â€ ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ ë•Œ
    # ------------------------------------------------------------------
    def callback_funcs_for_evaluate(self, message: Message):
        """
        The handling function for receiving the request of evaluating
        """
        sender, timestamp = message.sender, message.timestamp
        self.state = message.state

        # í•­ìƒ ë¨¼ì € ì´ˆê¸°í™”í•´ì„œ NameError ë°©ì§€
        metrics = {}

        # 1) ì„œë²„ íŒŒë¼ë¯¸í„°ë¡œ ë™ê¸°í™”
        if message.content is not None:
            self.trainer.update(
                message.content,
                strict=self._cfg.federate.share_local_model
            )

        # 2) í‰ê°€ ì‹¤í–‰
        try:
            if self.early_stopper.early_stopped and self._cfg.federate.method in ["local", "global"]:
                if self.best_results:
                    metrics = list(self.best_results.values())[-1]
                else:
                    metrics = {}
            else:
                # âœ… (ì¤‘ìš”) ì´ë²ˆ ë¼ìš´ë“œ í‰ê°€ ì‹œì‘ ì „, ìš”ì²­ëœ split ìºì‹œë¥¼ ì„ ì œ ë¦¬ì…‹
                for sp in set(self._cfg.eval.split):
                    self._reset_ctx_split_metrics(sp)

                if self._cfg.finetune.before_eval:
                    self.trainer.finetune()

                # ì˜ˆ: ['test','val'] ë˜ëŠ” ['val'] â€¦
                for split in self._cfg.eval.split:
                    eval_metrics = self.trainer.evaluate(target_data_split_name=split)

                    # âœ… ì›í•˜ëŠ” í¬ë§·ì˜ ë¡œê·¸ 3ì¢…(ë¡œì»¬ / per-proc / ì§‘ê³„) ì¶œë ¥
                    self._log_split_metrics(
                        role_str=f'Client #{self.ID}',
                        round_idx=self.state,
                        split=split,
                        trainer_ctx=self.trainer.ctx
                    )

                    # ê²°ê³¼ ë³‘í•©
                    if eval_metrics:
                        metrics.update(**eval_metrics)

        except Exception as e:
            # í‰ê°€ ì¤‘ ë¬¸ì œê°€ ë‚˜ë„ ì„œë²„ë¡œëŠ” ë¹ˆ dictë¼ë„ ë³´ë‚´ë„ë¡
            self.logger.warning(f"[evaluate] exception during evaluation: {e}", exc_info=True)

        # 3) rank0ë§Œ íŒŒì¼ ê¸°ë¡/ì„œë²„ ì „ì†¡
        is_main = self._is_main_process()
        if is_main:
            self._ensure_outdir()

            # ìš°ì„ ìˆœìœ„: trainer.ctx.eval_metrics(ì§‘ê³„ë³¸) -> metrics(ë³‘í•©ë³¸)
            ctx = getattr(self.trainer, "ctx", None)
            agg_all = getattr(ctx, "eval_metrics", {}) if ctx is not None else {}
            base = agg_all if agg_all else metrics

            # test_/val_ë§Œ ì¶”ì¶œ
            combined = {k: v for k, v in (base or {}).items()
                        if k.startswith("test_") or k.startswith("val_")}
            has_test = any(k.startswith("test_") for k in combined)
            has_val = any(k.startswith("val_") for k in combined)

            write_key = (self.ID, int(self.state))

            # âœ… íŒŒì¼ ê¸°ë¡: test & val ë‘˜ ë‹¤ ìˆê³ , ì•„ì§ ì•ˆ ì“´ ê²½ìš° 1íšŒë§Œ
            if has_test and has_val:
                if write_key not in self._eval_written_marker:
                    self._append_raw_line(
                        role_str=f"Client #{self.ID}",
                        round_idx=self.state,
                        results_dict=combined,
                        filename="eval_results.raw"
                    )
                    self._eval_written_marker.add(write_key)
                else:
                    self.logger.debug(f"[skip duplicate eval write] {write_key}")
            else:
                self.logger.debug(
                    f"[skip write eval_results.raw] only one split present. "
                    f"available={list(combined.keys())}"
                )

            # 3-2) í„°ë¯¸ë„ ìš”ì•½ ë¡œê·¸(ë³´ê¸° ì¢‹ê²Œ) - MonitorëŠ” ì´ì œ raw íŒŒì¼ì„ ì“°ì§€ ì•ŠìŒ
            try:
                self._monitor.format_eval_res(
                    metrics, rnd=self.state, role=f'Client #{self.ID}', forms=['log']
                )
            except Exception as e:
                self.logger.debug(f"[format_eval_res] skip log due to: {e}")

            # 3-3) best/early-stop ê°±ì‹  (í‚¤ ìë™ ì„ íƒì€ client ì¸¡ì—ì„œ ìˆ˜í–‰)
            if combined:
                want_key = self._cfg.eval.best_res_update_round_wise_key  # ì˜ˆ: 'test_loss'
                use_key = want_key if want_key in combined else None
                if use_key is None:
                    # ì„ í˜¸ ìˆœì„œ: val_loss â†’ val_avg_loss â†’ val_acc â†’ test_loss â†’ test_avg_loss â†’ test_acc
                    fallback_order = [
                        'val_loss', 'val_avg_loss', 'val_acc',
                        'test_loss', 'test_avg_loss', 'test_acc'
                    ]
                    use_key = next((k for k in fallback_order if k in combined), None)

                if use_key is None:
                    self.logger.warning(
                        f"[best-update] No suitable key in eval results. "
                        f"wanted='{want_key}', available={list(combined.keys())}"
                    )
                else:
                    # monitorëŠ” cfgì— ì§€ì •ëœ í‚¤ ì´ë¦„(want_key)ë§Œ ì°¾ì„ ìˆ˜ ìˆì–´ alias ì²˜ë¦¬
                    eval_for_best = dict(combined)
                    if use_key != want_key:
                        eval_for_best[want_key] = eval_for_best[use_key]
                        self.logger.warning(
                            f"[best-update] '{want_key}' not found; fallback to '{use_key}'."
                        )

                    try:
                        updated = self._monitor.update_best_result(
                            self.best_results, eval_for_best, results_type=f"client #{self.ID}"
                        )
                    except Exception as e:
                        self.logger.warning(f"[best-update] failed: {e}", exc_info=True)
                        updated = False

                    if updated and self._cfg.federate.save_client_model:
                        path = add_prefix_to_path(f'client_{self.ID}_', self._cfg.federate.save_to)
                        try:
                            self.trainer.save_model(path, self.state)
                        except Exception as e:
                            self.logger.warning(f"[save_model] failed: {e}", exc_info=True)

                    # íˆìŠ¤í† ë¦¬ ëˆ„ì  ë° early-stop íŠ¸ë˜í‚¹
                    try:
                        self.history_results = merge_dict_of_results(self.history_results, eval_for_best)
                        track_key = want_key if want_key in self.history_results else use_key
                        if track_key and track_key in self.history_results:
                            self.early_stopper.track_and_check(self.history_results[track_key])
                    except Exception as e:
                        self.logger.debug(f"[early-stopper] skip tracking due to: {e}")

        # 4) ì„œë²„ë¡œëŠ” rank0ë§Œ ì „ì†¡ (ì¤‘ë³µ ë°©ì§€)
        if is_main:
            self.comm_manager.send(
                Message(msg_type='metrics',
                        sender=self.ID,
                        receiver=[sender],
                        state=self.state,
                        timestamp=timestamp,
                        content=metrics)
            )

    def callback_funcs_for_finish(self, message: Message):
        """
        The handling function for receiving the signal of finishing the FL course.
        """
        logger.info(
            f"================= client {self.ID} received finish message =================")

        if message.content is not None:
            self.trainer.update(message.content,
                                strict=self._cfg.federate.share_local_model)

        # âœ… rank0ë§Œ ì¢…ë£Œ íŒŒì¼ ê¸°ë¡(ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë“±)
        if self._is_main_process():
            self._monitor.finish_fl()

    def callback_funcs_for_converged(self, message: Message):
        """
        The handling function for receiving the signal that the FL course converged
        """
        self._monitor.global_converged()

    @classmethod
    def get_msg_handler_dict(cls):
        return cls().msg_handlers_str
