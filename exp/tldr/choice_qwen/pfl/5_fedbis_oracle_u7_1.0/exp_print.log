2025-10-15 15:42:12 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/5_fedbis_oracle_u7_1.0/exp_print.log
2025-10-15 15:42:12 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/5_fedbis_oracle_u7_1.0/
2025-10-15 15:42:35 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-10-15 15:43:17 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-10-15 15:43:19 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-10-15 15:43:19 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-10-15 15:43:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140692151918656 | out_emb=(Linear) num=151646 ptr=140692151918656 | lora_ptr=None
2025-10-15 15:43:33 (federatedscope.llm.model.model_builder:188) INFO: [Warmup-Init] loaded from checkpoints_1.0_oracle/final_tldr_choice_qwen_fedbis_oracle_u7_round_211.ckpt (round=211) | missing=291 unexpected=0
2025-10-15 15:43:33 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-10-15 15:43:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:37 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-10-15 15:43:38 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:40 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-10-15 15:43:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:43 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-10-15 15:43:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:45 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-10-15 15:43:46 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:48 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-10-15 15:43:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:51 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-10-15 15:43:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:53 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-10-15 15:43:54 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:56 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-10-15 15:43:56 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:43:59 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-10-15 15:43:59 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:03 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-10-15 15:44:03 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:06 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-10-15 15:44:06 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:09 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-10-15 15:44:09 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:11 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-10-15 15:44:12 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:14 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-10-15 15:44:15 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:17 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-10-15 15:44:17 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:20 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-10-15 15:44:20 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:23 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-10-15 15:44:23 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:25 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-10-15 15:44:26 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:28 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-10-15 15:44:28 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:31 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-10-15 15:44:32 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:34 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-10-15 15:44:34 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:37 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-10-15 15:44:37 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:40 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-10-15 15:44:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:43 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-10-15 15:44:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:46 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-10-15 15:44:46 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:49 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-10-15 15:44:49 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:52 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-10-15 15:44:52 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:55 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-10-15 15:44:55 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:44:57 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-10-15 15:44:57 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:01 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-10-15 15:45:01 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:04 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-10-15 15:45:04 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:07 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-10-15 15:45:07 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:10 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-10-15 15:45:10 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:13 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-10-15 15:45:13 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:16 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-10-15 15:45:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:19 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-10-15 15:45:19 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:21 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-10-15 15:45:22 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:24 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-10-15 15:45:24 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:27 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-10-15 15:45:27 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:31 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-10-15 15:45:31 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:34 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-10-15 15:45:34 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:37 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-10-15 15:45:37 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:40 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-10-15 15:45:40 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:42 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-10-15 15:45:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:45 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-10-15 15:45:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:48 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-10-15 15:45:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:51 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-10-15 15:45:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:54 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-10-15 15:45:54 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:45:56 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-10-15 15:45:57 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:46:00 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-10-15 15:46:00 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:46:03 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-10-15 15:46:03 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:46:06 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-10-15 15:46:06 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:46:08 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-10-15 15:46:08 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-10-15 15:46:09 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 2688.
2025-10-15 15:46:09 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 2978.
2025-10-15 15:46:09 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 2688. 
Preserved para names in local update: {'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight'}.
2025-10-15 15:46:09 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-10-15 15:46:09 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-10-15 15:46:09 (federatedscope.llm.llm_local.server:200) INFO: Waited all clients join, start now...
2025-10-15 15:46:09 (federatedscope.llm.llm_local.server:217) INFO: ----------- Starting training (Round #0) -------------
2025-10-15 15:46:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:46:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:46:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:18 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:46:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:46:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:22 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:46:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:46:26 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=143.686371, avg_loss=0.718432, seen=200, correct=101, accuracy=0.505000
2025-10-15 15:46:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:46:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:46:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1818MB allocated=1793MB
2025-10-15 15:46:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:46:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:46:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.381557, avg_loss=0.684539, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:46:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:46:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1818MB allocated=1793MB
2025-10-15 15:46:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:46:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:46:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-15 15:46:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:46:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:46:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:46:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:46:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.331658, avg_loss=0.708229, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:46:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:46:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:46:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1818MB
2025-10-15 15:46:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 1.969911813735962, 'train_avg_loss': 0.4924779534339905, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:46:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.331658363342285, 'train_avg_loss': 0.7082286477088928, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:46:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.331658363342285, 'train_avg_loss': 0.7082286477088928, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:46:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:46:37 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:46:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:38 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:46:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:46:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:46:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-15 15:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-15 15:46:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=56.260082, avg_loss=0.760271, seen=74, correct=38, accuracy=0.513514
2025-10-15 15:46:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:46:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:46:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1810MB
2025-10-15 15:46:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:46:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:46:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.476868, avg_loss=0.686922, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:46:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:46:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:46:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1810MB
2025-10-15 15:46:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:46:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:46:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-15 15:46:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:46:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:46:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:46:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:46:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.466320, avg_loss=0.716645, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:46:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:46:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:46:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1826MB
2025-10-15 15:46:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.322362542152405, 'train_avg_loss': 0.5805906355381012, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:46:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.466320037841797, 'train_avg_loss': 0.7166450023651123, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:46:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.466320037841797, 'train_avg_loss': 0.7166450023651123, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:46:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:46:51 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:46:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:51 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:46:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:46:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-15 15:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-15 15:46:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=59.097588, avg_loss=0.712019, seen=83, correct=46, accuracy=0.554217
2025-10-15 15:46:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:46:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:46:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1818MB
2025-10-15 15:46:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:46:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:46:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.154636, avg_loss=0.728866, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:46:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:46:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:46:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1818MB
2025-10-15 15:46:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:46:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:46:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-15 15:46:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:46:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:46:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:46:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:46:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:47:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:47:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.078750, avg_loss=0.692422, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:47:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1886MB allocated=1835MB
2025-10-15 15:47:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.3972588777542114, 'train_avg_loss': 0.5993147194385529, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:47:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.078749656677246, 'train_avg_loss': 0.6924218535423279, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:47:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.078749656677246, 'train_avg_loss': 0.6924218535423279, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:47:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:47:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:47:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:04 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:47:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:47:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:47:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:47:09 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=131.431686, avg_loss=0.657158, seen=200, correct=109, accuracy=0.545000
2025-10-15 15:47:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1826MB
2025-10-15 15:47:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:47:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.698103, avg_loss=0.642453, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:47:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1826MB
2025-10-15 15:47:14 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:47:14 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:47:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-15 15:47:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:47:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:47:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:47:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:47:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.476011, avg_loss=0.654751, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:47:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1843MB
2025-10-15 15:47:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.2543288469314575, 'train_avg_loss': 0.8135822117328644, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:47:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.476011276245117, 'train_avg_loss': 0.6547507047653198, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:47:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.476011276245117, 'train_avg_loss': 0.6547507047653198, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:47:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:47:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:47:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:19 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:47:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:47:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:47:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-15 15:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-15 15:47:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=88.662582, avg_loss=0.647172, seen=137, correct=83, accuracy=0.605839
2025-10-15 15:47:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1835MB
2025-10-15 15:47:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:47:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.066750, avg_loss=0.676669, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:47:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1835MB
2025-10-15 15:47:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:47:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:47:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-15 15:47:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:47:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:47:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:47:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:47:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.920525, avg_loss=0.620033, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:47:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1912MB allocated=1851MB
2025-10-15 15:47:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.264834761619568, 'train_avg_loss': 0.566208690404892, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:47:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.920524597167969, 'train_avg_loss': 0.620032787322998, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:47:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.920524597167969, 'train_avg_loss': 0.620032787322998, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:47:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:47:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:47:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:30 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:47:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:47:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:47:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-15 15:47:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-15 15:47:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=27.146263, avg_loss=0.754063, seen=36, correct=17, accuracy=0.472222
2025-10-15 15:47:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1843MB
2025-10-15 15:47:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:47:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:47:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.052109, avg_loss=0.701303, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:47:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1843MB
2025-10-15 15:47:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:47:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:47:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-15 15:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:47:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:47:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:47:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:47:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.370089, avg_loss=0.773131, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:47:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1904MB allocated=1860MB
2025-10-15 15:47:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.2568269968032837, 'train_avg_loss': 0.8142067492008209, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:47:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.370088577270508, 'train_avg_loss': 0.7731305360794067, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:47:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.370088577270508, 'train_avg_loss': 0.7731305360794067, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:47:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:47:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:47:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:44 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:47:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:47:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-15 15:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-15 15:47:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=69.827644, avg_loss=0.623461, seen=112, correct=76, accuracy=0.678571
2025-10-15 15:47:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1851MB
2025-10-15 15:47:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:47:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:47:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.063379, avg_loss=0.701584, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:47:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1851MB
2025-10-15 15:47:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:47:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:47:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-15 15:47:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:47:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:47:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:47:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:47:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:47:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.534474, avg_loss=0.720905, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:47:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:47:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:47:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1868MB
2025-10-15 15:47:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.0951101779937744, 'train_avg_loss': 0.7737775444984436, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:47:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.53447437286377, 'train_avg_loss': 0.7209046483039856, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:47:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.53447437286377, 'train_avg_loss': 0.7209046483039856, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:47:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:48:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:48:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:00 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:48:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:48:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:48:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:48:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.454559, avg_loss=0.672273, seen=200, correct=110, accuracy=0.550000
2025-10-15 15:48:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1860MB
2025-10-15 15:48:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:48:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.780828, avg_loss=0.669521, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:48:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1860MB
2025-10-15 15:48:09 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:48:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:48:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-15 15:48:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:48:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:48:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:48:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:48:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.449709, avg_loss=0.590607, seen=16, correct=12, accuracy=0.750000
2025-10-15 15:48:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1877MB
2025-10-15 15:48:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.0740777254104614, 'train_avg_loss': 0.5185194313526154, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:48:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.449708938598633, 'train_avg_loss': 0.5906068086624146, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:48:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.449708938598633, 'train_avg_loss': 0.5906068086624146, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:48:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:48:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:48:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:14 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:48:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:48:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-15 15:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-15 15:48:19 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=116.447647, avg_loss=0.684986, seen=170, correct=94, accuracy=0.552941
2025-10-15 15:48:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1868MB
2025-10-15 15:48:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:48:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.224745, avg_loss=0.705619, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:48:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1868MB
2025-10-15 15:48:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:48:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:48:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-15 15:48:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:48:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:48:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:48:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:48:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=8.508093, avg_loss=0.531756, seen=16, correct=13, accuracy=0.812500
2025-10-15 15:48:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1924MB allocated=1885MB
2025-10-15 15:48:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 1.9099862575531006, 'train_avg_loss': 0.47749656438827515, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:48:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 8.508092880249023, 'train_avg_loss': 0.531755805015564, 'train_seen': 16, 'train_correct': 13, 'train_acc': 0.8125}}
2025-10-15 15:48:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 8.508092880249023, 'train_avg_loss': 0.531755805015564, 'train_seen': 16, 'train_correct': 13, 'train_acc': 0.8125}}
2025-10-15 15:48:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:48:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:48:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:31 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:48:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:48:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-15 15:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-15 15:48:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=84.255760, avg_loss=0.685006, seen=123, correct=62, accuracy=0.504065
2025-10-15 15:48:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1877MB
2025-10-15 15:48:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:48:38 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.015656, avg_loss=0.675391, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:48:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1877MB
2025-10-15 15:48:40 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:48:40 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:48:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-15 15:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:48:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:48:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:48:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:48:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.881290, avg_loss=0.680081, seen=16, correct=12, accuracy=0.750000
2025-10-15 15:48:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1932MB allocated=1893MB
2025-10-15 15:48:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.153785228729248, 'train_avg_loss': 0.538446307182312, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:48:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.881290435791016, 'train_avg_loss': 0.6800806522369385, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:48:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.881290435791016, 'train_avg_loss': 0.6800806522369385, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:48:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:48:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:48:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:45 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:48:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:48:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-15 15:48:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:48:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.515097, avg_loss=0.679650, seen=14, correct=8, accuracy=0.571429
2025-10-15 15:48:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-15 15:48:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:48:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.913353, avg_loss=0.697834, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:48:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-15 15:48:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:48:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:48:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-15 15:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:48:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:48:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:48:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:48:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.616072, avg_loss=0.726004, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:48:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:48:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:48:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1902MB
2025-10-15 15:48:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.8946077823638916, 'train_avg_loss': 0.7236519455909729, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:48:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.616071701049805, 'train_avg_loss': 0.7260044813156128, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:48:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.616071701049805, 'train_avg_loss': 0.7260044813156128, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:48:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:48:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:48:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:48:58 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:48:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:48:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:49:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-15 15:49:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-15 15:49:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=19.999296, avg_loss=0.624978, seen=32, correct=20, accuracy=0.625000
2025-10-15 15:49:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1893MB
2025-10-15 15:49:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:49:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:49:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.222790, avg_loss=0.730570, seen=40, correct=16, accuracy=0.400000
2025-10-15 15:49:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1893MB
2025-10-15 15:49:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.400000
2025-10-15 15:49:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:49:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-15 15:49:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:49:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:49:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:49:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.430297, avg_loss=0.589394, seen=16, correct=12, accuracy=0.750000
2025-10-15 15:49:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1958MB allocated=1910MB
2025-10-15 15:49:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.9184467792510986, 'train_avg_loss': 0.7296116948127747, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:49:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.430296897888184, 'train_avg_loss': 0.5893935561180115, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:49:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.430296897888184, 'train_avg_loss': 0.5893935561180115, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:49:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:49:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:49:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:10 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:49:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:49:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-15 15:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-15 15:49:13 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=53.690170, avg_loss=0.715869, seen=75, correct=36, accuracy=0.480000
2025-10-15 15:49:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-15 15:49:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:49:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:49:17 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.677567, avg_loss=0.691939, seen=40, correct=19, accuracy=0.475000
2025-10-15 15:49:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:18 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-15 15:49:18 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-15 15:49:18 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:49:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-15 15:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:19 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:49:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:19 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:49:19 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:49:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:49:20 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.403273, avg_loss=0.650205, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:49:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1962MB allocated=1919MB
2025-10-15 15:49:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.187387228012085, 'train_avg_loss': 0.7968468070030212, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:49:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.40327262878418, 'train_avg_loss': 0.6502045392990112, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:49:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.40327262878418, 'train_avg_loss': 0.6502045392990112, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:49:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:49:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:49:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:23 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:49:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:49:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:25 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:49:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:49:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:49:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.287613, avg_loss=0.696438, seen=200, correct=104, accuracy=0.520000
2025-10-15 15:49:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1910MB
2025-10-15 15:49:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:49:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.888123, avg_loss=0.697203, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:49:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1910MB
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-15 15:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:49:33 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:49:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:49:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.523524, avg_loss=0.782720, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:49:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1972MB allocated=1927MB
2025-10-15 15:49:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.881525993347168, 'train_avg_loss': 0.720381498336792, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:49:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.523524284362793, 'train_avg_loss': 0.7827202677726746, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:49:37 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.523524284362793, 'train_avg_loss': 0.7827202677726746, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:49:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:49:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:49:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:39 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:49:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:49:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:49:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-15 15:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-15 15:49:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=140.219879, avg_loss=0.726528, seen=193, correct=96, accuracy=0.497409
2025-10-15 15:49:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1919MB
2025-10-15 15:49:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:49:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.424747, avg_loss=0.635619, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:49:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1919MB
2025-10-15 15:49:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:49:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:49:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-15 15:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:49:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:49:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:49:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:49:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.558203, avg_loss=0.659888, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:49:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:49:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1992MB allocated=1935MB
2025-10-15 15:49:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.3080612421035767, 'train_avg_loss': 0.5770153105258942, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:49:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.558202743530273, 'train_avg_loss': 0.6598876714706421, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:49:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.558202743530273, 'train_avg_loss': 0.6598876714706421, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:49:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:49:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:49:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:54 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:49:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:49:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:49:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:49:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:49:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:49:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:49:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=134.700012, avg_loss=0.673500, seen=200, correct=110, accuracy=0.550000
2025-10-15 15:49:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1927MB
2025-10-15 15:50:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:50:02 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.698370, avg_loss=0.667459, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:50:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1927MB
2025-10-15 15:50:04 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:50:04 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:50:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-15 15:50:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:50:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:50:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:50:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:50:06 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.645500, avg_loss=0.602844, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:50:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1990MB allocated=1944MB
2025-10-15 15:50:08 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.148783504962921, 'train_avg_loss': 0.5371958762407303, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:50:08 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.645500183105469, 'train_avg_loss': 0.6028437614440918, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:50:08 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.645500183105469, 'train_avg_loss': 0.6028437614440918, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:50:08 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:50:09 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:50:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:09 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:50:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:11 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:50:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-15 15:50:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-15 15:50:12 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.142523, avg_loss=0.671417, seen=30, correct=16, accuracy=0.533333
2025-10-15 15:50:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1935MB
2025-10-15 15:50:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:50:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.855610, avg_loss=0.671390, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:50:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1935MB
2025-10-15 15:50:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:50:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:50:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-15 15:50:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:50:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:50:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:50:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:50:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=13.050384, avg_loss=0.815649, seen=16, correct=5, accuracy=0.312500
2025-10-15 15:50:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1952MB
2025-10-15 15:50:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 4.188788533210754, 'train_avg_loss': 1.0471971333026886, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:50:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 13.050383567810059, 'train_avg_loss': 0.8156489729881287, 'train_seen': 16, 'train_correct': 5, 'train_acc': 0.3125}}
2025-10-15 15:50:21 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 13.050383567810059, 'train_avg_loss': 0.8156489729881287, 'train_seen': 16, 'train_correct': 5, 'train_acc': 0.3125}}
2025-10-15 15:50:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:50:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:50:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:22 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:50:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:50:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-15 15:50:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-15 15:50:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=47.967831, avg_loss=0.695186, seen=69, correct=42, accuracy=0.608696
2025-10-15 15:50:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1944MB
2025-10-15 15:50:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:50:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.487946, avg_loss=0.687199, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:50:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1944MB
2025-10-15 15:50:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:50:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:50:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-15 15:50:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:50:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:50:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:50:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:50:31 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.833134, avg_loss=0.739571, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:50:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2008MB allocated=1961MB
2025-10-15 15:50:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.750395894050598, 'train_avg_loss': 0.6875989735126495, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:50:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.833133697509766, 'train_avg_loss': 0.7395708560943604, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:50:33 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.833133697509766, 'train_avg_loss': 0.7395708560943604, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:50:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:50:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:50:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:36 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:50:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:37 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:50:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:50:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:50:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=137.485352, avg_loss=0.687427, seen=200, correct=113, accuracy=0.565000
2025-10-15 15:50:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1952MB
2025-10-15 15:50:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:50:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.882584, avg_loss=0.647065, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:50:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1952MB
2025-10-15 15:50:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:50:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:50:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-15 15:50:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:50:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:50:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:50:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:50:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.066435, avg_loss=0.754152, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:50:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2002MB allocated=1969MB
2025-10-15 15:50:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.0751270055770874, 'train_avg_loss': 0.7687817513942719, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:50:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.066434860229492, 'train_avg_loss': 0.7541521787643433, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:50:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.066434860229492, 'train_avg_loss': 0.7541521787643433, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:50:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:50:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:50:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:50 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:50:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:50:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:50:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:50:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.781830, avg_loss=0.678909, seen=200, correct=118, accuracy=0.590000
2025-10-15 15:50:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:50:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1961MB
2025-10-15 15:50:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:50:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:50:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:50:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.801849, avg_loss=0.695046, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:50:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:50:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:50:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1961MB
2025-10-15 15:51:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:51:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:51:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-15 15:51:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:51:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:51:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:51:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:51:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.866457, avg_loss=0.616654, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:51:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2032MB allocated=1977MB
2025-10-15 15:51:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.2246260046958923, 'train_avg_loss': 0.5561565011739731, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:51:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.866456985473633, 'train_avg_loss': 0.616653561592102, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:51:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.866456985473633, 'train_avg_loss': 0.616653561592102, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:51:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:51:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:51:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:06 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:51:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:51:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:07 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:51:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-15 15:51:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-15 15:51:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=88.019707, avg_loss=0.666816, seen=132, correct=77, accuracy=0.583333
2025-10-15 15:51:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-15 15:51:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:51:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:51:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.617046, avg_loss=0.740426, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:51:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-15 15:51:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:51:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:51:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-15 15:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:51:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:51:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:51:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:51:18 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.072621, avg_loss=0.629539, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:51:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2046MB allocated=1986MB
2025-10-15 15:51:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.908316731452942, 'train_avg_loss': 0.7270791828632355, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:51:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.07262134552002, 'train_avg_loss': 0.6295388340950012, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:51:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.07262134552002, 'train_avg_loss': 0.6295388340950012, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:51:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:51:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:51:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:22 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:51:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:51:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:51:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-15 15:51:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-15 15:51:25 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=75.564659, avg_loss=0.686951, seen=110, correct=64, accuracy=0.581818
2025-10-15 15:51:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1977MB
2025-10-15 15:51:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:51:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:51:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.730185, avg_loss=0.693255, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:51:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1977MB
2025-10-15 15:51:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:51:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:51:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-15 15:51:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:51:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:51:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:51:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:51:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.090543, avg_loss=0.568159, seen=16, correct=13, accuracy=0.812500
2025-10-15 15:51:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2066MB allocated=1994MB
2025-10-15 15:51:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 1.4717239141464233, 'train_avg_loss': 0.36793097853660583, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:51:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.090542793273926, 'train_avg_loss': 0.5681589245796204, 'train_seen': 16, 'train_correct': 13, 'train_acc': 0.8125}}
2025-10-15 15:51:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.090542793273926, 'train_avg_loss': 0.5681589245796204, 'train_seen': 16, 'train_correct': 13, 'train_acc': 0.8125}}
2025-10-15 15:51:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:51:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:51:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:37 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:51:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:51:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-15 15:51:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-15 15:51:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=55.884903, avg_loss=0.673312, seen=83, correct=49, accuracy=0.590361
2025-10-15 15:51:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1986MB
2025-10-15 15:51:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:51:44 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.085636, avg_loss=0.652141, seen=40, correct=28, accuracy=0.700000
2025-10-15 15:51:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1986MB
2025-10-15 15:51:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-15 15:51:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:51:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-15 15:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:51:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:51:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:51:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:51:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.465027, avg_loss=0.716564, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:51:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-15 15:51:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.0165635347366333, 'train_avg_loss': 0.7541408836841583, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:51:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.46502685546875, 'train_avg_loss': 0.7165641784667969, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:51:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.46502685546875, 'train_avg_loss': 0.7165641784667969, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:51:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:51:52 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:51:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:53 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:51:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:51:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:54 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:51:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-15 15:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-15 15:51:56 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=38.623276, avg_loss=0.715246, seen=54, correct=24, accuracy=0.444444
2025-10-15 15:51:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:51:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1994MB
2025-10-15 15:51:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:51:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:51:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:51:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:51:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.879999, avg_loss=0.622000, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:51:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1994MB
2025-10-15 15:52:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:52:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:52:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-15 15:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:02 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:52:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:02 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:52:02 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:52:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:52:03 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.431443, avg_loss=0.651965, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:52:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2046MB allocated=2011MB
2025-10-15 15:52:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.9814484119415283, 'train_avg_loss': 0.7453621029853821, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:52:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.431443214416504, 'train_avg_loss': 0.6519652009010315, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:52:06 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.431443214416504, 'train_avg_loss': 0.6519652009010315, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:52:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:52:08 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:52:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:08 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:52:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:09 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:52:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-15 15:52:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-15 15:52:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=94.053230, avg_loss=0.691568, seen=136, correct=77, accuracy=0.566176
2025-10-15 15:52:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=2003MB
2025-10-15 15:52:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:52:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:52:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.080185, avg_loss=0.627005, seen=40, correct=28, accuracy=0.700000
2025-10-15 15:52:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=2003MB
2025-10-15 15:52:17 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-15 15:52:17 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:52:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-15 15:52:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:52:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:52:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:52:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:52:19 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.703514, avg_loss=0.668970, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:52:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2084MB allocated=2019MB
2025-10-15 15:52:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.2753148078918457, 'train_avg_loss': 0.5688287019729614, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:52:22 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.703514099121094, 'train_avg_loss': 0.6689696311950684, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:52:22 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.703514099121094, 'train_avg_loss': 0.6689696311950684, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:52:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:52:23 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:52:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:24 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:52:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:52:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:26 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:52:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-15 15:52:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-15 15:52:28 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=91.877319, avg_loss=0.685652, seen=134, correct=75, accuracy=0.559701
2025-10-15 15:52:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=2011MB
2025-10-15 15:52:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:52:31 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.115679, avg_loss=0.727892, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:52:31 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:33 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=2011MB
2025-10-15 15:52:33 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:52:33 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:52:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-15 15:52:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:52:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:52:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:52:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:52:35 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.052544, avg_loss=0.690784, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:52:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2070MB allocated=2028MB
2025-10-15 15:52:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.975839138031006, 'train_avg_loss': 0.7439597845077515, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:52:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.052543640136719, 'train_avg_loss': 0.6907839775085449, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:52:38 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.052543640136719, 'train_avg_loss': 0.6907839775085449, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:52:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:52:39 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:52:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:40 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:52:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:52:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:42 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:52:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:52:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.070297, avg_loss=0.680351, seen=200, correct=109, accuracy=0.545000
2025-10-15 15:52:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2019MB
2025-10-15 15:52:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:52:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.500938, avg_loss=0.662523, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:52:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2019MB
2025-10-15 15:52:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:52:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:52:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-15 15:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:52:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:52:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:52:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:52:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.786471, avg_loss=0.611654, seen=16, correct=12, accuracy=0.750000
2025-10-15 15:52:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:52:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:52:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2096MB allocated=2036MB
2025-10-15 15:52:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.353700637817383, 'train_avg_loss': 0.5884251594543457, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:52:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.786471366882324, 'train_avg_loss': 0.6116544604301453, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:52:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.786471366882324, 'train_avg_loss': 0.6116544604301453, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:52:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:52:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:52:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:52:57 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:52:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:52:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:52:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:52:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:53:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.214401, avg_loss=0.666072, seen=200, correct=115, accuracy=0.575000
2025-10-15 15:53:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2028MB
2025-10-15 15:53:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:53:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:53:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.382071, avg_loss=0.684552, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:53:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2028MB
2025-10-15 15:53:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:53:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:53:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-15 15:53:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:53:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:53:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:53:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:53:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.757629, avg_loss=0.734852, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:53:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2094MB allocated=2044MB
2025-10-15 15:53:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.516286849975586, 'train_avg_loss': 0.6290717124938965, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:53:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.75762939453125, 'train_avg_loss': 0.7348518371582031, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:53:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.75762939453125, 'train_avg_loss': 0.7348518371582031, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:53:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:53:14 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:53:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:53:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:16 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:53:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-15 15:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-15 15:53:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=73.112236, avg_loss=0.664657, seen=110, correct=62, accuracy=0.563636
2025-10-15 15:53:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2036MB
2025-10-15 15:53:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:53:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:53:21 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.696295, avg_loss=0.692407, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:53:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2036MB
2025-10-15 15:53:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:53:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:53:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-15 15:53:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:53:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:53:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:53:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:53:24 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.198687, avg_loss=0.699918, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:53:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2090MB allocated=2053MB
2025-10-15 15:53:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.9638726711273193, 'train_avg_loss': 0.7409681677818298, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:53:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.198686599731445, 'train_avg_loss': 0.6999179124832153, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:53:26 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.198686599731445, 'train_avg_loss': 0.6999179124832153, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:53:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:53:27 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:53:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:27 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:53:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:29 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:53:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-15 15:53:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-15 15:53:32 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=109.284325, avg_loss=0.714277, seen=153, correct=82, accuracy=0.535948
2025-10-15 15:53:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2044MB
2025-10-15 15:53:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:53:35 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.294640, avg_loss=0.682366, seen=40, correct=19, accuracy=0.475000
2025-10-15 15:53:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2044MB
2025-10-15 15:53:36 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-15 15:53:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:53:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-15 15:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:53:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:53:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:53:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:53:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=13.341187, avg_loss=0.833824, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:53:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2114MB allocated=2061MB
2025-10-15 15:53:40 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.1422420740127563, 'train_avg_loss': 0.7855605185031891, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:53:40 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 13.3411865234375, 'train_avg_loss': 0.8338241577148438, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:53:40 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 13.3411865234375, 'train_avg_loss': 0.8338241577148438, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:53:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:53:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:53:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:42 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:53:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:53:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:53:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-15 15:53:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-15 15:53:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=103.621819, avg_loss=0.704910, seen=147, correct=82, accuracy=0.557823
2025-10-15 15:53:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2053MB
2025-10-15 15:53:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:53:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.287094, avg_loss=0.707177, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:53:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2053MB
2025-10-15 15:53:52 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:53:52 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:53:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-15 15:53:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:52 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:53:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:52 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:53:52 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:53:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:53:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=7.796761, avg_loss=0.487298, seen=16, correct=12, accuracy=0.750000
2025-10-15 15:53:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:53:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:53:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-15 15:53:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.056485950946808, 'train_avg_loss': 0.514121487736702, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:53:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 7.796760559082031, 'train_avg_loss': 0.48729753494262695, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:53:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 7.796760559082031, 'train_avg_loss': 0.48729753494262695, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:53:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:53:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:53:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:53:57 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:53:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:53:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-15 15:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:53:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-15 15:54:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=126.283096, avg_loss=0.671719, seen=188, correct=105, accuracy=0.558511
2025-10-15 15:54:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-15 15:54:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:54:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:54:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.013004, avg_loss=0.750325, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:54:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-15 15:54:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:54:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:54:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-15 15:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:54:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:54:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:54:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:54:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.084969, avg_loss=0.755311, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:54:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2078MB
2025-10-15 15:54:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.2269139289855957, 'train_avg_loss': 0.8067284822463989, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:54:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.084968566894531, 'train_avg_loss': 0.7553105354309082, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:54:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.084968566894531, 'train_avg_loss': 0.7553105354309082, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:54:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:54:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:54:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:13 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:54:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:54:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:54:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-15 15:54:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-15 15:54:17 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=110.978493, avg_loss=0.693616, seen=160, correct=86, accuracy=0.537500
2025-10-15 15:54:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2070MB
2025-10-15 15:54:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:54:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:54:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.416162, avg_loss=0.660404, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:54:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2070MB
2025-10-15 15:54:23 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:54:23 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:54:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-15 15:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:23 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:54:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:23 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:54:23 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:54:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:54:25 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.489520, avg_loss=0.655595, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:54:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2124MB allocated=2086MB
2025-10-15 15:54:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.1328622102737427, 'train_avg_loss': 0.7832155525684357, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:54:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.489520072937012, 'train_avg_loss': 0.6555950045585632, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:54:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.489520072937012, 'train_avg_loss': 0.6555950045585632, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:54:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:54:29 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:54:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:29 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:54:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:54:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:31 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:54:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-15 15:54:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-15 15:54:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=106.893295, avg_loss=0.663934, seen=161, correct=99, accuracy=0.614907
2025-10-15 15:54:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2078MB
2025-10-15 15:54:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:54:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.340008, avg_loss=0.683500, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:54:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2078MB
2025-10-15 15:54:39 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:54:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:54:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-15 15:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:39 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:54:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:39 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:54:39 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:54:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:54:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.977324, avg_loss=0.748583, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:54:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2142MB allocated=2095MB
2025-10-15 15:54:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.0009350180625916, 'train_avg_loss': 0.7502337545156479, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:54:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.977323532104492, 'train_avg_loss': 0.7485827207565308, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:54:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.977323532104492, 'train_avg_loss': 0.7485827207565308, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:54:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:54:44 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:54:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:45 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:54:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:54:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:46 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:54:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-15 15:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-15 15:54:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=97.638245, avg_loss=0.723246, seen=135, correct=64, accuracy=0.474074
2025-10-15 15:54:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2086MB
2025-10-15 15:54:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:54:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:54:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.143433, avg_loss=0.678586, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:54:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2086MB
2025-10-15 15:54:55 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:54:55 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:54:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-15 15:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:55 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:54:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:54:55 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:54:55 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:54:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:54:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=8.873894, avg_loss=0.554618, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:54:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:54:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:54:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:54:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2168MB allocated=2103MB
2025-10-15 15:54:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.4335042238235474, 'train_avg_loss': 0.6083760559558868, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:54:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 8.873893737792969, 'train_avg_loss': 0.5546183586120605, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:54:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 8.873893737792969, 'train_avg_loss': 0.5546183586120605, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:54:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:55:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:55:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:00 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:55:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:55:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:55:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-15 15:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-15 15:55:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=130.936066, avg_loss=0.696468, seen=188, correct=101, accuracy=0.537234
2025-10-15 15:55:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2095MB
2025-10-15 15:55:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:55:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:55:09 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.013636, avg_loss=0.625341, seen=40, correct=27, accuracy=0.675000
2025-10-15 15:55:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2095MB
2025-10-15 15:55:11 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-15 15:55:11 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:55:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-15 15:55:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:55:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:55:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:55:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:55:13 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.027485, avg_loss=0.689218, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:55:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2112MB
2025-10-15 15:55:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.387207567691803, 'train_avg_loss': 0.5968018919229507, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:55:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.027484893798828, 'train_avg_loss': 0.6892178058624268, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:55:15 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.027484893798828, 'train_avg_loss': 0.6892178058624268, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:55:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:55:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:55:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:17 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:55:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:19 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:55:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-15 15:55:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-15 15:55:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=62.512703, avg_loss=0.702390, seen=89, correct=47, accuracy=0.528090
2025-10-15 15:55:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-15 15:55:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:55:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:55:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.102074, avg_loss=0.677552, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:55:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-15 15:55:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:55:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:55:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-15 15:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:55:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:55:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:55:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:55:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.090845, avg_loss=0.630678, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:55:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2174MB allocated=2120MB
2025-10-15 15:55:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.165122866630554, 'train_avg_loss': 0.7912807166576385, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:55:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.090845108032227, 'train_avg_loss': 0.6306778192520142, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:55:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.090845108032227, 'train_avg_loss': 0.6306778192520142, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:55:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:55:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:55:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:32 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:55:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:55:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:55:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-15 15:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-15 15:55:35 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.225906, avg_loss=0.747810, seen=11, correct=7, accuracy=0.636364
2025-10-15 15:55:35 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2112MB
2025-10-15 15:55:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:55:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.538464, avg_loss=0.738462, seen=40, correct=19, accuracy=0.475000
2025-10-15 15:55:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2112MB
2025-10-15 15:55:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-15 15:55:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:55:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-15 15:55:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:55:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:55:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:55:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:55:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.685651, avg_loss=0.667853, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:55:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2172MB allocated=2128MB
2025-10-15 15:55:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.5958092212677, 'train_avg_loss': 0.648952305316925, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:55:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.685650825500488, 'train_avg_loss': 0.6678531765937805, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:55:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.685650825500488, 'train_avg_loss': 0.6678531765937805, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:55:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:55:47 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:55:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:55:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:55:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-15 15:55:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-15 15:55:50 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=50.830696, avg_loss=0.705982, seen=72, correct=39, accuracy=0.541667
2025-10-15 15:55:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2160MB allocated=2120MB
2025-10-15 15:55:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:55:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.955135, avg_loss=0.798878, seen=40, correct=17, accuracy=0.425000
2025-10-15 15:55:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:55:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2160MB allocated=2120MB
2025-10-15 15:55:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-15 15:55:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:55:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-15 15:55:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:55:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:55:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:55:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:55:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:55:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:55:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.308199, avg_loss=0.581762, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:55:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:55:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2194MB allocated=2137MB
2025-10-15 15:56:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.2152425050735474, 'train_avg_loss': 0.5538106262683868, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:56:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.308198928833008, 'train_avg_loss': 0.581762433052063, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:56:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.308198928833008, 'train_avg_loss': 0.581762433052063, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:56:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:56:01 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:56:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:02 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:56:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:03 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:56:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-15 15:56:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-15 15:56:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=83.619774, avg_loss=0.702687, seen=119, correct=64, accuracy=0.537815
2025-10-15 15:56:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2160MB allocated=2128MB
2025-10-15 15:56:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:56:13 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.052797, avg_loss=0.676320, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:56:13 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2160MB allocated=2128MB
2025-10-15 15:56:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:56:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:56:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-15 15:56:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:56:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:56:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:56:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:56:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.838951, avg_loss=0.739934, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:56:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2182MB allocated=2145MB
2025-10-15 15:56:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.906457841396332, 'train_avg_loss': 0.726614460349083, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:56:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.838951110839844, 'train_avg_loss': 0.7399344444274902, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:56:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.838951110839844, 'train_avg_loss': 0.7399344444274902, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:56:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:56:22 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:56:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:22 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:56:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:56:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:56:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:56:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:56:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.891663, avg_loss=0.694458, seen=200, correct=105, accuracy=0.525000
2025-10-15 15:56:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2137MB
2025-10-15 15:56:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:56:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:56:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.158674, avg_loss=0.753967, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:56:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2137MB
2025-10-15 15:56:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:56:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:56:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-15 15:56:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:56:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:56:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:56:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:56:37 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.353896, avg_loss=0.647119, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:56:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2202MB allocated=2154MB
2025-10-15 15:56:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 1.9004147052764893, 'train_avg_loss': 0.4751036763191223, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:56:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.353896141052246, 'train_avg_loss': 0.6471185088157654, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:56:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.353896141052246, 'train_avg_loss': 0.6471185088157654, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:56:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:56:41 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:56:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:41 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:56:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:56:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:56:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-15 15:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-15 15:56:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=39.297913, avg_loss=0.689437, seen=57, correct=29, accuracy=0.508772
2025-10-15 15:56:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2145MB
2025-10-15 15:56:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:56:47 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.656755, avg_loss=0.641419, seen=40, correct=28, accuracy=0.700000
2025-10-15 15:56:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2145MB
2025-10-15 15:56:49 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-15 15:56:49 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:56:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-15 15:56:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:50 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:56:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:50 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:56:50 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:56:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:56:51 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.533279, avg_loss=0.720830, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:56:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:56:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2222MB allocated=2162MB
2025-10-15 15:56:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.513375759124756, 'train_avg_loss': 0.628343939781189, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:56:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.533279418945312, 'train_avg_loss': 0.720829963684082, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:56:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.533279418945312, 'train_avg_loss': 0.720829963684082, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:56:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:56:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:56:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:56:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:56:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:56:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:56:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:57:00 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.389313, avg_loss=0.696947, seen=200, correct=110, accuracy=0.550000
2025-10-15 15:57:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2154MB
2025-10-15 15:57:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:57:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.376062, avg_loss=0.784402, seen=40, correct=14, accuracy=0.350000
2025-10-15 15:57:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2154MB
2025-10-15 15:57:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.350000
2025-10-15 15:57:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:57:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-15 15:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:57:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:57:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:57:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:57:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.402051, avg_loss=0.712628, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:57:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2170MB
2025-10-15 15:57:11 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.955611228942871, 'train_avg_loss': 0.7389028072357178, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:57:11 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.402050971984863, 'train_avg_loss': 0.712628185749054, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:57:11 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.402050971984863, 'train_avg_loss': 0.712628185749054, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:57:11 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:57:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:57:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:13 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:57:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:57:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:57:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:57:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=136.256516, avg_loss=0.681283, seen=200, correct=111, accuracy=0.555000
2025-10-15 15:57:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2162MB
2025-10-15 15:57:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:57:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:57:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.832739, avg_loss=0.670818, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:57:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2162MB
2025-10-15 15:57:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:57:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:57:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-15 15:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:57:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:57:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:57:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:57:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.084299, avg_loss=0.692769, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:57:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2226MB allocated=2179MB
2025-10-15 15:57:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.767890453338623, 'train_avg_loss': 0.6919726133346558, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:57:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.084299087524414, 'train_avg_loss': 0.6927686929702759, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:57:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.084299087524414, 'train_avg_loss': 0.6927686929702759, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:57:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:57:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:57:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:32 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:57:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:57:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-15 15:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-15 15:57:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.862997, avg_loss=0.805727, seen=11, correct=5, accuracy=0.454545
2025-10-15 15:57:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2170MB
2025-10-15 15:57:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:57:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.451443, avg_loss=0.711286, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:57:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2170MB
2025-10-15 15:57:39 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:57:39 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:57:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-15 15:57:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:40 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:57:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:40 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:57:40 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:57:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:57:41 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.517562, avg_loss=0.657348, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:57:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2236MB allocated=2187MB
2025-10-15 15:57:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.6528321504592896, 'train_avg_loss': 0.6632080376148224, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:57:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.517561912536621, 'train_avg_loss': 0.6573476195335388, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:57:43 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.517561912536621, 'train_avg_loss': 0.6573476195335388, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:57:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:57:45 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:57:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:45 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:57:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:57:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:57:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-15 15:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-15 15:57:49 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=87.354370, avg_loss=0.693289, seen=126, correct=78, accuracy=0.619048
2025-10-15 15:57:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2179MB
2025-10-15 15:57:52 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:52 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:57:53 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.066486, avg_loss=0.601662, seen=40, correct=27, accuracy=0.675000
2025-10-15 15:57:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:57:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2179MB
2025-10-15 15:57:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-15 15:57:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:57:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-15 15:57:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:57:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:57:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:57:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:57:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:57:58 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.167432, avg_loss=0.635464, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:57:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:57:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:57:59 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2258MB allocated=2196MB
2025-10-15 15:58:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.1162959337234497, 'train_avg_loss': 0.5290739834308624, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:58:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.167431831359863, 'train_avg_loss': 0.6354644894599915, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:58:00 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.167431831359863, 'train_avg_loss': 0.6354644894599915, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:58:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:58:02 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:58:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:02 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:58:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:04 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:58:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-15 15:58:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-15 15:58:05 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=48.896969, avg_loss=0.776142, seen=63, correct=27, accuracy=0.428571
2025-10-15 15:58:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2187MB
2025-10-15 15:58:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:58:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:58:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.546898, avg_loss=0.688672, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:58:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2187MB
2025-10-15 15:58:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:58:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:58:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-15 15:58:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:58:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:58:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:58:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:58:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.383115, avg_loss=0.586445, seen=16, correct=12, accuracy=0.750000
2025-10-15 15:58:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2204MB
2025-10-15 15:58:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 1.9805216789245605, 'train_avg_loss': 0.49513041973114014, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:58:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.3831148147583, 'train_avg_loss': 0.5864446759223938, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:58:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.3831148147583, 'train_avg_loss': 0.5864446759223938, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:58:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:58:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:58:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:16 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:58:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:58:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:58:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:58:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.989075, avg_loss=0.669945, seen=200, correct=113, accuracy=0.565000
2025-10-15 15:58:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:23 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2196MB
2025-10-15 15:58:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:58:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:58:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.234905, avg_loss=0.680873, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:58:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2196MB
2025-10-15 15:58:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:58:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:58:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-15 15:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:58:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:58:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:58:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:58:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.954325, avg_loss=0.747145, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:58:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2242MB allocated=2212MB
2025-10-15 15:58:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.337833881378174, 'train_avg_loss': 0.8344584703445435, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:58:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.954324722290039, 'train_avg_loss': 0.7471452951431274, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:58:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.954324722290039, 'train_avg_loss': 0.7471452951431274, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:58:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:58:34 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:58:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:34 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:58:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:36 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:58:36 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-15 15:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-15 15:58:38 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=94.369690, avg_loss=0.709547, seen=133, correct=72, accuracy=0.541353
2025-10-15 15:58:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-10-15 15:58:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:58:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:58:42 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.046959, avg_loss=0.726174, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:58:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:42 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-10-15 15:58:44 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:58:44 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:58:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-15 15:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:58:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:58:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:58:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:58:46 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.172585, avg_loss=0.760787, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:58:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-15 15:58:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.851760745048523, 'train_avg_loss': 0.7129401862621307, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:58:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.172584533691406, 'train_avg_loss': 0.7607865333557129, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:58:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.172584533691406, 'train_avg_loss': 0.7607865333557129, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:58:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:58:50 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:58:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:51 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:58:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:52 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:58:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-15 15:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-15 15:58:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=7.876183, avg_loss=0.716017, seen=11, correct=6, accuracy=0.545455
2025-10-15 15:58:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:58:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2212MB
2025-10-15 15:58:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:58:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:58:58 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.247078, avg_loss=0.756177, seen=40, correct=19, accuracy=0.475000
2025-10-15 15:58:58 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:58:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:00 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2212MB
2025-10-15 15:59:00 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-15 15:59:00 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:59:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-15 15:59:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:59:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:59:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:59:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:59:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.738887, avg_loss=0.733680, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:59:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2278MB allocated=2229MB
2025-10-15 15:59:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.7174694538116455, 'train_avg_loss': 0.9293673634529114, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:59:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.738886833190918, 'train_avg_loss': 0.7336804270744324, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:59:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.738886833190918, 'train_avg_loss': 0.7336804270744324, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:59:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:06 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:59:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:07 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:59:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:59:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-15 15:59:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-15 15:59:11 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=96.968079, avg_loss=0.664165, seen=146, correct=87, accuracy=0.595890
2025-10-15 15:59:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2221MB
2025-10-15 15:59:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:59:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:59:15 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.371742, avg_loss=0.659294, seen=40, correct=27, accuracy=0.675000
2025-10-15 15:59:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2221MB
2025-10-15 15:59:16 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.675000
2025-10-15 15:59:16 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:59:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-15 15:59:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:17 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:59:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:17 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:59:17 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:59:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:59:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.359172, avg_loss=0.647448, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:59:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2282MB allocated=2238MB
2025-10-15 15:59:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.296924591064453, 'train_avg_loss': 0.5742311477661133, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:59:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.359171867370605, 'train_avg_loss': 0.6474482417106628, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:59:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.359171867370605, 'train_avg_loss': 0.6474482417106628, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:59:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:59:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:21 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:59:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:59:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:59:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-15 15:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-15 15:59:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=33.023922, avg_loss=0.717911, seen=46, correct=25, accuracy=0.543478
2025-10-15 15:59:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2229MB
2025-10-15 15:59:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:59:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.660490, avg_loss=0.666512, seen=40, correct=29, accuracy=0.725000
2025-10-15 15:59:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2229MB
2025-10-15 15:59:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-15 15:59:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:59:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-15 15:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:30 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:59:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:30 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:59:30 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:59:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:59:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.472569, avg_loss=0.717036, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:59:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:33 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2294MB allocated=2246MB
2025-10-15 15:59:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.5825421810150146, 'train_avg_loss': 0.6456355452537537, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:59:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.472569465637207, 'train_avg_loss': 0.7170355916023254, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:59:34 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.472569465637207, 'train_avg_loss': 0.7170355916023254, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:59:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:59:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:36 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:59:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:59:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-15 15:59:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-15 15:59:40 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=72.204819, avg_loss=0.722048, seen=100, correct=52, accuracy=0.520000
2025-10-15 15:59:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-15 15:59:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:59:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:59:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.450989, avg_loss=0.761275, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:59:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-15 15:59:46 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:59:46 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:59:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-15 15:59:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:46 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:59:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:59:46 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:59:46 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:59:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:59:48 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.609824, avg_loss=0.600614, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:59:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:59:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:59:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2312MB allocated=2254MB
2025-10-15 15:59:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.2918585538864136, 'train_avg_loss': 0.5729646384716034, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:59:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.609824180603027, 'train_avg_loss': 0.6006140112876892, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:59:50 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.609824180603027, 'train_avg_loss': 0.6006140112876892, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:59:51 (federatedscope.core.workers.server:493) INFO: Server: Training is finished! (skip final evaluation)
2025-10-15 15:59:51 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 16.292579333333332, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8752, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:51 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-10-15 15:59:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:51 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 16.298143366666668, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:51 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-10-15 15:59:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:51 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 16.2359313, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:51 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-10-15 15:59:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 16.19389195, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:52 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-10-15 15:59:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 16.151269516666666, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:52 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-10-15 15:59:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 16.1081665, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:52 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-10-15 15:59:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 16.065483866666668, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:52 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-10-15 15:59:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 16.0219773, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:52 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-10-15 15:59:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 15.979010466666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:52 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-10-15 15:59:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 15.9349124, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:52 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-10-15 15:59:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 15.890322383333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:52 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-10-15 15:59:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 15.826688466666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:53 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-10-15 15:59:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 15.781039533333335, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:53 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-10-15 15:59:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 15.737131233333335, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:53 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-10-15 15:59:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 15.69284695, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:53 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-10-15 15:59:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 15.646204666666668, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:53 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-10-15 15:59:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 15.6021605, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:53 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-10-15 15:59:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 15.5576089, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:53 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-10-15 15:59:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 15.513544316666668, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:54 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-10-15 15:59:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 15.471756166666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:54 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-10-15 15:59:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 15.429583766666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:54 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-10-15 15:59:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 15.387786733333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:54 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-10-15 15:59:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 15.330565083333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:54 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-10-15 15:59:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 15.288634983333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:54 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-10-15 15:59:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 15.242765666666665, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:54 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-10-15 15:59:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 15.1920719, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:54 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-10-15 15:59:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 15.14368885, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:55 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-10-15 15:59:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 15.0954159, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:55 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-10-15 15:59:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 15.048119466666666, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:55 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-10-15 15:59:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 15.00525495, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:55 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-10-15 15:59:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 14.963471, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:55 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-10-15 15:59:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 14.905545933333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:55 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-10-15 15:59:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 14.863854833333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:55 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-10-15 15:59:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 14.813956333333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:56 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-10-15 15:59:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 14.76187825, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:56 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-10-15 15:59:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 14.714615116666666, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:56 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-10-15 15:59:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 14.668697316666668, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:56 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-10-15 15:59:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 14.622662333333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:56 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-10-15 15:59:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 14.577678183333335, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:56 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-10-15 15:59:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 14.5329852, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:56 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-10-15 15:59:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 14.488868033333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:56 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-10-15 15:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 14.4289334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:57 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-10-15 15:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 14.378807866666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:57 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-10-15 15:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 14.33442735, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:57 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-10-15 15:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 14.289912483333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:57 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-10-15 15:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 14.2446854, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:57 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-10-15 15:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 14.199087216666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:57 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-10-15 15:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 14.1542703, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:57 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-10-15 15:59:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 14.1096648, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:57 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-10-15 15:59:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 14.06511835, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:58 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-10-15 15:59:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 14.021159233333332, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:58 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-10-15 15:59:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 13.959760183333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:58 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-10-15 15:59:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 13.915653166666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:58 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-10-15 15:59:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:59:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 13.872928333333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:59:58 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 15.112095870987654, 'sys_avg/total_model_size': '495.12M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '875.33K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-10-15 15:59:58 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.7250691282663136, 'sys_std/total_model_size': '68.01M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '119.06K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
