2025-10-15 15:05:25 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/3_fedbis_oracle_u7_1.0/exp_print.log
2025-10-15 15:05:25 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/3_fedbis_oracle_u7_1.0/
2025-10-15 15:05:49 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-10-15 15:06:32 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-10-15 15:06:34 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-10-15 15:06:34 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-10-15 15:06:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140178740535360 | out_emb=(Linear) num=151646 ptr=140178740535360 | lora_ptr=None
2025-10-15 15:06:49 (federatedscope.llm.model.model_builder:188) INFO: [Warmup-Init] loaded from checkpoints_1.0_oracle/final_tldr_choice_qwen_fedbis_oracle_u7_round_211.ckpt (round=211) | missing=291 unexpected=0
2025-10-15 15:06:49 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-10-15 15:06:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:06:53 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-10-15 15:06:54 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:06:56 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-10-15 15:06:56 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:06:58 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-10-15 15:06:59 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:01 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-10-15 15:07:01 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:04 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-10-15 15:07:04 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:06 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-10-15 15:07:06 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:09 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-10-15 15:07:09 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:11 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-10-15 15:07:12 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:14 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-10-15 15:07:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:18 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-10-15 15:07:18 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:21 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-10-15 15:07:21 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:23 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-10-15 15:07:23 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:26 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-10-15 15:07:26 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:29 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-10-15 15:07:29 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:31 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-10-15 15:07:31 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:34 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-10-15 15:07:34 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:37 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-10-15 15:07:37 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:39 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-10-15 15:07:39 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:42 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-10-15 15:07:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:45 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-10-15 15:07:46 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:48 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-10-15 15:07:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:51 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-10-15 15:07:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:54 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-10-15 15:07:54 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:56 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-10-15 15:07:57 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:07:59 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-10-15 15:07:59 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:02 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-10-15 15:08:02 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:04 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-10-15 15:08:05 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:07 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-10-15 15:08:07 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:10 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-10-15 15:08:10 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:14 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-10-15 15:08:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:16 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-10-15 15:08:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:19 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-10-15 15:08:19 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:22 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-10-15 15:08:22 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:25 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-10-15 15:08:25 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:27 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-10-15 15:08:28 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:30 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-10-15 15:08:30 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:33 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-10-15 15:08:33 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:36 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-10-15 15:08:36 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:39 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-10-15 15:08:39 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:42 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-10-15 15:08:42 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:45 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-10-15 15:08:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:48 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-10-15 15:08:48 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:51 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-10-15 15:08:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:53 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-10-15 15:08:54 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:56 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-10-15 15:08:56 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:08:59 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-10-15 15:08:59 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:09:02 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-10-15 15:09:02 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:09:04 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-10-15 15:09:05 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:09:07 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-10-15 15:09:07 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:09:11 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-10-15 15:09:11 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:09:14 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-10-15 15:09:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:09:16 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-10-15 15:09:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-10-15 15:09:19 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-10-15 15:09:19 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-10-15 15:09:19 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 2688.
2025-10-15 15:09:19 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 2978.
2025-10-15 15:09:19 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 2688. 
Preserved para names in local update: {'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_6.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_5.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_6.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_5.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_3.weight'}.
2025-10-15 15:09:19 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-10-15 15:09:19 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-10-15 15:09:19 (federatedscope.llm.llm_local.server:200) INFO: Waited all clients join, start now...
2025-10-15 15:09:20 (federatedscope.llm.llm_local.server:217) INFO: ----------- Starting training (Round #0) -------------
2025-10-15 15:09:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:09:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:09:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:09:30 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:09:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:09:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:09:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:09:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:09:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.443298, avg_loss=0.712216, seen=200, correct=104, accuracy=0.520000
2025-10-15 15:09:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:09:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1818MB allocated=1793MB
2025-10-15 15:09:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:09:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:09:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:09:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.992504, avg_loss=0.649813, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:09:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:09:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1818MB allocated=1793MB
2025-10-15 15:09:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:09:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:09:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-10-15 15:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:09:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:09:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:09:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:09:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:09:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.150064, avg_loss=0.634379, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:09:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:09:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1818MB
2025-10-15 15:09:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.2948094606399536, 'train_avg_loss': 0.8237023651599884, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:09:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.150064468383789, 'train_avg_loss': 0.6343790292739868, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:09:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.150064468383789, 'train_avg_loss': 0.6343790292739868, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:09:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:09:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:09:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:09:49 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:09:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:09:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:09:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-10-15 15:09:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:09:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-15 15:09:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=74, loss_sum=53.341179, avg_loss=0.720827, seen=74, correct=37, accuracy=0.500000
2025-10-15 15:09:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:09:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:09:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1810MB
2025-10-15 15:09:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:09:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:09:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.253935, avg_loss=0.706348, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:09:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:09:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:09:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:09:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1810MB
2025-10-15 15:09:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:09:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:09:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-10-15 15:09:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:00 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:10:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:00 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:10:00 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:10:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:10:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.750496, avg_loss=0.796906, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:10:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1890MB allocated=1826MB
2025-10-15 15:10:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.997194528579712, 'train_avg_loss': 0.749298632144928, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:10:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.750495910644531, 'train_avg_loss': 0.7969059944152832, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:10:03 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.750495910644531, 'train_avg_loss': 0.7969059944152832, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:10:03 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:10:04 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:10:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:04 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:10:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:10:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-15 15:10:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-15 15:10:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=60.831249, avg_loss=0.732907, seen=83, correct=45, accuracy=0.542169
2025-10-15 15:10:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:08 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1818MB
2025-10-15 15:10:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:10:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.104042, avg_loss=0.752601, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:10:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1818MB
2025-10-15 15:10:13 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:10:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:10:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-10-15 15:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:10:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:10:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:10:15 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.146968, avg_loss=0.696685, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:10:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1884MB allocated=1835MB
2025-10-15 15:10:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.149382472038269, 'train_avg_loss': 0.7873456180095673, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:10:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.146967887878418, 'train_avg_loss': 0.6966854929924011, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:10:17 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.146967887878418, 'train_avg_loss': 0.6966854929924011, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:10:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:10:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:10:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:19 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:10:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:21 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:10:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:10:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.658386, avg_loss=0.698292, seen=200, correct=108, accuracy=0.540000
2025-10-15 15:10:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1826MB
2025-10-15 15:10:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:10:27 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.460720, avg_loss=0.686518, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:10:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1860MB allocated=1826MB
2025-10-15 15:10:28 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:10:28 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:10:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-10-15 15:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:29 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:10:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:29 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:10:29 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:10:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:10:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.716728, avg_loss=0.732296, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:10:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1888MB allocated=1843MB
2025-10-15 15:10:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.4711356163024902, 'train_avg_loss': 0.8677839040756226, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:10:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.716728210449219, 'train_avg_loss': 0.7322955131530762, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:10:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.716728210449219, 'train_avg_loss': 0.7322955131530762, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:10:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:10:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:10:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:33 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:10:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:10:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-10-15 15:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-10-15 15:10:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=137, loss_sum=98.582893, avg_loss=0.719583, seen=137, correct=65, accuracy=0.474453
2025-10-15 15:10:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1835MB
2025-10-15 15:10:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:10:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.332684, avg_loss=0.683317, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:10:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1835MB
2025-10-15 15:10:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:10:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:10:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-10-15 15:10:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:10:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:10:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:10:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:10:42 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.589167, avg_loss=0.724323, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:10:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1908MB allocated=1851MB
2025-10-15 15:10:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.2267401218414307, 'train_avg_loss': 0.8066850304603577, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:10:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.589166641235352, 'train_avg_loss': 0.7243229150772095, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:10:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.589166641235352, 'train_avg_loss': 0.7243229150772095, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:10:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:10:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:10:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:46 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:10:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:47 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:10:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-10-15 15:10:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-10-15 15:10:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=36, loss_sum=26.660212, avg_loss=0.740561, seen=36, correct=13, accuracy=0.361111
2025-10-15 15:10:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1843MB
2025-10-15 15:10:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:10:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.656651, avg_loss=0.716416, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:10:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1843MB
2025-10-15 15:10:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:10:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:10:53 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-10-15 15:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:53 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:10:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:53 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:10:53 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:10:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:10:54 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.964508, avg_loss=0.622782, seen=16, correct=12, accuracy=0.750000
2025-10-15 15:10:54 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:10:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:10:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1906MB allocated=1860MB
2025-10-15 15:10:56 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.4336100220680237, 'train_avg_loss': 0.6084025055170059, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:10:56 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.964508056640625, 'train_avg_loss': 0.6227817535400391, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:10:56 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.964508056640625, 'train_avg_loss': 0.6227817535400391, 'train_seen': 16, 'train_correct': 12, 'train_acc': 0.75}}
2025-10-15 15:10:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:10:58 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:10:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:10:58 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:10:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:11:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-10-15 15:11:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-15 15:11:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=112, loss_sum=78.675446, avg_loss=0.702459, seen=112, correct=56, accuracy=0.500000
2025-10-15 15:11:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1851MB
2025-10-15 15:11:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:11:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:11:05 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.314337, avg_loss=0.757858, seen=40, correct=17, accuracy=0.425000
2025-10-15 15:11:05 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1880MB allocated=1851MB
2025-10-15 15:11:07 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-15 15:11:07 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:11:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-10-15 15:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:07 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:11:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:07 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:11:07 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:11:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:11:09 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.116976, avg_loss=0.632311, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:11:09 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1902MB allocated=1868MB
2025-10-15 15:11:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.30288827419281, 'train_avg_loss': 0.5757220685482025, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:11:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.116975784301758, 'train_avg_loss': 0.6323109865188599, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:11:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.116975784301758, 'train_avg_loss': 0.6323109865188599, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:11:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:11:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:11:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:12 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:11:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:11:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:11:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:11:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.610443, avg_loss=0.713052, seen=200, correct=101, accuracy=0.505000
2025-10-15 15:11:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1860MB
2025-10-15 15:11:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:11:22 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.743561, avg_loss=0.668589, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:11:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1860MB
2025-10-15 15:11:24 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:11:24 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:11:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-10-15 15:11:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:24 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:11:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:24 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:11:24 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:11:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:11:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.675861, avg_loss=0.667241, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:11:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1930MB allocated=1877MB
2025-10-15 15:11:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.4709105491638184, 'train_avg_loss': 0.6177276372909546, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:11:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.675861358642578, 'train_avg_loss': 0.6672413349151611, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:11:27 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #42', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.675861358642578, 'train_avg_loss': 0.6672413349151611, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:11:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:11:28 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:11:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:29 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:11:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:11:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:30 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:11:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-10-15 15:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-10-15 15:11:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=170, loss_sum=119.178619, avg_loss=0.701051, seen=170, correct=91, accuracy=0.535294
2025-10-15 15:11:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1868MB
2025-10-15 15:11:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:11:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:11:37 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.461555, avg_loss=0.636539, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:11:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:37 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1868MB
2025-10-15 15:11:38 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:11:38 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:11:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-10-15 15:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:38 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:11:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:38 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:11:38 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:11:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.819057, avg_loss=0.738691, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:11:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1924MB allocated=1885MB
2025-10-15 15:11:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.600510835647583, 'train_avg_loss': 0.6501277089118958, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:11:42 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.819056510925293, 'train_avg_loss': 0.7386910319328308, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:11:42 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.819056510925293, 'train_avg_loss': 0.7386910319328308, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:11:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:11:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:11:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:43 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:11:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:11:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-10-15 15:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-10-15 15:11:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=123, loss_sum=87.635162, avg_loss=0.712481, seen=123, correct=64, accuracy=0.520325
2025-10-15 15:11:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:49 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1877MB
2025-10-15 15:11:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:11:50 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.468288, avg_loss=0.636707, seen=40, correct=28, accuracy=0.700000
2025-10-15 15:11:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1900MB allocated=1877MB
2025-10-15 15:11:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-15 15:11:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:11:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-10-15 15:11:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:11:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:11:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:11:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:11:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.777254, avg_loss=0.798578, seen=16, correct=5, accuracy=0.312500
2025-10-15 15:11:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:11:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1932MB allocated=1893MB
2025-10-15 15:11:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.1566444635391235, 'train_avg_loss': 0.7891611158847809, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:11:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.777254104614258, 'train_avg_loss': 0.7985783815383911, 'train_seen': 16, 'train_correct': 5, 'train_acc': 0.3125}}
2025-10-15 15:11:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.777254104614258, 'train_avg_loss': 0.7985783815383911, 'train_seen': 16, 'train_correct': 5, 'train_acc': 0.3125}}
2025-10-15 15:11:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:11:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:11:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:11:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:58 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:11:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-10-15 15:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:11:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:11:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:11:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=14, loss_sum=9.525837, avg_loss=0.680417, seen=14, correct=7, accuracy=0.500000
2025-10-15 15:11:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-15 15:12:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:12:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.345320, avg_loss=0.758633, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:12:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1885MB
2025-10-15 15:12:05 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:12:05 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:12:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-10-15 15:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:05 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:12:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:05 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:12:05 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:12:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:12:07 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.931248, avg_loss=0.620703, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:12:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1964MB allocated=1902MB
2025-10-15 15:12:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.28507661819458, 'train_avg_loss': 0.571269154548645, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:12:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.93124771118164, 'train_avg_loss': 0.6207029819488525, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:12:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.93124771118164, 'train_avg_loss': 0.6207029819488525, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:12:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:12:11 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:12:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:11 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:12:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:12:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:13 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:12:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-10-15 15:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-15 15:12:14 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=32, loss_sum=23.092278, avg_loss=0.721634, seen=32, correct=14, accuracy=0.437500
2025-10-15 15:12:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1893MB
2025-10-15 15:12:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:12:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:12:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.677637, avg_loss=0.641941, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:12:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:18 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1920MB allocated=1893MB
2025-10-15 15:12:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:12:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:12:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-10-15 15:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:20 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:12:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:20 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:12:20 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:12:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:12:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.850662, avg_loss=0.740666, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:12:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1910MB
2025-10-15 15:12:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.916948676109314, 'train_avg_loss': 0.9792371690273285, 'train_seen': 4, 'train_correct': 0, 'train_acc': 0.0}}
2025-10-15 15:12:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.850662231445312, 'train_avg_loss': 0.740666389465332, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:12:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.850662231445312, 'train_avg_loss': 0.740666389465332, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:12:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:12:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:12:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:26 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:12:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:12:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-10-15 15:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-10-15 15:12:29 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=75, loss_sum=49.654381, avg_loss=0.662058, seen=75, correct=44, accuracy=0.586667
2025-10-15 15:12:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-15 15:12:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:12:32 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.508814, avg_loss=0.712720, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:12:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1902MB
2025-10-15 15:12:34 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:12:34 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:12:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-10-15 15:12:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:34 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:12:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:34 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:12:34 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:12:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:12:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.678144, avg_loss=0.729884, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:12:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1919MB
2025-10-15 15:12:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.249088168144226, 'train_avg_loss': 0.8122720420360565, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:12:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.678144454956055, 'train_avg_loss': 0.7298840284347534, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:12:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.678144454956055, 'train_avg_loss': 0.7298840284347534, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:12:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:12:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:12:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:40 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:12:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:12:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:41 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:12:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:12:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:12:45 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.926056, avg_loss=0.714630, seen=200, correct=102, accuracy=0.510000
2025-10-15 15:12:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1910MB
2025-10-15 15:12:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:12:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.531067, avg_loss=0.738277, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:12:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1940MB allocated=1910MB
2025-10-15 15:12:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:12:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:12:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-10-15 15:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:12:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:12:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:12:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:12:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.356613, avg_loss=0.709788, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:12:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:12:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:12:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1968MB allocated=1927MB
2025-10-15 15:12:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.505533218383789, 'train_avg_loss': 0.6263833045959473, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:12:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.356613159179688, 'train_avg_loss': 0.7097883224487305, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:12:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #53', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.356613159179688, 'train_avg_loss': 0.7097883224487305, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:12:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:12:57 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:12:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:12:57 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:12:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:12:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:59 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:12:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-10-15 15:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:12:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-10-15 15:13:02 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=193, loss_sum=142.815369, avg_loss=0.739976, seen=193, correct=83, accuracy=0.430052
2025-10-15 15:13:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1919MB
2025-10-15 15:13:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:13:06 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.358807, avg_loss=0.708970, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:13:06 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:08 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1919MB
2025-10-15 15:13:08 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:13:08 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:13:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-10-15 15:13:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:08 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:13:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:08 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:13:08 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:13:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:13:10 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.399004, avg_loss=0.587438, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:13:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:11 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1988MB allocated=1935MB
2025-10-15 15:13:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.019461989402771, 'train_avg_loss': 0.5048654973506927, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:13:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.399003982543945, 'train_avg_loss': 0.5874377489089966, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:13:12 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.399003982543945, 'train_avg_loss': 0.5874377489089966, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:13:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:13:13 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:13:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:14 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:13:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:15 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:13:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:13:18 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=138.395142, avg_loss=0.691976, seen=200, correct=115, accuracy=0.575000
2025-10-15 15:13:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1927MB
2025-10-15 15:13:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:13:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:13:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.341389, avg_loss=0.658535, seen=40, correct=26, accuracy=0.650000
2025-10-15 15:13:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1927MB
2025-10-15 15:13:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.650000
2025-10-15 15:13:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:13:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-10-15 15:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:13:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:13:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:13:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:13:27 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.275296, avg_loss=0.704706, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:13:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1992MB allocated=1944MB
2025-10-15 15:13:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.093060255050659, 'train_avg_loss': 0.7732650637626648, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:13:29 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.275296211242676, 'train_avg_loss': 0.7047060132026672, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:13:29 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #38', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.275296211242676, 'train_avg_loss': 0.7047060132026672, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:13:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:13:31 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:13:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:31 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:13:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:13:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:13:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-10-15 15:13:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-10-15 15:13:33 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=30, loss_sum=20.157001, avg_loss=0.671900, seen=30, correct=17, accuracy=0.566667
2025-10-15 15:13:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:34 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1935MB
2025-10-15 15:13:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:13:36 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.404760, avg_loss=0.685119, seen=40, correct=19, accuracy=0.475000
2025-10-15 15:13:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1960MB allocated=1935MB
2025-10-15 15:13:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-15 15:13:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:13:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-10-15 15:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:13:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:13:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:13:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:13:38 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.943773, avg_loss=0.683986, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:13:38 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:40 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1952MB
2025-10-15 15:13:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.5389139652252197, 'train_avg_loss': 0.6347284913063049, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:13:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.94377326965332, 'train_avg_loss': 0.6839858293533325, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:13:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.94377326965332, 'train_avg_loss': 0.6839858293533325, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:13:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:13:42 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:13:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:42 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:13:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:44 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:13:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-10-15 15:13:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-15 15:13:46 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=69, loss_sum=49.977657, avg_loss=0.724314, seen=69, correct=35, accuracy=0.507246
2025-10-15 15:13:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1944MB
2025-10-15 15:13:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:13:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:13:49 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.632673, avg_loss=0.715817, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:13:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1944MB
2025-10-15 15:13:51 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:13:51 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:13:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-10-15 15:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:13:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:13:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:13:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:13:53 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.647635, avg_loss=0.602977, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:13:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:13:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2010MB allocated=1961MB
2025-10-15 15:13:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.463834285736084, 'train_avg_loss': 0.615958571434021, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:13:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.647634506225586, 'train_avg_loss': 0.6029771566390991, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:13:54 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.647634506225586, 'train_avg_loss': 0.6029771566390991, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:13:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:13:55 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:13:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:13:56 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:13:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:13:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:13:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:13:57 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:14:01 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=135.026886, avg_loss=0.675134, seen=200, correct=122, accuracy=0.610000
2025-10-15 15:14:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1952MB
2025-10-15 15:14:03 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:14:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:14:04 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.518082, avg_loss=0.712952, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:14:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=1980MB allocated=1952MB
2025-10-15 15:14:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:14:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:14:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-10-15 15:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:14:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:14:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:14:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:14:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.580481, avg_loss=0.723780, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:14:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2002MB allocated=1969MB
2025-10-15 15:14:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.1863049268722534, 'train_avg_loss': 0.7965762317180634, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:14:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.580480575561523, 'train_avg_loss': 0.7237800359725952, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:14:09 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.580480575561523, 'train_avg_loss': 0.7237800359725952, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:14:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:14:10 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:14:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:11 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:14:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:12 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:14:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:14:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:14:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=133.114670, avg_loss=0.665573, seen=200, correct=118, accuracy=0.590000
2025-10-15 15:14:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:17 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1961MB
2025-10-15 15:14:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:14:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:14:19 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.823463, avg_loss=0.670587, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:14:19 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:21 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1961MB
2025-10-15 15:14:21 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:14:21 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:14:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-10-15 15:14:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:14:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:14:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:14:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:14:23 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.431643, avg_loss=0.651978, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:14:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2030MB allocated=1977MB
2025-10-15 15:14:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.3125545978546143, 'train_avg_loss': 0.5781386494636536, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:14:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.43164348602295, 'train_avg_loss': 0.6519777178764343, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:14:24 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.43164348602295, 'train_avg_loss': 0.6519777178764343, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:14:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:14:25 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:14:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:26 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:14:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:14:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:27 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:14:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-10-15 15:14:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-10-15 15:14:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=132, loss_sum=93.523071, avg_loss=0.708508, seen=132, correct=69, accuracy=0.522727
2025-10-15 15:14:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-15 15:14:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:14:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:14:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.033909, avg_loss=0.725848, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:14:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2000MB allocated=1969MB
2025-10-15 15:14:35 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:14:35 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:14:35 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-10-15 15:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:35 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:14:35 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:35 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:14:35 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:14:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:14:36 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.964381, avg_loss=0.685274, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:14:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2036MB allocated=1986MB
2025-10-15 15:14:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.050008475780487, 'train_avg_loss': 0.5125021189451218, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:14:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.964381217956543, 'train_avg_loss': 0.6852738261222839, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:14:39 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #49', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.964381217956543, 'train_avg_loss': 0.6852738261222839, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:14:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:14:40 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:14:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:41 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:14:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:14:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:43 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:14:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-15 15:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-15 15:14:44 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=80.005241, avg_loss=0.727320, seen=110, correct=51, accuracy=0.463636
2025-10-15 15:14:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1977MB
2025-10-15 15:14:46 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:14:48 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=31.972443, avg_loss=0.799311, seen=40, correct=14, accuracy=0.350000
2025-10-15 15:14:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1977MB
2025-10-15 15:14:50 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.350000
2025-10-15 15:14:50 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:14:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-10-15 15:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:51 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:14:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:51 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:14:51 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:14:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:14:52 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.894009, avg_loss=0.743376, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:14:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:14:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2058MB allocated=1994MB
2025-10-15 15:14:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.667241096496582, 'train_avg_loss': 0.6668102741241455, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:14:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.89400863647461, 'train_avg_loss': 0.7433755397796631, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:14:55 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.89400863647461, 'train_avg_loss': 0.7433755397796631, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:14:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:14:56 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:14:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:57 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:14:57 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:14:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:57 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:14:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-10-15 15:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:14:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:14:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-10-15 15:14:59 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=83, loss_sum=56.777817, avg_loss=0.684070, seen=83, correct=46, accuracy=0.554217
2025-10-15 15:14:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:14:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1986MB
2025-10-15 15:15:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:15:03 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.512775, avg_loss=0.712819, seen=40, correct=19, accuracy=0.475000
2025-10-15 15:15:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:05 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:06 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1986MB
2025-10-15 15:15:06 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-15 15:15:06 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:15:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-10-15 15:15:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:06 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:15:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:06 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:15:06 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:15:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:15:08 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.871002, avg_loss=0.679438, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:15:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2054MB allocated=2003MB
2025-10-15 15:15:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.6833256483078003, 'train_avg_loss': 0.6708314120769501, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:15:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.871002197265625, 'train_avg_loss': 0.6794376373291016, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:15:10 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.871002197265625, 'train_avg_loss': 0.6794376373291016, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:15:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:15:12 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:15:12 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:12 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:15:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:14 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:15:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-10-15 15:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-10-15 15:15:15 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=54, loss_sum=36.759613, avg_loss=0.680734, seen=54, correct=31, accuracy=0.574074
2025-10-15 15:15:15 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:17 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1994MB
2025-10-15 15:15:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:15:18 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.726652, avg_loss=0.718166, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:15:18 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:20 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2020MB allocated=1994MB
2025-10-15 15:15:20 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:15:20 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:15:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-10-15 15:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:21 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:15:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:21 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:15:21 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:15:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:15:22 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.618029, avg_loss=0.726127, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:15:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2048MB allocated=2011MB
2025-10-15 15:15:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.2508599758148193, 'train_avg_loss': 0.8127149939537048, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:15:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.61802864074707, 'train_avg_loss': 0.7261267900466919, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:15:25 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #36', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.61802864074707, 'train_avg_loss': 0.7261267900466919, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:15:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:15:26 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:15:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:26 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:15:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:15:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:28 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:15:28 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-10-15 15:15:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:28 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-15 15:15:30 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=136, loss_sum=97.132126, avg_loss=0.714207, seen=136, correct=69, accuracy=0.507353
2025-10-15 15:15:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:32 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=2003MB
2025-10-15 15:15:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:15:34 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.606522, avg_loss=0.765163, seen=40, correct=19, accuracy=0.475000
2025-10-15 15:15:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=2003MB
2025-10-15 15:15:37 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-15 15:15:37 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:15:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-10-15 15:15:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:37 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:15:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:37 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:15:37 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:15:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:15:39 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.189466, avg_loss=0.761842, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:15:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2019MB
2025-10-15 15:15:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.7156784534454346, 'train_avg_loss': 0.9289196133613586, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:15:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.189465522766113, 'train_avg_loss': 0.7618415951728821, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:15:41 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.189465522766113, 'train_avg_loss': 0.7618415951728821, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:15:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:15:43 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:15:43 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:43 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:15:43 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:45 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:15:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-10-15 15:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-15 15:15:47 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=134, loss_sum=90.038284, avg_loss=0.671927, seen=134, correct=78, accuracy=0.582090
2025-10-15 15:15:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=2011MB
2025-10-15 15:15:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:15:51 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.304790, avg_loss=0.707620, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:15:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2040MB allocated=2011MB
2025-10-15 15:15:53 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:15:53 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:15:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-10-15 15:15:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:15:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:15:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:15:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:15:55 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.560420, avg_loss=0.660026, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:15:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:15:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:15:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2068MB allocated=2028MB
2025-10-15 15:15:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.262361705303192, 'train_avg_loss': 0.565590426325798, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:15:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.560420036315918, 'train_avg_loss': 0.6600262522697449, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:15:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.560420036315918, 'train_avg_loss': 0.6600262522697449, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:15:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:15:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:15:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:15:59 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:16:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:16:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:00 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:16:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:16:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=146.524353, avg_loss=0.732622, seen=200, correct=99, accuracy=0.495000
2025-10-15 15:16:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2019MB
2025-10-15 15:16:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:16:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.665207, avg_loss=0.766630, seen=40, correct=17, accuracy=0.425000
2025-10-15 15:16:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2019MB
2025-10-15 15:16:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-15 15:16:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:16:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-10-15 15:16:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:11 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:16:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:11 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:16:11 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:16:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.515665, avg_loss=0.719729, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:16:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2094MB allocated=2036MB
2025-10-15 15:16:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.9082298278808594, 'train_avg_loss': 0.9770574569702148, 'train_seen': 4, 'train_correct': 0, 'train_acc': 0.0}}
2025-10-15 15:16:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.515665054321289, 'train_avg_loss': 0.7197290658950806, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:16:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.515665054321289, 'train_avg_loss': 0.7197290658950806, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:16:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:16:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:16:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:16:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:16:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:16:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:16:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=139.306824, avg_loss=0.696534, seen=200, correct=109, accuracy=0.545000
2025-10-15 15:16:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:21 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2028MB
2025-10-15 15:16:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:16:23 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.471138, avg_loss=0.711778, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:16:23 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:25 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2028MB
2025-10-15 15:16:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:16:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:16:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-10-15 15:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:16:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:16:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:16:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:16:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.478301, avg_loss=0.779894, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:16:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2094MB allocated=2044MB
2025-10-15 15:16:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.4143494367599487, 'train_avg_loss': 0.6035873591899872, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:16:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.478301048278809, 'train_avg_loss': 0.7798938155174255, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:16:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.478301048278809, 'train_avg_loss': 0.7798938155174255, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:16:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:16:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:16:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:32 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:16:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:16:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-10-15 15:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-10-15 15:16:36 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=110, loss_sum=78.598465, avg_loss=0.714531, seen=110, correct=53, accuracy=0.481818
2025-10-15 15:16:36 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:38 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2036MB
2025-10-15 15:16:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:16:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:16:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.525284, avg_loss=0.663132, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:16:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2060MB allocated=2036MB
2025-10-15 15:16:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:16:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:16:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-10-15 15:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:16:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:16:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:16:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.987676, avg_loss=0.749230, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:16:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2088MB allocated=2053MB
2025-10-15 15:16:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.4377108216285706, 'train_avg_loss': 0.6094277054071426, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:16:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.987675666809082, 'train_avg_loss': 0.7492297291755676, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:16:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #46', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.987675666809082, 'train_avg_loss': 0.7492297291755676, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:16:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:16:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:16:46 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:46 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:16:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:16:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:16:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-10-15 15:16:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-10-15 15:16:51 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=153, loss_sum=106.083687, avg_loss=0.693357, seen=153, correct=89, accuracy=0.581699
2025-10-15 15:16:51 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:53 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2044MB
2025-10-15 15:16:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:16:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.512886, avg_loss=0.737822, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:16:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:56 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:16:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2044MB
2025-10-15 15:16:57 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:16:57 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:16:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-10-15 15:16:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:16:58 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:16:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:16:58 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:16:58 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:16:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:16:59 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.972397, avg_loss=0.623275, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:16:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2110MB allocated=2061MB
2025-10-15 15:17:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.1345804929733276, 'train_avg_loss': 0.7836451232433319, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:17:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.972396850585938, 'train_avg_loss': 0.6232748031616211, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:17:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.972396850585938, 'train_avg_loss': 0.6232748031616211, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:17:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:17:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:17:03 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:03 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:17:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:17:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-10-15 15:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-15 15:17:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=147, loss_sum=108.658478, avg_loss=0.739173, seen=147, correct=70, accuracy=0.476190
2025-10-15 15:17:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2053MB
2025-10-15 15:17:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:17:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:17:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.626873, avg_loss=0.665672, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:17:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2080MB allocated=2053MB
2025-10-15 15:17:13 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:17:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:17:14 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-10-15 15:17:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:14 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:17:14 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:14 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:17:14 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:17:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:17:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.298653, avg_loss=0.706166, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:17:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2106MB allocated=2070MB
2025-10-15 15:17:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 1.855007529258728, 'train_avg_loss': 0.463751882314682, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:17:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.298652648925781, 'train_avg_loss': 0.7061657905578613, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:17:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #47', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.298652648925781, 'train_avg_loss': 0.7061657905578613, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:17:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:17:17 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:17:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:18 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:17:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:18 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:17:19 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-15 15:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:19 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-15 15:17:22 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=134.410721, avg_loss=0.714951, seen=188, correct=94, accuracy=0.500000
2025-10-15 15:17:22 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:23 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:24 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-15 15:17:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:17:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:17:25 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.266506, avg_loss=0.681663, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:17:25 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2061MB
2025-10-15 15:17:27 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:17:27 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:17:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-10-15 15:17:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:27 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:17:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:27 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:17:27 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:17:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:17:29 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.673462, avg_loss=0.729591, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:17:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2132MB allocated=2078MB
2025-10-15 15:17:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.4557164907455444, 'train_avg_loss': 0.8639291226863861, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:17:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.6734619140625, 'train_avg_loss': 0.7295913696289062, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:17:32 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.6734619140625, 'train_avg_loss': 0.7295913696289062, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:17:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:17:33 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:17:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:33 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:17:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:34 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:17:34 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-10-15 15:17:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:34 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-10-15 15:17:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=160, loss_sum=116.251740, avg_loss=0.726573, seen=160, correct=79, accuracy=0.493750
2025-10-15 15:17:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:38 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2070MB
2025-10-15 15:17:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:17:40 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.530853, avg_loss=0.663271, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:17:40 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:41 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:42 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2100MB allocated=2070MB
2025-10-15 15:17:42 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:17:42 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:17:42 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-10-15 15:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:42 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:17:42 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:42 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:17:42 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:17:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:17:44 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.662010, avg_loss=0.728876, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:17:44 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:45 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:46 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2122MB allocated=2086MB
2025-10-15 15:17:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.6584084033966064, 'train_avg_loss': 0.9146021008491516, 'train_seen': 4, 'train_correct': 0, 'train_acc': 0.0}}
2025-10-15 15:17:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.662010192871094, 'train_avg_loss': 0.7288756370544434, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:17:47 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.662010192871094, 'train_avg_loss': 0.7288756370544434, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:17:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:17:48 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:17:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:48 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:17:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:17:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:49 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:17:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-10-15 15:17:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-10-15 15:17:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=161, loss_sum=109.817490, avg_loss=0.682096, seen=161, correct=89, accuracy=0.552795
2025-10-15 15:17:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:53 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2078MB
2025-10-15 15:17:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:17:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:17:55 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.937248, avg_loss=0.648431, seen=40, correct=23, accuracy=0.575000
2025-10-15 15:17:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:55 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:56 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2078MB
2025-10-15 15:17:56 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.575000
2025-10-15 15:17:56 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:17:56 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-10-15 15:17:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:56 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:17:56 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:17:56 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:17:56 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:17:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:17:57 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.057428, avg_loss=0.691089, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:17:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:17:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:17:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2095MB
2025-10-15 15:17:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.771821141242981, 'train_avg_loss': 0.6929552853107452, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:17:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.057428359985352, 'train_avg_loss': 0.6910892724990845, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:17:59 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.057428359985352, 'train_avg_loss': 0.6910892724990845, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:17:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:18:00 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:18:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:00 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:18:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:02 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:18:02 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-10-15 15:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:02 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-15 15:18:04 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=135, loss_sum=95.279190, avg_loss=0.705772, seen=135, correct=69, accuracy=0.511111
2025-10-15 15:18:04 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:06 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:07 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2086MB
2025-10-15 15:18:07 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:18:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:07 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:18:08 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=24.799343, avg_loss=0.619984, seen=40, correct=29, accuracy=0.725000
2025-10-15 15:18:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2086MB
2025-10-15 15:18:10 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.725000
2025-10-15 15:18:10 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:18:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-10-15 15:18:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:10 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:18:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:10 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:18:10 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:18:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:18:12 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.088306, avg_loss=0.755519, seen=16, correct=5, accuracy=0.312500
2025-10-15 15:18:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:14 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:14 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2164MB allocated=2103MB
2025-10-15 15:18:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.722003936767578, 'train_avg_loss': 0.6805009841918945, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:18:14 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.088306427001953, 'train_avg_loss': 0.7555191516876221, 'train_seen': 16, 'train_correct': 5, 'train_acc': 0.3125}}
2025-10-15 15:18:14 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.088306427001953, 'train_avg_loss': 0.7555191516876221, 'train_seen': 16, 'train_correct': 5, 'train_acc': 0.3125}}
2025-10-15 15:18:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:18:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:18:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:16 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:18:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:18:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:18:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-10-15 15:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-10-15 15:18:21 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=188, loss_sum=131.777496, avg_loss=0.700944, seen=188, correct=98, accuracy=0.521277
2025-10-15 15:18:21 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2095MB
2025-10-15 15:18:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:18:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:18:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.140795, avg_loss=0.678520, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:18:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:25 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2120MB allocated=2095MB
2025-10-15 15:18:25 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:18:25 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:18:25 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-10-15 15:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:25 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:18:25 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:25 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:18:25 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:18:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:18:26 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=9.997985, avg_loss=0.624874, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:18:26 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:27 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:28 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2150MB allocated=2112MB
2025-10-15 15:18:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.8604576587677, 'train_avg_loss': 0.715114414691925, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:18:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 9.997984886169434, 'train_avg_loss': 0.6248740553855896, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:18:28 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #52', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 9.997984886169434, 'train_avg_loss': 0.6248740553855896, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:18:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:18:30 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:18:30 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:30 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:18:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:32 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:18:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-10-15 15:18:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-10-15 15:18:34 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=89, loss_sum=63.427544, avg_loss=0.712669, seen=89, correct=42, accuracy=0.471910
2025-10-15 15:18:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:36 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:37 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-15 15:18:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:18:39 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.842800, avg_loss=0.671070, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:18:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2103MB
2025-10-15 15:18:41 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:18:41 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:18:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-10-15 15:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:41 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:18:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:41 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:18:41 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:18:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:18:43 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.939848, avg_loss=0.683740, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:18:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2120MB
2025-10-15 15:18:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.198756158351898, 'train_avg_loss': 0.7996890395879745, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:18:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.939847946166992, 'train_avg_loss': 0.683740496635437, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:18:45 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #43', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.939847946166992, 'train_avg_loss': 0.683740496635437, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:18:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:18:46 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:18:47 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:47 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:18:47 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:48 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:18:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-15 15:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-15 15:18:48 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=6.615589, avg_loss=0.601417, seen=11, correct=8, accuracy=0.727273
2025-10-15 15:18:48 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:49 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:50 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2112MB
2025-10-15 15:18:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:18:52 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=25.799400, avg_loss=0.644985, seen=40, correct=28, accuracy=0.700000
2025-10-15 15:18:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:52 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:54 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2140MB allocated=2112MB
2025-10-15 15:18:54 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.700000
2025-10-15 15:18:54 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:18:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-10-15 15:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:54 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:18:54 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:18:54 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:18:54 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:18:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:18:56 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.681676, avg_loss=0.730105, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:18:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:18:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:18:58 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2170MB allocated=2128MB
2025-10-15 15:18:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.030479669570923, 'train_avg_loss': 0.7576199173927307, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:18:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.681675910949707, 'train_avg_loss': 0.7301047444343567, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:18:58 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.681675910949707, 'train_avg_loss': 0.7301047444343567, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:18:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:18:59 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:19:00 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:00 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:19:00 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:01 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:19:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-10-15 15:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-10-15 15:19:03 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=72, loss_sum=52.608124, avg_loss=0.730668, seen=72, correct=38, accuracy=0.527778
2025-10-15 15:19:03 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:04 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:05 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2160MB allocated=2120MB
2025-10-15 15:19:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:19:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:19:07 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.212959, avg_loss=0.655324, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:19:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:07 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:09 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2160MB allocated=2120MB
2025-10-15 15:19:09 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:19:09 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:19:09 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-10-15 15:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:09 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:19:09 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:09 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:19:09 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:19:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:19:11 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.966968, avg_loss=0.810435, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:19:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2192MB allocated=2137MB
2025-10-15 15:19:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.375906825065613, 'train_avg_loss': 0.8439767062664032, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:19:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.966967582702637, 'train_avg_loss': 0.8104354739189148, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:19:13 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.966967582702637, 'train_avg_loss': 0.8104354739189148, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:19:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:19:15 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:19:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:15 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:19:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:19:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:17 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:19:17 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-10-15 15:19:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:17 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-10-15 15:19:20 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=119, loss_sum=84.747162, avg_loss=0.712161, seen=119, correct=65, accuracy=0.546218
2025-10-15 15:19:20 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:22 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:22 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2160MB allocated=2128MB
2025-10-15 15:19:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:19:24 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.321014, avg_loss=0.733025, seen=40, correct=17, accuracy=0.425000
2025-10-15 15:19:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:24 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2160MB allocated=2128MB
2025-10-15 15:19:26 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-15 15:19:26 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:19:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-10-15 15:19:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:26 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:19:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:26 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:19:26 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:19:28 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.043338, avg_loss=0.627709, seen=16, correct=10, accuracy=0.625000
2025-10-15 15:19:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2145MB
2025-10-15 15:19:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 1.8685266971588135, 'train_avg_loss': 0.46713167428970337, 'train_seen': 4, 'train_correct': 4, 'train_acc': 1.0}}
2025-10-15 15:19:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.04333782196045, 'train_avg_loss': 0.6277086138725281, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:19:30 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #41', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.04333782196045, 'train_avg_loss': 0.6277086138725281, 'train_seen': 16, 'train_correct': 10, 'train_acc': 0.625}}
2025-10-15 15:19:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:19:32 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:19:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:32 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:19:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:33 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:19:33 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:33 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:19:37 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=147.261993, avg_loss=0.736310, seen=200, correct=102, accuracy=0.510000
2025-10-15 15:19:37 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:39 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:39 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2137MB
2025-10-15 15:19:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:19:41 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=26.417934, avg_loss=0.660448, seen=40, correct=25, accuracy=0.625000
2025-10-15 15:19:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:43 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2137MB
2025-10-15 15:19:43 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.625000
2025-10-15 15:19:43 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:19:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-10-15 15:19:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:44 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:19:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:44 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:19:44 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:19:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:19:45 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.041542, avg_loss=0.690096, seen=16, correct=11, accuracy=0.687500
2025-10-15 15:19:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:47 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2154MB
2025-10-15 15:19:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.5865769386291504, 'train_avg_loss': 0.6466442346572876, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:19:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.041542053222656, 'train_avg_loss': 0.690096378326416, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:19:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.041542053222656, 'train_avg_loss': 0.690096378326416, 'train_seen': 16, 'train_correct': 11, 'train_acc': 0.6875}}
2025-10-15 15:19:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:19:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:19:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:50 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:19:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:19:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:50 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:19:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-10-15 15:19:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-10-15 15:19:52 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=57, loss_sum=38.350479, avg_loss=0.672815, seen=57, correct=30, accuracy=0.526316
2025-10-15 15:19:52 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2145MB
2025-10-15 15:19:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:19:56 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.328568, avg_loss=0.683214, seen=40, correct=22, accuracy=0.550000
2025-10-15 15:19:56 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:19:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:58 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:19:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2145MB
2025-10-15 15:19:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.550000
2025-10-15 15:19:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:19:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-10-15 15:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:19:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:19:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:19:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:19:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:20:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:20:00 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.003523, avg_loss=0.687720, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:20:00 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:01 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:02 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2216MB allocated=2162MB
2025-10-15 15:20:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.4525389671325684, 'train_avg_loss': 0.8631347417831421, 'train_seen': 4, 'train_correct': 0, 'train_acc': 0.0}}
2025-10-15 15:20:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.003522872924805, 'train_avg_loss': 0.6877201795578003, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:20:02 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.003522872924805, 'train_avg_loss': 0.6877201795578003, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:20:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:20:03 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:20:04 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:04 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:20:04 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:20:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:05 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:20:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:20:08 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.449356, avg_loss=0.702247, seen=200, correct=101, accuracy=0.505000
2025-10-15 15:20:08 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:10 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:11 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2154MB
2025-10-15 15:20:11 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:20:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:11 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:20:12 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.140511, avg_loss=0.703513, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:20:12 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:13 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2180MB allocated=2154MB
2025-10-15 15:20:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:20:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:20:15 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-10-15 15:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:15 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:20:15 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:15 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:20:15 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:20:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:20:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.374746, avg_loss=0.710922, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:20:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:19 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2170MB
2025-10-15 15:20:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.2011237144470215, 'train_avg_loss': 0.8002809286117554, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:20:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.374746322631836, 'train_avg_loss': 0.7109216451644897, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:20:19 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.374746322631836, 'train_avg_loss': 0.7109216451644897, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:20:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:20:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:20:21 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:21 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:20:21 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:20:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:20:23 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:23 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:20:27 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=142.569778, avg_loss=0.712849, seen=200, correct=105, accuracy=0.525000
2025-10-15 15:20:27 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:28 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:29 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2162MB
2025-10-15 15:20:29 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:29 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:20:30 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.591335, avg_loss=0.739783, seen=40, correct=19, accuracy=0.475000
2025-10-15 15:20:30 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:30 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:32 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2162MB
2025-10-15 15:20:32 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.475000
2025-10-15 15:20:32 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:20:32 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-10-15 15:20:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:20:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:20:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:20:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:20:34 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.915433, avg_loss=0.682215, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:20:34 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2224MB allocated=2179MB
2025-10-15 15:20:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.6836516857147217, 'train_avg_loss': 0.6709129214286804, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:20:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.915432929992676, 'train_avg_loss': 0.6822145581245422, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:20:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #37', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.915432929992676, 'train_avg_loss': 0.6822145581245422, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:20:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:20:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:20:36 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:36 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:20:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:20:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:38 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:20:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-15 15:20:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-15 15:20:39 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=9.334456, avg_loss=0.848587, seen=11, correct=3, accuracy=0.272727
2025-10-15 15:20:39 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:40 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:41 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2170MB
2025-10-15 15:20:41 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:41 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:20:43 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.417681, avg_loss=0.710442, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:20:43 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2200MB allocated=2170MB
2025-10-15 15:20:45 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:20:45 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:20:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-10-15 15:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:45 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:20:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:45 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:20:45 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:20:47 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.083142, avg_loss=0.755196, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:20:47 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2232MB allocated=2187MB
2025-10-15 15:20:48 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.6643515825271606, 'train_avg_loss': 0.6660878956317902, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:20:48 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.083142280578613, 'train_avg_loss': 0.7551963925361633, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:20:48 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.083142280578613, 'train_avg_loss': 0.7551963925361633, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:20:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:20:49 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:20:50 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:50 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:20:50 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:20:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:51 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:20:51 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-10-15 15:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:51 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-10-15 15:20:53 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=126, loss_sum=89.756088, avg_loss=0.712350, seen=126, correct=67, accuracy=0.531746
2025-10-15 15:20:53 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:54 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:55 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2179MB
2025-10-15 15:20:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:20:57 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.487289, avg_loss=0.687182, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:20:57 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:20:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:20:59 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2179MB
2025-10-15 15:20:59 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:20:59 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:20:59 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-10-15 15:20:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:20:59 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:20:59 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:20:59 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:20:59 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:21:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:21:01 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.301877, avg_loss=0.706367, seen=16, correct=7, accuracy=0.437500
2025-10-15 15:21:01 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:02 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:03 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2196MB
2025-10-15 15:21:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.273934483528137, 'train_avg_loss': 0.8184836208820343, 'train_seen': 4, 'train_correct': 0, 'train_acc': 0.0}}
2025-10-15 15:21:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.30187702178955, 'train_avg_loss': 0.7063673138618469, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:21:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.30187702178955, 'train_avg_loss': 0.7063673138618469, 'train_seen': 16, 'train_correct': 7, 'train_acc': 0.4375}}
2025-10-15 15:21:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:21:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:21:05 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:05 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:21:05 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:21:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:06 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:21:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-10-15 15:21:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-10-15 15:21:07 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=63, loss_sum=43.097050, avg_loss=0.684080, seen=63, correct=35, accuracy=0.555556
2025-10-15 15:21:07 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:09 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:10 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2187MB
2025-10-15 15:21:10 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:21:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:10 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:21:11 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.484692, avg_loss=0.712117, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:21:11 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:13 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2187MB
2025-10-15 15:21:13 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:21:13 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:21:13 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-10-15 15:21:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:13 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:21:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:13 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:21:13 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:21:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:21:14 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.848374, avg_loss=0.678023, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:21:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:16 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:16 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2204MB
2025-10-15 15:21:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.25333833694458, 'train_avg_loss': 0.813334584236145, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:21:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.848374366760254, 'train_avg_loss': 0.6780233979225159, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:21:16 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.848374366760254, 'train_avg_loss': 0.6780233979225159, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:21:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:21:18 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:21:18 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:18 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:21:18 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:20 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:21:20 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-10-15 15:21:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:20 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-10-15 15:21:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=200, loss_sum=140.041473, avg_loss=0.700207, seen=200, correct=108, accuracy=0.540000
2025-10-15 15:21:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:27 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2196MB
2025-10-15 15:21:27 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:21:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:27 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:21:29 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.408100, avg_loss=0.685203, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:21:29 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:31 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:31 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2220MB allocated=2196MB
2025-10-15 15:21:31 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:21:31 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:21:31 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-10-15 15:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:32 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:21:32 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:32 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:21:32 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:21:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:21:33 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.113497, avg_loss=0.757094, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:21:33 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:35 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:36 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2212MB
2025-10-15 15:21:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.4842371940612793, 'train_avg_loss': 0.8710592985153198, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:21:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.113496780395508, 'train_avg_loss': 0.7570935487747192, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:21:36 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #40', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.113496780395508, 'train_avg_loss': 0.7570935487747192, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:21:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:21:38 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:21:38 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:38 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:21:38 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:21:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:40 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:21:40 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-10-15 15:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:40 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-10-15 15:21:42 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=133, loss_sum=92.538139, avg_loss=0.695775, seen=133, correct=72, accuracy=0.541353
2025-10-15 15:21:42 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:44 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:45 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-10-15 15:21:45 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:21:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:45 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:21:46 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=28.477440, avg_loss=0.711936, seen=40, correct=20, accuracy=0.500000
2025-10-15 15:21:46 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:48 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:48 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2204MB
2025-10-15 15:21:48 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.500000
2025-10-15 15:21:48 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:21:49 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-10-15 15:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:49 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:21:49 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:49 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:21:49 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:21:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:21:50 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.202984, avg_loss=0.700186, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:21:50 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:51 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:52 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2264MB allocated=2221MB
2025-10-15 15:21:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.3874670267105103, 'train_avg_loss': 0.8468667566776276, 'train_seen': 4, 'train_correct': 1, 'train_acc': 0.25}}
2025-10-15 15:21:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.202983856201172, 'train_avg_loss': 0.7001864910125732, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:21:52 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #50', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.202983856201172, 'train_avg_loss': 0.7001864910125732, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:21:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:21:53 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:21:53 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:53 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:21:54 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:55 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:21:55 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-10-15 15:21:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:55 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-10-15 15:21:55 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=11, loss_sum=8.641264, avg_loss=0.785569, seen=11, correct=4, accuracy=0.363636
2025-10-15 15:21:55 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:57 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:21:57 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2212MB
2025-10-15 15:21:58 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:21:58 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:21:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:21:59 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.475857, avg_loss=0.686896, seen=40, correct=24, accuracy=0.600000
2025-10-15 15:21:59 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:21:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:00 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:01 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2240MB allocated=2212MB
2025-10-15 15:22:01 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.600000
2025-10-15 15:22:01 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:22:01 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-10-15 15:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:01 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:22:01 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:01 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:22:01 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:22:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:22:02 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=11.490286, avg_loss=0.718143, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:22:02 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:03 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:04 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2229MB
2025-10-15 15:22:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.6288318634033203, 'train_avg_loss': 0.6572079658508301, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:22:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 11.490285873413086, 'train_avg_loss': 0.7181428670883179, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:22:04 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 11.490285873413086, 'train_avg_loss': 0.7181428670883179, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:22:04 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:05 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:22:06 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:06 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:22:06 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:22:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:08 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:22:08 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-10-15 15:22:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:08 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-10-15 15:22:10 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=146, loss_sum=100.784454, avg_loss=0.690304, seen=146, correct=76, accuracy=0.520548
2025-10-15 15:22:10 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:12 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:12 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2221MB
2025-10-15 15:22:12 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:13 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:22:14 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=27.508942, avg_loss=0.687724, seen=40, correct=21, accuracy=0.525000
2025-10-15 15:22:14 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:15 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:15 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2221MB
2025-10-15 15:22:15 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.525000
2025-10-15 15:22:15 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:22:16 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-10-15 15:22:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:16 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:22:16 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:16 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:22:16 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:22:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:22:17 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=10.850318, avg_loss=0.678145, seen=16, correct=9, accuracy=0.562500
2025-10-15 15:22:17 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:19 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:20 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-15 15:22:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.024371862411499, 'train_avg_loss': 0.7560929656028748, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:22:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 10.85031795501709, 'train_avg_loss': 0.6781448721885681, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:22:20 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 10.85031795501709, 'train_avg_loss': 0.6781448721885681, 'train_seen': 16, 'train_correct': 9, 'train_acc': 0.5625}}
2025-10-15 15:22:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:21 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:22:22 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:22 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:22:22 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:23 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:22:24 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-10-15 15:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:24 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-10-15 15:22:24 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=46, loss_sum=29.486298, avg_loss=0.641006, seen=46, correct=28, accuracy=0.608696
2025-10-15 15:22:24 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:26 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:26 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2229MB
2025-10-15 15:22:26 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:26 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:22:28 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=30.168285, avg_loss=0.754207, seen=40, correct=18, accuracy=0.450000
2025-10-15 15:22:28 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:29 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:30 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2260MB allocated=2229MB
2025-10-15 15:22:30 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.450000
2025-10-15 15:22:30 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:22:30 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-10-15 15:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:31 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:22:31 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:31 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:22:31 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:22:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:22:32 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=13.248860, avg_loss=0.828054, seen=16, correct=6, accuracy=0.375000
2025-10-15 15:22:32 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:34 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:35 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2290MB allocated=2246MB
2025-10-15 15:22:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 3.3598926067352295, 'train_avg_loss': 0.8399731516838074, 'train_seen': 4, 'train_correct': 2, 'train_acc': 0.5}}
2025-10-15 15:22:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 13.248860359191895, 'train_avg_loss': 0.8280537724494934, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:22:35 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #48', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 13.248860359191895, 'train_avg_loss': 0.8280537724494934, 'train_seen': 16, 'train_correct': 6, 'train_acc': 0.375}}
2025-10-15 15:22:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:36 (federatedscope.llm.trainer.trainer:402) INFO: [mid-eval] every_n_train_steps=10
2025-10-15 15:22:37 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:37 (federatedscope.llm.trainer.trainer:1592) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-10-15 15:22:37 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:39 (federatedscope.llm.trainer.trainer:1731) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-10-15 15:22:39 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-10-15 15:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:39 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-10-15 15:22:41 (federatedscope.llm.trainer.trainer:1280) INFO: [val|final] total=100, loss_sum=67.950394, avg_loss=0.679504, seen=100, correct=53, accuracy=0.530000
2025-10-15 15:22:41 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:43 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:44 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-15 15:22:44 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-10-15 15:22:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:44 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-10-15 15:22:45 (federatedscope.llm.trainer.trainer:1280) INFO: [test|final] total=40, loss_sum=29.918434, avg_loss=0.747961, seen=40, correct=17, accuracy=0.425000
2025-10-15 15:22:45 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:46 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:47 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2280MB allocated=2238MB
2025-10-15 15:22:47 (federatedscope.llm.trainer.trainer:1790) INFO: [EarlyStop] new best test_acc=0.425000
2025-10-15 15:22:47 (federatedscope.llm.trainer.trainer:443) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-10-15 15:22:48 (federatedscope.llm.trainer.trainer:314) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-10-15 15:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:48 (federatedscope.llm.trainer.trainer:833) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-10-15 15:22:48 (federatedscope.llm.trainer.trainer:854) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=1, num_train_batch_last_epoch=1, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-10-15 15:22:48 (federatedscope.llm.trainer.trainer:875) INFO: [force-step-schedule] epoch=1, num_batches=1, grad_accum_step=2 (=> total micro-batches = 2)
2025-10-15 15:22:48 (federatedscope.llm.trainer.trainer:562) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=1, grad_accum_step=2, will_run_step(loops)=2
2025-10-15 15:22:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-10-15 15:22:49 (federatedscope.llm.trainer.trainer:1280) INFO: [train|final] total=16, loss_sum=12.365685, avg_loss=0.772855, seen=16, correct=8, accuracy=0.500000
2025-10-15 15:22:49 (federatedscope.llm.trainer.trainer:1308) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-10-15 15:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=58720256000 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-10-15 15:22:50 (federatedscope.llm.trainer.trainer:1332) INFO: Accelerator memory has been freed (object preserved).
2025-10-15 15:22:51 (federatedscope.llm.trainer.trainer:1355) INFO: [VRAM] round=0 reserved=2310MB allocated=2254MB
2025-10-15 15:22:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {'train_total': 4, 'train_loss': 2.689105987548828, 'train_avg_loss': 0.672276496887207, 'train_seen': 4, 'train_correct': 3, 'train_acc': 0.75}}
2025-10-15 15:22:51 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Aggregated': True, 'Results_raw': {'train_total': 16, 'train_loss': 12.36568546295166, 'train_avg_loss': 0.7728553414344788, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:22:51 (federatedscope.core.workers.client:247) INFO: {'Role': 'Client #45', 'Round': 0, 'Results_raw': {'train_total': 16, 'train_loss': 12.36568546295166, 'train_avg_loss': 0.7728553414344788, 'train_seen': 16, 'train_correct': 8, 'train_acc': 0.5}}
2025-10-15 15:22:51 (federatedscope.core.workers.server:493) INFO: Server: Training is finished! (skip final evaluation)
2025-10-15 15:22:51 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 16.0383456, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 8752, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:51 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-10-15 15:22:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 16.045703383333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:52 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-10-15 15:22:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 15.978425266666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:52 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-10-15 15:22:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 15.938578783333332, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:52 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-10-15 15:22:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 15.898020650000001, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:52 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-10-15 15:22:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:52 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 15.856655216666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:52 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-10-15 15:22:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 15.815444083333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:53 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-10-15 15:22:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 15.77402795, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:53 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-10-15 15:22:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 15.733009883333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:53 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-10-15 15:22:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 15.691419883333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:53 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-10-15 15:22:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 15.648903033333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:53 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-10-15 15:22:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 15.587821383333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:53 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-10-15 15:22:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 15.546089216666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:53 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-10-15 15:22:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:53 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 15.505545833333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:53 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-10-15 15:22:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 15.463810216666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:54 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-10-15 15:22:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 15.4185339, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:54 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-10-15 15:22:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 15.375800716666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:54 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-10-15 15:22:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 15.333819, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:54 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-10-15 15:22:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 15.291286283333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:54 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-10-15 15:22:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 15.248607866666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:54 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-10-15 15:22:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 15.20608335, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:54 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-10-15 15:22:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:54 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 15.1638208, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:54 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-10-15 15:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 15.106624033333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:55 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-10-15 15:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 15.063837766666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:55 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-10-15 15:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 15.020161716666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:55 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-10-15 15:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 14.974171983333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:55 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-10-15 15:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 14.93204685, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:55 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-10-15 15:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 14.889851966666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:55 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-10-15 15:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 14.846912283333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:55 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-10-15 15:22:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:55 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 14.80388455, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:55 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-10-15 15:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 14.760827, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:56 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-10-15 15:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 14.702166033333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:56 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-10-15 15:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 14.659246099999999, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:56 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-10-15 15:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 14.61597745, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:56 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-10-15 15:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 14.571444133333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:56 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-10-15 15:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 14.524333516666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:56 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-10-15 15:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 14.480765766666668, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:56 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-10-15 15:22:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:56 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 14.437107783333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:56 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-10-15 15:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 14.393521, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:57 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-10-15 15:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 14.349286633333332, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:57 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-10-15 15:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 14.305411866666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:57 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-10-15 15:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 14.245730833333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:57 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-10-15 15:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 14.202358433333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:57 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-10-15 15:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 14.159502699999999, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:57 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-10-15 15:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 14.113843516666666, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:57 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-10-15 15:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:57 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 14.067337783333333, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:57 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-10-15 15:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 14.024048366666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:58 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-10-15 15:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 13.980561450000001, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:58 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-10-15 15:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 13.936472616666666, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:58 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-10-15 15:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 13.893051783333334, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:58 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-10-15 15:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 13.850003333333332, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:58 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-10-15 15:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 13.790072416666668, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:58 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-10-15 15:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 13.747284166666667, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:58 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-10-15 15:22:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2688 skipped=0 missing=291 unexpected=0
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 13.70366145, 'total_model_size': 528965760, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 913080, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 14.902060362654325, 'sys_avg/total_model_size': '495.12M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '875.33K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-10-15 15:22:58 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.69677550606727, 'sys_std/total_model_size': '68.01M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '119.06K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
