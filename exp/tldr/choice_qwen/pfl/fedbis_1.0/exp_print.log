2025-11-11 15:54:16 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/fedbis_1.0/exp_print.log
2025-11-11 15:54:16 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/fedbis_1.0
2025-11-11 15:54:36 (federatedscope.core.data.base_translator:236) INFO: Main process: Completion file found. Skipping generation.
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:266) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=75955, Val=29029, Test=39603, Total=144587
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=503, Val=26, Test=40, Total=569
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=1378, Val=72, Test=40, Total=1490
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=1021, Val=53, Test=40, Total=1114
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=2655, Val=139, Test=40, Total=2834
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=3146, Val=165, Test=40, Total=3351
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=4423, Val=200, Test=40, Total=4663
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=2331, Val=122, Test=40, Total=2493
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=315, Val=16, Test=40, Total=371
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=601, Val=31, Test=40, Total=672
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=647, Val=34, Test=40, Total=721
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2893, Val=152, Test=40, Total=3085
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=18241, Val=200, Test=40, Total=18481
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=5557, Val=200, Test=40, Total=5797
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=3713, Val=195, Test=40, Total=3948
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=8035, Val=200, Test=40, Total=8275
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=2083, Val=109, Test=40, Total=2232
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=9535, Val=200, Test=40, Total=9775
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=3066, Val=161, Test=40, Total=3267
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=176, Val=9, Test=40, Total=225
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=9424, Val=200, Test=40, Total=9664
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=1724, Val=90, Test=40, Total=1854
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=825, Val=43, Test=40, Total=908
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=43, Val=2, Test=40, Total=85
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=9397, Val=200, Test=40, Total=9637
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=5211, Val=200, Test=40, Total=5451
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=4112, Val=200, Test=40, Total=4352
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=6572, Val=200, Test=40, Total=6812
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=8062, Val=200, Test=40, Total=8302
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=2723, Val=143, Test=40, Total=2906
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=229, Val=12, Test=40, Total=281
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=2594, Val=136, Test=40, Total=2770
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=3560, Val=187, Test=40, Total=3787
2025-11-11 15:55:09 (federatedscope.core.data.base_translator:275) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=7379, Val=200, Test=40, Total=7619
2025-11-11 15:55:10 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-11-11 15:55:10 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-11-11 15:55:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=140331326083136 | out_emb=(Linear) num=151646 ptr=140331326083136 | lora_ptr=None
2025-11-11 15:55:16 (federatedscope.llm.model.model_builder:195) INFO: [Warmup-Init] loaded from checkpoints_1.0_250/final_tldr_choice_qwen_fedbis_round_200.ckpt (round=200) | missing=291 unexpected=0
2025-11-11 15:55:16 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-11-11 15:55:16 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:18 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-11-11 15:55:18 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:20 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-11-11 15:55:20 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:22 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-11-11 15:55:22 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:24 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-11-11 15:55:24 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:26 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-11-11 15:55:26 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:27 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-11-11 15:55:28 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:29 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-11-11 15:55:30 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:31 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-11-11 15:55:31 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:33 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-11-11 15:55:33 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:35 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-11-11 15:55:35 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:37 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-11-11 15:55:37 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:39 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-11-11 15:55:39 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:41 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-11-11 15:55:41 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:43 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-11-11 15:55:43 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:45 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-11-11 15:55:45 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:47 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-11-11 15:55:47 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:49 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-11-11 15:55:49 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:50 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-11-11 15:55:51 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:52 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-11-11 15:55:53 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:54 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-11-11 15:55:55 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:56 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-11-11 15:55:56 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:55:58 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-11-11 15:55:58 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:00 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-11-11 15:56:00 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:02 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-11-11 15:56:02 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:04 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-11-11 15:56:04 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:06 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-11-11 15:56:06 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:08 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-11-11 15:56:08 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:10 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-11-11 15:56:10 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:12 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-11-11 15:56:12 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:13 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-11-11 15:56:14 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:15 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-11-11 15:56:15 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:17 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-11-11 15:56:17 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:19 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-11-11 15:56:19 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:21 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-11-11 15:56:21 (federatedscope.llm.trainer.trainer:181) INFO: Choice token IDs: [362, 425]
2025-11-11 15:56:23 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-11-11 15:56:23 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-11-11 15:56:23 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 336.
2025-11-11 15:56:23 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 626.
2025-11-11 15:56:23 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 336. 
Preserved para names in local update: {'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight'}.
2025-11-11 15:56:23 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-11-11 15:56:23 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-11-11 15:56:23 (federatedscope.core.workers.server:984) INFO: Waited all clients join, start now...
2025-11-11 15:56:23 (federatedscope.core.workers.server:993) INFO: ----------- Starting training (Round #0) -------------
2025-11-11 15:56:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:56:28 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:56:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:28 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:56:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:56:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:56:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:56:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=524.849609, avg_loss=2.624248, seen=200, correct=14, accuracy=0.070000
2025-11-11 15:56:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=988MB allocated=983MB
2025-11-11 15:56:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:56:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=109.714500, avg_loss=2.742863, seen=40, correct=4, accuracy=0.100000
2025-11-11 15:56:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=988MB allocated=983MB
2025-11-11 15:56:37 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.100000
2025-11-11 15:56:37 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:56:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2016, total=8062)
2025-11-11 15:56:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:38 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:56:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:38 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:56:38 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1008, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:56:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:56:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:56:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:56:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:56:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:56:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=41.326622, avg_loss=0.206633, seen=200, correct=189, accuracy=0.945000
2025-11-11 15:56:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:56:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:56:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:56:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:56:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.935497, avg_loss=0.198387, seen=40, correct=39, accuracy=0.975000
2025-11-11 15:56:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:56:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:56:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:56:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 15:56:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:56:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:56:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:56:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:56:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:56:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=31.627249, avg_loss=0.158136, seen=200, correct=189, accuracy=0.945000
2025-11-11 15:57:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.592971, avg_loss=0.164824, seen=40, correct=38, accuracy=0.950000
2025-11-11 15:57:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 15:57:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:57:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:57:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:57:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:57:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.396442, avg_loss=0.101982, seen=200, correct=193, accuracy=0.965000
2025-11-11 15:57:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.242800, avg_loss=0.131070, seen=40, correct=38, accuracy=0.950000
2025-11-11 15:57:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:17 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 15:57:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:57:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:57:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:57:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:57:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.210274, avg_loss=0.101051, seen=200, correct=190, accuracy=0.950000
2025-11-11 15:57:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.979408, avg_loss=0.124485, seen=40, correct=38, accuracy=0.950000
2025-11-11 15:57:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:30 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 15:57:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 15:57:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 15:57:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 15:57:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=23.179874, avg_loss=0.115899, seen=200, correct=190, accuracy=0.950000
2025-11-11 15:57:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.603400, avg_loss=0.090085, seen=40, correct=38, accuracy=0.950000
2025-11-11 15:57:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:43 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.975000, curr=0.950000
2025-11-11 15:57:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 15:57:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 15:57:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 15:57:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:57:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:57:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=25.504354, avg_loss=0.127522, seen=200, correct=190, accuracy=0.950000
2025-11-11 15:57:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:57:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:57:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:57:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.246019, avg_loss=0.106150, seen=40, correct=39, accuracy=0.975000
2025-11-11 15:57:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:57:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:57:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:57:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:57:56 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 15:58:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 15:58:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 15:58:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 15:58:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=21.293804, avg_loss=0.106469, seen=200, correct=189, accuracy=0.945000
2025-11-11 15:58:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.724500, avg_loss=0.093113, seen=40, correct=39, accuracy=0.975000
2025-11-11 15:58:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:10 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 15:58:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 15:58:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 15:58:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 15:58:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.697468, avg_loss=0.103487, seen=200, correct=193, accuracy=0.965000
2025-11-11 15:58:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.688350, avg_loss=0.092209, seen=40, correct=38, accuracy=0.950000
2025-11-11 15:58:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:23 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=21.554834, avg_loss=0.107774, seen=200, correct=190, accuracy=0.950000
2025-11-11 15:58:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.862470, avg_loss=0.096562, seen=40, correct=38, accuracy=0.950000
2025-11-11 15:58:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=27.760214, avg_loss=0.138801, seen=200, correct=188, accuracy=0.940000
2025-11-11 15:58:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.832642, avg_loss=0.120816, seen=40, correct=38, accuracy=0.950000
2025-11-11 15:58:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:49 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 15:58:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 15:58:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 15:58:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1092MB allocated=1025MB
2025-11-11 15:58:50 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 15:58:50 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {}}
2025-11-11 15:58:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 15:58:50 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 15:58:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:50 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 15:58:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 15:58:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:58:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=517.341003, avg_loss=2.586705, seen=200, correct=11, accuracy=0.055000
2025-11-11 15:58:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1030MB allocated=1008MB
2025-11-11 15:58:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:58:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=103.025360, avg_loss=2.575634, seen=40, correct=5, accuracy=0.125000
2025-11-11 15:58:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:58:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1030MB allocated=1008MB
2025-11-11 15:58:57 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.125000
2025-11-11 15:58:57 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 15:58:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2356, total=9424)
2025-11-11 15:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:58:58 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 15:58:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:58:58 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 15:58:58 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1178, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 15:59:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 15:59:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 15:59:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 15:59:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:59:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:59:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=39.129848, avg_loss=0.195649, seen=200, correct=192, accuracy=0.960000
2025-11-11 15:59:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:59:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.987113, avg_loss=0.149678, seen=40, correct=39, accuracy=0.975000
2025-11-11 15:59:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:59:11 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 15:59:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 15:59:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 15:59:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 15:59:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:59:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:59:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.476681, avg_loss=0.102383, seen=200, correct=195, accuracy=0.975000
2025-11-11 15:59:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:59:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.381252, avg_loss=0.084531, seen=40, correct=39, accuracy=0.975000
2025-11-11 15:59:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:59:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 15:59:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 15:59:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 15:59:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 15:59:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:59:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:59:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=14.066904, avg_loss=0.070335, seen=200, correct=195, accuracy=0.975000
2025-11-11 15:59:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.471583, avg_loss=0.061790, seen=40, correct=39, accuracy=0.975000
2025-11-11 15:59:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:59:37 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 15:59:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 15:59:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 15:59:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 15:59:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 15:59:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=13.001675, avg_loss=0.065008, seen=200, correct=195, accuracy=0.975000
2025-11-11 15:59:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:59:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 15:59:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 15:59:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 15:59:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.295810, avg_loss=0.057395, seen=40, correct=39, accuracy=0.975000
2025-11-11 15:59:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 15:59:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 15:59:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 15:59:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 15:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 15:59:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:00:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=10.132189, avg_loss=0.050661, seen=200, correct=196, accuracy=0.980000
2025-11-11 16:00:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.537639, avg_loss=0.088441, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:00:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:03 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:00:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:00:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:00:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:00:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:00:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.629177, avg_loss=0.098146, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:00:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.776567, avg_loss=0.044414, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:00:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:00:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:00:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:00:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:00:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:00:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:00:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=17.925323, avg_loss=0.089627, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:00:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.667256, avg_loss=0.016681, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:00:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:30 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:00:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:00:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:00:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:00:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:00:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:00:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=21.064098, avg_loss=0.105320, seen=200, correct=190, accuracy=0.950000
2025-11-11 16:00:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.765107, avg_loss=0.019128, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:00:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:00:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:00:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:00:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:00:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:00:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:00:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=12.700937, avg_loss=0.063505, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:00:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:00:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:00:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:00:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.689535, avg_loss=0.042238, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:00:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:00:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:00:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:00:56 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:01:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:01:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=13.029447, avg_loss=0.065147, seen=200, correct=196, accuracy=0.980000
2025-11-11 16:01:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.514489, avg_loss=0.037862, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:01:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:01:09 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:01:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:01:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:01:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1102MB allocated=1041MB
2025-11-11 16:01:10 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:01:10 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:01:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:01:10 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:01:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:10 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:01:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:01:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:01:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:01:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=18.820623, avg_loss=0.172666, seen=109, correct=104, accuracy=0.954128
2025-11-11 16:01:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1052MB allocated=1025MB
2025-11-11 16:01:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.755770, avg_loss=0.143894, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:01:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1052MB allocated=1025MB
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=521, total=2083)
2025-11-11 16:01:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:01:16 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=261, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:01:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:01:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:01:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:01:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:01:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=23.755508, avg_loss=0.217940, seen=109, correct=102, accuracy=0.935780
2025-11-11 16:01:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:01:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.455469, avg_loss=0.161387, seen=40, correct=36, accuracy=0.900000
2025-11-11 16:01:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:01:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.900000
2025-11-11 16:01:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:01:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:01:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:01:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:01:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:01:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=18.289608, avg_loss=0.167795, seen=109, correct=102, accuracy=0.935780
2025-11-11 16:01:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:01:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.656800, avg_loss=0.066420, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:01:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:01:40 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:01:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:01:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:01:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:01:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:01:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=19.030262, avg_loss=0.174590, seen=109, correct=103, accuracy=0.944954
2025-11-11 16:01:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:01:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:01:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:01:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:01:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.401031, avg_loss=0.110026, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:01:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:01:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:01:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:01:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.950000
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:01:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:02:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=18.736721, avg_loss=0.171897, seen=109, correct=103, accuracy=0.944954
2025-11-11 16:02:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.038908, avg_loss=0.075973, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:02:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.950000
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:02:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=18.876448, avg_loss=0.173178, seen=109, correct=102, accuracy=0.935780
2025-11-11 16:02:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.708192, avg_loss=0.067705, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:02:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:02:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:02:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:02:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:02:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:02:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:02:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=19.593670, avg_loss=0.179758, seen=109, correct=104, accuracy=0.954128
2025-11-11 16:02:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.672507, avg_loss=0.091813, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:02:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.950000
2025-11-11 16:02:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:02:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:02:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:02:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:02:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=14.684608, avg_loss=0.134721, seen=109, correct=105, accuracy=0.963303
2025-11-11 16:02:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.704308, avg_loss=0.067608, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:02:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:39 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:02:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:02:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:02:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:02:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:02:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:02:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=15.436672, avg_loss=0.141621, seen=109, correct=105, accuracy=0.963303
2025-11-11 16:02:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:02:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:02:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:02:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.047863, avg_loss=0.076197, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:02:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:02:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:02:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:02:51 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:02:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:02:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:02:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:02:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:02:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:03:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=15.452215, avg_loss=0.141763, seen=109, correct=103, accuracy=0.944954
2025-11-11 16:03:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.462649, avg_loss=0.061566, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:03:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:03:03 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=1.000000, curr=0.975000
2025-11-11 16:03:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:03:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:03:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:03:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=109)
2025-11-11 16:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-11-11 16:03:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=109, loss_sum=14.280895, avg_loss=0.131017, seen=109, correct=103, accuracy=0.944954
2025-11-11 16:03:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:03:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.323367, avg_loss=0.058084, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:03:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:03:15 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:03:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:03:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1130MB allocated=1058MB
2025-11-11 16:03:16 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:03:16 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:03:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:03:16 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:03:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:16 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:03:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:03:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:03:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=484.233734, avg_loss=2.575711, seen=188, correct=13, accuracy=0.069149
2025-11-11 16:03:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1074MB allocated=1041MB
2025-11-11 16:03:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=89.135498, avg_loss=2.228387, seen=40, correct=6, accuracy=0.150000
2025-11-11 16:03:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1074MB allocated=1041MB
2025-11-11 16:03:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.150000
2025-11-11 16:03:23 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:03:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-11-11 16:03:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:23 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:03:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:23 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:03:23 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:03:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:03:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:03:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:03:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:03:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:03:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=42.569870, avg_loss=0.226435, seen=188, correct=178, accuracy=0.946809
2025-11-11 16:03:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:03:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=10.499212, avg_loss=0.262480, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:03:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:03:36 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:03:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:03:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=28.735540, avg_loss=0.152849, seen=188, correct=175, accuracy=0.930851
2025-11-11 16:03:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:03:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:03:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:03:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.931633, avg_loss=0.223291, seen=40, correct=36, accuracy=0.900000
2025-11-11 16:03:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:03:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:03:49 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.925000, curr=0.900000
2025-11-11 16:03:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:03:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:03:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:03:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:03:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:03:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:03:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=20.519257, avg_loss=0.109145, seen=188, correct=178, accuracy=0.946809
2025-11-11 16:03:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:03:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.697729, avg_loss=0.242443, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:04:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:04:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:04:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:04:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:04:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:04:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=18.624271, avg_loss=0.099065, seen=188, correct=181, accuracy=0.962766
2025-11-11 16:04:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=11.208686, avg_loss=0.280217, seen=40, correct=35, accuracy=0.875000
2025-11-11 16:04:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.925000, curr=0.875000
2025-11-11 16:04:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:04:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:04:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:04:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:04:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=44.318600, avg_loss=0.235737, seen=188, correct=173, accuracy=0.920213
2025-11-11 16:04:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=11.313811, avg_loss=0.282845, seen=40, correct=35, accuracy=0.875000
2025-11-11 16:04:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.925000, curr=0.875000
2025-11-11 16:04:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:04:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:04:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:04:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:04:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:04:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=20.676920, avg_loss=0.109984, seen=188, correct=180, accuracy=0.957447
2025-11-11 16:04:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.299193, avg_loss=0.232480, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:04:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:42 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:04:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:04:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:04:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:04:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:04:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:04:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=29.428255, avg_loss=0.156533, seen=188, correct=177, accuracy=0.941489
2025-11-11 16:04:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:04:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:04:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.847554, avg_loss=0.196189, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:04:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:04:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:04:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:04:55 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:05:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:05:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:05:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:05:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:05:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=27.629421, avg_loss=0.146965, seen=188, correct=174, accuracy=0.925532
2025-11-11 16:05:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:05:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.069045, avg_loss=0.201726, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:05:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:05:08 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:05:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:05:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:05:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:05:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:05:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=19.062527, avg_loss=0.101396, seen=188, correct=182, accuracy=0.968085
2025-11-11 16:05:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.004299, avg_loss=0.200107, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:05:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:05:21 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:05:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:05:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:05:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:05:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-11-11 16:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 16:05:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=188, loss_sum=20.148540, avg_loss=0.107173, seen=188, correct=179, accuracy=0.952128
2025-11-11 16:05:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:05:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.335472, avg_loss=0.183387, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:05:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:05:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:05:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:05:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:05:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1146MB allocated=1075MB
2025-11-11 16:05:35 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:05:35 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:05:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:05:35 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:05:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:35 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:05:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:05:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:05:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:05:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=136.413757, avg_loss=3.172413, seen=43, correct=1, accuracy=0.023256
2025-11-11 16:05:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1094MB allocated=1058MB
2025-11-11 16:05:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=120.539124, avg_loss=3.013478, seen=40, correct=1, accuracy=0.025000
2025-11-11 16:05:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1094MB allocated=1058MB
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.025000
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=207, total=825)
2025-11-11 16:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:05:40 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=104, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:05:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:05:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:05:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:05:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:05:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=4.843786, avg_loss=0.112646, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:05:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:05:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:05:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:05:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.530486, avg_loss=0.163262, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:05:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:05:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:05:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:05:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:05:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:05:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:05:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:05:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:05:58 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=1.088637, avg_loss=0.025317, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:05:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:05:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.558057, avg_loss=0.063951, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:06:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:06:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:06:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:06:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:06:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:06:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=0.499992, avg_loss=0.011628, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:06:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.693421, avg_loss=0.042336, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:06:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:13 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=0.545941, avg_loss=0.012696, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:06:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.635186, avg_loss=0.040880, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:06:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:06:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:06:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:06:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:06:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:06:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:06:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=0.403406, avg_loss=0.009382, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:06:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.685544, avg_loss=0.042139, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:06:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:34 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:06:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:06:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:06:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:06:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:06:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=0.226787, avg_loss=0.005274, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:06:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.215208, avg_loss=0.030380, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:06:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:06:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:06:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:06:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:06:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:06:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=0.655090, avg_loss=0.015235, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:06:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:06:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:06:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:06:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.020547, avg_loss=0.050514, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:06:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:06:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:06:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:06:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:07:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:07:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:07:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:07:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:07:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=0.149947, avg_loss=0.003487, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:07:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:07:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.309965, avg_loss=0.057749, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:07:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:07:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:07:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:07:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:07:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:07:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:07:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=0.636250, avg_loss=0.014797, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:07:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:07:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.153587, avg_loss=0.028840, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:07:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:07:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:07:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:07:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:07:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:07:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:07:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=11
2025-11-11 16:07:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=43, loss_sum=0.539722, avg_loss=0.012552, seen=43, correct=43, accuracy=1.000000
2025-11-11 16:07:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:07:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.416203, avg_loss=0.035405, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:07:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:07:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:07:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:07:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:07:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1168MB allocated=1092MB
2025-11-11 16:07:29 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:07:29 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:07:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:07:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:07:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:07:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=29.316568, avg_loss=0.146583, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:07:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1094MB allocated=1075MB
2025-11-11 16:07:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.139863, avg_loss=0.103497, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:07:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1094MB allocated=1075MB
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2009, total=8035)
2025-11-11 16:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:07:36 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1005, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:07:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:07:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:07:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:07:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:07:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.046041, avg_loss=0.100230, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:07:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:07:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:07:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:07:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.116050, avg_loss=0.077901, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:07:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:07:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:07:49 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:07:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:07:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:07:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:07:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:07:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:07:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:07:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:07:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=22.794390, avg_loss=0.113972, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:07:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.028846, avg_loss=0.150721, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:08:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:02 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.925000
2025-11-11 16:08:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:08:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:08:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:08:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:08:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.112484, avg_loss=0.090562, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:08:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.772299, avg_loss=0.119307, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:08:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.925000
2025-11-11 16:08:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:08:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:08:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:08:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.023352, avg_loss=0.080117, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:08:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.064396, avg_loss=0.076610, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:08:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:29 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.975000, curr=0.950000
2025-11-11 16:08:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:08:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:08:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:08:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:08:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.296577, avg_loss=0.096483, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:08:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.041614, avg_loss=0.051040, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:08:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:42 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:08:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:08:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:08:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:08:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:08:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:08:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=22.926971, avg_loss=0.114635, seen=200, correct=189, accuracy=0.945000
2025-11-11 16:08:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:08:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:08:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.299268, avg_loss=0.132482, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:08:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:08:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:08:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:08:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:08:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.925000
2025-11-11 16:09:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:09:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:09:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:09:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.950989, avg_loss=0.099755, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:09:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.440980, avg_loss=0.086025, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:09:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:09 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.950000
2025-11-11 16:09:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:09:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:09:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:09:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.607670, avg_loss=0.093038, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:09:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.289680, avg_loss=0.057242, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:09:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:22 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:09:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:09:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:09:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:09:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.685884, avg_loss=0.098429, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:09:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.842099, avg_loss=0.096052, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:09:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=1.000000, curr=0.950000
2025-11-11 16:09:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:09:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:09:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:09:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:09:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=17.547699, avg_loss=0.087738, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:09:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.572915, avg_loss=0.064323, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:09:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:48 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=1.000000, curr=0.975000
2025-11-11 16:09:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:09:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:09:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1140MB allocated=1109MB
2025-11-11 16:09:49 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:09:49 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:09:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:09:49 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:09:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:49 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:09:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:09:51 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=6.665683, avg_loss=0.196049, seen=34, correct=32, accuracy=0.941176
2025-11-11 16:09:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1114MB allocated=1092MB
2025-11-11 16:09:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:09:53 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.613980, avg_loss=0.140350, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:09:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:09:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1114MB allocated=1092MB
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=162, total=647)
2025-11-11 16:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:09:54 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=81, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:10:01 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:10:01 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:10:01 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:10:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:10:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:10:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=2.976916, avg_loss=0.087556, seen=34, correct=32, accuracy=0.941176
2025-11-11 16:10:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:04 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.990359, avg_loss=0.149759, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:10:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:05 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:10:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:10:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:10:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:10:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:10:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:10:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=4.085147, avg_loss=0.120151, seen=34, correct=32, accuracy=0.941176
2025-11-11 16:10:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.550648, avg_loss=0.038766, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:10:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:10:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:10:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:10:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:10:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:10:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:10:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=2.155132, avg_loss=0.063386, seen=34, correct=33, accuracy=0.970588
2025-11-11 16:10:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.424924, avg_loss=0.110623, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:10:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:27 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=3.749074, avg_loss=0.110267, seen=34, correct=32, accuracy=0.941176
2025-11-11 16:10:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.251324, avg_loss=0.056283, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:10:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:38 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:10:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:10:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:10:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:10:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:10:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:10:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=2.144323, avg_loss=0.063068, seen=34, correct=33, accuracy=0.970588
2025-11-11 16:10:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.187843, avg_loss=0.129696, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:10:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:48 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:10:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:10:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:10:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:10:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:10:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:10:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=3.051352, avg_loss=0.089746, seen=34, correct=33, accuracy=0.970588
2025-11-11 16:10:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:10:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:10:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:10:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.687699, avg_loss=0.067192, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:10:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:10:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:10:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:10:59 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 16:11:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:11:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:11:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:11:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:11:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:11:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=2.212021, avg_loss=0.065059, seen=34, correct=33, accuracy=0.970588
2025-11-11 16:11:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.824433, avg_loss=0.120611, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:11:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:10 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 16:11:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:11:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:11:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:11:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:11:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:11:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=4.903564, avg_loss=0.144222, seen=34, correct=32, accuracy=0.941176
2025-11-11 16:11:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.955236, avg_loss=0.023881, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:11:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:21 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:11:27 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:11:27 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:11:27 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:11:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:11:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=3.320588, avg_loss=0.097664, seen=34, correct=33, accuracy=0.970588
2025-11-11 16:11:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.131238, avg_loss=0.153281, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:11:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:31 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:11:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:11:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:11:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:11:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=34)
2025-11-11 16:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-11-11 16:11:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=34, loss_sum=4.571417, avg_loss=0.134453, seen=34, correct=33, accuracy=0.970588
2025-11-11 16:11:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.655743, avg_loss=0.016394, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:11:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1186MB allocated=1125MB
2025-11-11 16:11:43 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:11:43 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:11:43 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:11:43 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:11:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:44 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:11:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:11:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:11:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=13.879372, avg_loss=0.192769, seen=72, correct=66, accuracy=0.916667
2025-11-11 16:11:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1134MB allocated=1109MB
2025-11-11 16:11:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.617602, avg_loss=0.090440, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:11:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1134MB allocated=1109MB
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=345, total=1378)
2025-11-11 16:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:11:49 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=173, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:11:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:11:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:11:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:11:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:11:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:11:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=14.269526, avg_loss=0.198188, seen=72, correct=67, accuracy=0.930556
2025-11-11 16:11:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:11:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:11:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:11:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:11:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:11:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.181920, avg_loss=0.029548, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:11:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:11:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:11:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:00 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:12:07 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:12:07 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:12:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:12:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:12:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:12:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=12.688044, avg_loss=0.176223, seen=72, correct=65, accuracy=0.902778
2025-11-11 16:12:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.763495, avg_loss=0.019087, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:12:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:11 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:12:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:12:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:12:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:12:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:12:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=24.964136, avg_loss=0.346724, seen=72, correct=65, accuracy=0.902778
2025-11-11 16:12:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.381257, avg_loss=0.109531, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:12:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:22 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:12:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:12:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:12:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:12:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:12:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:12:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=14.651544, avg_loss=0.203494, seen=72, correct=65, accuracy=0.902778
2025-11-11 16:12:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.036265, avg_loss=0.025907, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:12:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:33 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:12:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:12:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:12:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:12:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:12:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=12.602459, avg_loss=0.175034, seen=72, correct=65, accuracy=0.902778
2025-11-11 16:12:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.930106, avg_loss=0.073253, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:12:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:44 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:12:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:12:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:12:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:12:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:12:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:12:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=11.904083, avg_loss=0.165334, seen=72, correct=66, accuracy=0.916667
2025-11-11 16:12:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:12:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:12:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.302525, avg_loss=0.057563, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:12:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:12:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:12:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:12:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:12:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:13:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:13:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:13:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:13:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:13:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=14.079567, avg_loss=0.195550, seen=72, correct=67, accuracy=0.930556
2025-11-11 16:13:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.684424, avg_loss=0.067111, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:13:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:07 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:13:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:13:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:13:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:13:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:13:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:13:15 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=9.137807, avg_loss=0.126914, seen=72, correct=68, accuracy=0.944444
2025-11-11 16:13:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:17 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.320305, avg_loss=0.058008, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:13:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:18 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=1.000000, curr=0.975000
2025-11-11 16:13:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:13:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:13:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:13:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:13:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=13.188511, avg_loss=0.183174, seen=72, correct=67, accuracy=0.930556
2025-11-11 16:13:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.237088, avg_loss=0.105927, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:13:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:29 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=1.000000, curr=0.975000
2025-11-11 16:13:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:13:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:13:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:13:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-11-11 16:13:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-11-11 16:13:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=72, loss_sum=8.847494, avg_loss=0.122882, seen=72, correct=68, accuracy=0.944444
2025-11-11 16:13:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.849936, avg_loss=0.046248, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:13:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:40 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=1.000000, curr=0.975000
2025-11-11 16:13:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:13:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1204MB allocated=1142MB
2025-11-11 16:13:41 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:13:41 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:13:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:13:41 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:13:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:41 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:13:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:13:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:13:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=30.827791, avg_loss=0.202814, seen=152, correct=142, accuracy=0.934211
2025-11-11 16:13:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1154MB allocated=1125MB
2025-11-11 16:13:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:13:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.731010, avg_loss=0.193275, seen=40, correct=35, accuracy=0.875000
2025-11-11 16:13:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1154MB allocated=1125MB
2025-11-11 16:13:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.875000
2025-11-11 16:13:48 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:13:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=724, total=2893)
2025-11-11 16:13:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:48 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:13:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:48 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:13:48 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=362, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:13:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:13:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:13:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:13:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:13:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:13:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:13:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=29.236971, avg_loss=0.192348, seen=152, correct=144, accuracy=0.947368
2025-11-11 16:13:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:13:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:13:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:13:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:13:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.281082, avg_loss=0.107027, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:14:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:01 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:14:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:14:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:14:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:14:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:14:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=24.259438, avg_loss=0.159602, seen=152, correct=145, accuracy=0.953947
2025-11-11 16:14:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.927467, avg_loss=0.148187, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:14:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:13 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:14:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:14:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:14:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:14:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:14:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=20.620878, avg_loss=0.135664, seen=152, correct=145, accuracy=0.953947
2025-11-11 16:14:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.280902, avg_loss=0.057023, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:14:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:26 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:14:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:14:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:14:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:14:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:14:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:14:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=19.884079, avg_loss=0.130816, seen=152, correct=144, accuracy=0.947368
2025-11-11 16:14:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.341728, avg_loss=0.083543, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:14:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:38 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:14:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:14:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:14:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:14:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:14:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:14:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=23.463587, avg_loss=0.154366, seen=152, correct=144, accuracy=0.947368
2025-11-11 16:14:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:14:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:14:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:14:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.288884, avg_loss=0.132222, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:14:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:14:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:14:51 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:14:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:15:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=20.476648, avg_loss=0.134715, seen=152, correct=146, accuracy=0.960526
2025-11-11 16:15:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.663848, avg_loss=0.091596, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:15:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 16:15:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:15:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:15:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:15:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:15:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=23.045425, avg_loss=0.151615, seen=152, correct=143, accuracy=0.940789
2025-11-11 16:15:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.945596, avg_loss=0.148640, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:15:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.925000
2025-11-11 16:15:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:15:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:15:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:15:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:15:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=17.925510, avg_loss=0.117931, seen=152, correct=144, accuracy=0.947368
2025-11-11 16:15:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.962012, avg_loss=0.074050, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:15:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:29 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:15:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:15:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:15:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:15:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:15:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:15:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=17.901184, avg_loss=0.117771, seen=152, correct=145, accuracy=0.953947
2025-11-11 16:15:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.889043, avg_loss=0.072226, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:15:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:41 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:15:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:15:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:15:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:15:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=38, total=152)
2025-11-11 16:15:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=38
2025-11-11 16:15:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=152, loss_sum=19.997942, avg_loss=0.131565, seen=152, correct=144, accuracy=0.947368
2025-11-11 16:15:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:15:53 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.741098, avg_loss=0.118527, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:15:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:54 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:15:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:15:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:15:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1226MB allocated=1159MB
2025-11-11 16:15:55 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:15:55 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:15:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:15:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:15:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:15:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:15:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:15:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:15:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:15:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=528.296875, avg_loss=2.641484, seen=200, correct=9, accuracy=0.045000
2025-11-11 16:15:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:15:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1174MB allocated=1142MB
2025-11-11 16:16:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=98.515198, avg_loss=2.462880, seen=40, correct=3, accuracy=0.075000
2025-11-11 16:16:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1174MB allocated=1142MB
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.075000
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1643, total=6572)
2025-11-11 16:16:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:16:02 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=822, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:16:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:16:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:16:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:16:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:16:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:16:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=37.379402, avg_loss=0.186897, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:16:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:16:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.907310, avg_loss=0.197683, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:16:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:16:15 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:16:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:16:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:16:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:16:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:16:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.409092, avg_loss=0.097045, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:16:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:16:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.822692, avg_loss=0.145567, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:16:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:16:28 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:16:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:16:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=15.385975, avg_loss=0.076930, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:16:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:16:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.690501, avg_loss=0.142263, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:16:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:16:42 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:16:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:16:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:16:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:16:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:16:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:16:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=14.296258, avg_loss=0.071481, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:16:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:16:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:16:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:16:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.473165, avg_loss=0.161829, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:16:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:16:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:16:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:16:55 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:17:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:17:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.057617, avg_loss=0.080288, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:17:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:17:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.330974, avg_loss=0.183274, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:17:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:17:08 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:17:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:17:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.891733, avg_loss=0.094459, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:17:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:17:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.732469, avg_loss=0.143312, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:17:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:17:22 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:17:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:17:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:17:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:17:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:17:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:17:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=21.361660, avg_loss=0.106808, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:17:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:17:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.151350, avg_loss=0.228784, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:17:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:17:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 16:17:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:17:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:17:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:17:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:17:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.156664, avg_loss=0.080783, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:17:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:17:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:17:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:17:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.161911, avg_loss=0.129048, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:17:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:17:48 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.950000, curr=0.925000
2025-11-11 16:17:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:17:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:17:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:17:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:17:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:17:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:17:58 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=15.107927, avg_loss=0.075540, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:17:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:17:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:17:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:17:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:18:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.335405, avg_loss=0.158385, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:18:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:18:01 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.950000, curr=0.925000
2025-11-11 16:18:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:18:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:18:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:18:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:18:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=15.835156, avg_loss=0.079176, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:18:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:18:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.716911, avg_loss=0.217923, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:18:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:18:14 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.950000, curr=0.925000
2025-11-11 16:18:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:18:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:18:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1244MB allocated=1176MB
2025-11-11 16:18:15 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:18:15 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:18:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:18:15 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:18:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:15 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:18:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:18:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:18:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:18:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=555.130188, avg_loss=2.775651, seen=200, correct=15, accuracy=0.075000
2025-11-11 16:18:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1194MB allocated=1159MB
2025-11-11 16:18:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=108.584351, avg_loss=2.714609, seen=40, correct=2, accuracy=0.050000
2025-11-11 16:18:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1194MB allocated=1159MB
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.050000
2025-11-11 16:18:22 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:18:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1303, total=5211)
2025-11-11 16:18:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:23 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:18:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:23 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:18:23 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=652, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:18:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:18:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:18:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:18:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:18:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:18:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=37.925613, avg_loss=0.189628, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:18:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:18:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.949255, avg_loss=0.148731, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:18:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:18:36 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:18:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.357178, avg_loss=0.096786, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:18:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.987574, avg_loss=0.074689, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:18:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:18:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:18:49 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:18:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:18:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:18:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:18:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:18:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:18:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:19:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.260717, avg_loss=0.081304, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:19:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.682257, avg_loss=0.092056, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:19:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:03 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:19:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:19:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:19:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:19:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:19:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:19:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=15.828332, avg_loss=0.079142, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:19:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.018012, avg_loss=0.050450, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:19:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:19:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:19:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:19:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:19:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:19:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:19:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=14.568490, avg_loss=0.072842, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:19:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.043318, avg_loss=0.026083, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:19:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:29 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:19:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:19:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:19:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:19:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:19:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:19:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.808792, avg_loss=0.094044, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:19:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.802197, avg_loss=0.095055, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:19:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:42 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.950000
2025-11-11 16:19:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:19:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:19:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:19:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:19:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=13.863801, avg_loss=0.069319, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:19:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:19:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:19:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.328081, avg_loss=0.083202, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:19:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:19:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:20:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:20:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:20:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:20:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:20:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:20:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=12.756516, avg_loss=0.063783, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:20:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:20:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.556571, avg_loss=0.063914, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:20:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:20:09 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:20:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=17.328556, avg_loss=0.086643, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:20:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:20:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.812680, avg_loss=0.020317, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:20:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:20:22 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:20:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:20:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:20:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:20:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:20:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.099756, avg_loss=0.100499, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:20:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:20:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.774601, avg_loss=0.119365, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:20:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.950000
2025-11-11 16:20:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:20:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:20:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1272MB allocated=1193MB
2025-11-11 16:20:36 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:20:36 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:20:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:20:36 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:20:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:36 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:20:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:20:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:20:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:20:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=525.340332, avg_loss=2.626702, seen=200, correct=9, accuracy=0.045000
2025-11-11 16:20:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1194MB allocated=1176MB
2025-11-11 16:20:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=107.905067, avg_loss=2.697627, seen=40, correct=2, accuracy=0.050000
2025-11-11 16:20:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1194MB allocated=1176MB
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.050000
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1845, total=7379)
2025-11-11 16:20:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:20:43 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=923, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:20:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:20:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:20:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:20:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:20:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:20:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=33.080734, avg_loss=0.165404, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:20:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:20:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:20:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:20:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.330969, avg_loss=0.183274, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:20:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:20:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:20:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:20:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:20:57 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:21:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:21:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:21:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:21:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:21:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.224203, avg_loss=0.081121, seen=200, correct=196, accuracy=0.980000
2025-11-11 16:21:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:21:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:21:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:21:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.224361, avg_loss=0.105609, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:21:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:21:10 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:21:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:21:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:21:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:21:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:21:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=12.867214, avg_loss=0.064336, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:21:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:21:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:21:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.755655, avg_loss=0.118891, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:21:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:21:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:21:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:21:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:21:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:21:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:21:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=12.697542, avg_loss=0.063488, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:21:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:21:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:21:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:21:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.198232, avg_loss=0.154956, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:21:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:21:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 16:21:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:21:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:21:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:21:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:21:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=13.605006, avg_loss=0.068025, seen=200, correct=196, accuracy=0.980000
2025-11-11 16:21:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:21:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:21:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:21:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:21:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.398678, avg_loss=0.109967, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:21:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:21:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:21:50 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:21:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:21:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:21:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:21:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:21:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.812939, avg_loss=0.094065, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:22:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.641960, avg_loss=0.141049, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:22:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:03 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:22:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:22:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:22:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:22:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:22:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.808229, avg_loss=0.099041, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:22:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.047945, avg_loss=0.201199, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:22:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:22:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:22:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:22:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:22:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:22:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=10.149458, avg_loss=0.050747, seen=200, correct=196, accuracy=0.980000
2025-11-11 16:22:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.440114, avg_loss=0.086003, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:22:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:29 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 16:22:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:22:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:22:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:22:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:22:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.334499, avg_loss=0.096672, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:22:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.113173, avg_loss=0.102829, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:22:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:43 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 16:22:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:22:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:22:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:22:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:22:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=11.055228, avg_loss=0.055276, seen=200, correct=197, accuracy=0.985000
2025-11-11 16:22:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:22:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.772676, avg_loss=0.094317, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:22:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:22:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:56 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:22:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:22:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:22:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1264MB allocated=1209MB
2025-11-11 16:22:57 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:22:57 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:22:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:22:57 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:22:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:22:57 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:22:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:22:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:22:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:22:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:23:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=347.254639, avg_loss=2.553343, seen=136, correct=13, accuracy=0.095588
2025-11-11 16:23:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1214MB allocated=1193MB
2025-11-11 16:23:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=98.139389, avg_loss=2.453485, seen=40, correct=4, accuracy=0.100000
2025-11-11 16:23:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1214MB allocated=1193MB
2025-11-11 16:23:03 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.100000
2025-11-11 16:23:03 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:23:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=649, total=2594)
2025-11-11 16:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:04 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:23:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:04 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:23:04 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=325, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:23:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:23:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:23:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:23:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:23:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:23:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=33.086945, avg_loss=0.243286, seen=136, correct=125, accuracy=0.919118
2025-11-11 16:23:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:23:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.019046, avg_loss=0.175476, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:23:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:23:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:23:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:23:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:23:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:23:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:23:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=18.356831, avg_loss=0.134977, seen=136, correct=128, accuracy=0.941176
2025-11-11 16:23:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.382780, avg_loss=0.084570, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:23:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:23:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:23:35 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:23:35 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:23:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:23:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:23:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=16.759249, avg_loss=0.123230, seen=136, correct=130, accuracy=0.955882
2025-11-11 16:23:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:23:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.617584, avg_loss=0.065440, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:23:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:23:40 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 16:23:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:23:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:23:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:23:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:23:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=15.806164, avg_loss=0.116222, seen=136, correct=129, accuracy=0.948529
2025-11-11 16:23:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:23:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:23:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:23:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.655193, avg_loss=0.116380, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:23:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:23:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:23:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:23:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 16:23:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:23:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:23:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:23:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:23:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:24:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=16.236057, avg_loss=0.119383, seen=136, correct=130, accuracy=0.955882
2025-11-11 16:24:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.775875, avg_loss=0.144397, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:24:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.975000, curr=0.950000
2025-11-11 16:24:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:24:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:24:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:24:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:24:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=15.198812, avg_loss=0.111756, seen=136, correct=130, accuracy=0.955882
2025-11-11 16:24:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.387559, avg_loss=0.084689, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:24:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.975000, curr=0.950000
2025-11-11 16:24:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:24:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:24:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:24:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:24:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:24:25 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=14.403902, avg_loss=0.105911, seen=136, correct=128, accuracy=0.941176
2025-11-11 16:24:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.369546, avg_loss=0.084239, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:24:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:28 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.975000, curr=0.950000
2025-11-11 16:24:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:24:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:24:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:24:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:24:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:24:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=14.447020, avg_loss=0.106228, seen=136, correct=130, accuracy=0.955882
2025-11-11 16:24:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.732266, avg_loss=0.068307, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:24:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:40 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:24:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:24:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:24:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:24:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:24:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:24:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=27.090168, avg_loss=0.199192, seen=136, correct=125, accuracy=0.919118
2025-11-11 16:24:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:24:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:24:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.338375, avg_loss=0.058459, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:24:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:24:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:24:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:24:52 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:24:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:24:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:24:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:24:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-11-11 16:24:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:24:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-11-11 16:25:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=136, loss_sum=16.665766, avg_loss=0.122542, seen=136, correct=129, accuracy=0.948529
2025-11-11 16:25:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:25:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.680222, avg_loss=0.142006, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:25:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:25:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:25:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:25:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1262MB allocated=1226MB
2025-11-11 16:25:05 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:25:05 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:25:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:25:05 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:25:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:05 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:25:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:25:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:25:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=522.046997, avg_loss=2.610235, seen=200, correct=9, accuracy=0.045000
2025-11-11 16:25:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1234MB allocated=1209MB
2025-11-11 16:25:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:11 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=111.365852, avg_loss=2.784146, seen=40, correct=3, accuracy=0.075000
2025-11-11 16:25:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1234MB allocated=1209MB
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.075000
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1028, total=4112)
2025-11-11 16:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:25:12 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=514, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:25:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:25:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:25:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:25:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:25:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=36.625149, avg_loss=0.183126, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:25:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:25:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.338373, avg_loss=0.133459, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:25:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:25:25 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:25:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:25:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:25:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:25:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:25:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=25.949121, avg_loss=0.129746, seen=200, correct=190, accuracy=0.950000
2025-11-11 16:25:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:25:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.748317, avg_loss=0.043708, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:25:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:25:38 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:25:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:25:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:25:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:25:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:25:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.774769, avg_loss=0.083874, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:25:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:25:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:25:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:25:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:25:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.977526, avg_loss=0.049438, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:25:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:25:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:25:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:25:51 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:25:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:25:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:25:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:25:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:25:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:25:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=21.992933, avg_loss=0.109965, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:26:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:04 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.182317, avg_loss=0.029558, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:26:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:04 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:26:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:26:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:26:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:26:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.895367, avg_loss=0.104477, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:26:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.964501, avg_loss=0.024113, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:26:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:26:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:26:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:26:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:26:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=15.239969, avg_loss=0.076200, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:26:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.367995, avg_loss=0.059200, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:26:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:30 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:26:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:26:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:26:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:26:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=15.599882, avg_loss=0.077999, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:26:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.832863, avg_loss=0.020822, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:26:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:26:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:26:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:26:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:26:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:26:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:26:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=31.331783, avg_loss=0.156659, seen=200, correct=187, accuracy=0.935000
2025-11-11 16:26:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:26:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:26:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:26:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.420189, avg_loss=0.010505, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:26:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:26:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:26:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:26:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:26:56 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:27:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:27:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:27:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:27:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:27:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.320812, avg_loss=0.091604, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:27:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:27:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.719627, avg_loss=0.017991, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:27:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:27:09 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:27:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:27:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:27:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:27:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:27:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.865429, avg_loss=0.084327, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:27:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:27:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.283967, avg_loss=0.032099, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:27:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:27:22 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:27:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:27:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1310MB allocated=1243MB
2025-11-11 16:27:23 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:27:23 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:27:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:27:23 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:27:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:23 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:27:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:27:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:27:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:27:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=388.071198, avg_loss=2.713785, seen=143, correct=8, accuracy=0.055944
2025-11-11 16:27:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1254MB allocated=1226MB
2025-11-11 16:27:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=100.085861, avg_loss=2.502147, seen=40, correct=6, accuracy=0.150000
2025-11-11 16:27:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1254MB allocated=1226MB
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.150000
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=681, total=2723)
2025-11-11 16:27:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:27:30 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=341, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=21.992439, avg_loss=0.153793, seen=143, correct=141, accuracy=0.986014
2025-11-11 16:27:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:27:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.482258, avg_loss=0.187056, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:27:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:27:42 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:27:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:27:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:27:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:27:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:27:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=10.801908, avg_loss=0.075538, seen=143, correct=139, accuracy=0.972028
2025-11-11 16:27:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:27:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:27:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:27:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:27:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.639661, avg_loss=0.090992, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:27:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:27:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:27:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:27:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:27:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:28:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:28:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:28:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:28:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:28:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:28:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=7.015815, avg_loss=0.049062, seen=143, correct=141, accuracy=0.986014
2025-11-11 16:28:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.822234, avg_loss=0.095556, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:28:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:07 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 16:28:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:28:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:28:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:28:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:28:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:28:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=5.688337, avg_loss=0.039779, seen=143, correct=140, accuracy=0.979021
2025-11-11 16:28:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.988608, avg_loss=0.099715, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:28:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:20 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 16:28:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:28:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:28:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:28:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:28:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:28:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=3.411949, avg_loss=0.023860, seen=143, correct=142, accuracy=0.993007
2025-11-11 16:28:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.116793, avg_loss=0.102920, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:28:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:32 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:28:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=4.019792, avg_loss=0.028110, seen=143, correct=143, accuracy=1.000000
2025-11-11 16:28:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.650382, avg_loss=0.091260, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:28:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:28:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:28:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:28:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:28:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:28:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=7.905340, avg_loss=0.055282, seen=143, correct=140, accuracy=0.979021
2025-11-11 16:28:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:28:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:28:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:28:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.924687, avg_loss=0.073117, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:28:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:28:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:28:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:28:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:28:57 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.925000
2025-11-11 16:29:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:29:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:29:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:29:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:29:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:29:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=5.754359, avg_loss=0.040240, seen=143, correct=141, accuracy=0.986014
2025-11-11 16:29:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:29:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.414926, avg_loss=0.085373, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:29:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:29:09 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:29:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:29:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:29:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:29:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:29:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:29:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=4.978087, avg_loss=0.034812, seen=143, correct=141, accuracy=0.986014
2025-11-11 16:29:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:29:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.661904, avg_loss=0.091548, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:29:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:29:22 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:29:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:29:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:29:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:29:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=36, total=143)
2025-11-11 16:29:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=36
2025-11-11 16:29:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=143, loss_sum=6.243436, avg_loss=0.043660, seen=143, correct=140, accuracy=0.979021
2025-11-11 16:29:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:29:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.516041, avg_loss=0.087901, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:29:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:29:34 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:29:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:29:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:29:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1330MB allocated=1260MB
2025-11-11 16:29:35 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:29:35 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:29:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:29:35 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:29:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:35 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:29:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:29:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:29:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:29:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=1.233882, avg_loss=0.077118, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:29:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1274MB allocated=1243MB
2025-11-11 16:29:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.966716, avg_loss=0.174168, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:29:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1274MB allocated=1243MB
2025-11-11 16:29:40 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:29:40 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:29:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=79, total=315)
2025-11-11 16:29:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:40 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:29:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:40 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:29:40 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=40, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:29:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:29:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:29:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:29:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:29:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.321256, avg_loss=0.020078, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:29:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:29:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:29:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.389456, avg_loss=0.109736, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:29:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:29:50 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:29:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:29:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.205961, avg_loss=0.012873, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:29:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:29:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:29:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:29:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:29:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:29:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.354817, avg_loss=0.058870, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:30:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:01 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:30:07 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:30:07 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:30:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:30:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:30:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:30:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.184889, avg_loss=0.011556, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:30:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.683160, avg_loss=0.117079, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:30:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:11 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:30:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:30:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:30:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:30:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:30:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:30:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.070135, avg_loss=0.004383, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:30:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.523318, avg_loss=0.113083, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:30:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:21 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 16:30:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:30:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:30:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:30:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:30:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:30:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.068872, avg_loss=0.004305, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:30:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.320182, avg_loss=0.033005, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:30:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:31 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:30:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:30:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:30:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:30:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:30:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:30:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.061169, avg_loss=0.003823, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:30:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.357224, avg_loss=0.158931, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:30:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:42 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.950000
2025-11-11 16:30:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:30:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:30:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:30:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:30:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:30:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.900496, avg_loss=0.056281, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:30:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:30:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:30:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.959271, avg_loss=0.123982, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:30:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:30:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:30:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.950000
2025-11-11 16:30:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:30:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:30:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:30:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:30:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:30:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:30:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.055665, avg_loss=0.003479, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:30:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:31:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.742319, avg_loss=0.093558, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:31:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:31:02 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.950000
2025-11-11 16:31:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:31:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:31:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:31:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:31:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:31:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.047828, avg_loss=0.002989, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:31:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:31:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.831948, avg_loss=0.145799, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:31:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:31:13 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=1.000000, curr=0.950000
2025-11-11 16:31:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:31:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:31:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:31:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=16)
2025-11-11 16:31:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-11-11 16:31:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=16, loss_sum=0.015609, avg_loss=0.000976, seen=16, correct=16, accuracy=1.000000
2025-11-11 16:31:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:31:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.895416, avg_loss=0.147385, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:31:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:31:23 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=1.000000, curr=0.975000
2025-11-11 16:31:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:31:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:31:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1276MB
2025-11-11 16:31:24 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:31:24 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:31:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:31:24 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:31:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:24 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:31:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:31:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:31:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=36.556946, avg_loss=0.182785, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:31:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1294MB allocated=1260MB
2025-11-11 16:31:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.218169, avg_loss=0.180454, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:31:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1294MB allocated=1260MB
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1390, total=5557)
2025-11-11 16:31:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:31:31 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=695, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:31:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:31:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=36.743149, avg_loss=0.183716, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:31:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:31:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.303842, avg_loss=0.157596, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:31:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:31:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:31:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:31:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:31:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:31:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:31:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:31:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=35.173637, avg_loss=0.175868, seen=200, correct=184, accuracy=0.920000
2025-11-11 16:31:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:31:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:31:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:31:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:31:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.991485, avg_loss=0.199787, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:31:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:31:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:31:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:31:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:31:58 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 16:32:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:32:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:32:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:32:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:32:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=31.133070, avg_loss=0.155665, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:32:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:32:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:32:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:32:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.583792, avg_loss=0.189595, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:32:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:32:11 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:32:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:32:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:32:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:32:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:32:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=24.338770, avg_loss=0.121694, seen=200, correct=188, accuracy=0.940000
2025-11-11 16:32:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:32:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:32:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:32:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.182194, avg_loss=0.129555, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:32:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:32:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:32:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:32:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:32:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:32:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:32:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=22.469309, avg_loss=0.112347, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:32:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:32:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:32:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:32:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.600308, avg_loss=0.215008, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:32:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:32:37 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 16:32:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:32:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:32:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:32:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:32:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.267784, avg_loss=0.101339, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:32:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:32:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:32:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:32:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.125045, avg_loss=0.153126, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:32:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:32:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:32:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:32:50 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.950000, curr=0.925000
2025-11-11 16:32:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:32:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:32:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:32:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:32:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:32:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=27.950747, avg_loss=0.139754, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:33:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.909779, avg_loss=0.197744, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:33:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:04 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:33:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:33:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:33:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:33:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:33:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=26.300920, avg_loss=0.131505, seen=200, correct=190, accuracy=0.950000
2025-11-11 16:33:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.640730, avg_loss=0.091018, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:33:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:33:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:33:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:33:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:33:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:33:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=25.165989, avg_loss=0.125830, seen=200, correct=188, accuracy=0.940000
2025-11-11 16:33:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.014013, avg_loss=0.075350, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:33:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:30 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:33:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:33:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:33:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:33:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:33:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:33:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=23.976784, avg_loss=0.119884, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:33:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.476364, avg_loss=0.061909, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:33:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:33:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:33:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:33:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1358MB allocated=1293MB
2025-11-11 16:33:44 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:33:44 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:33:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:33:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:33:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:33:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:33:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=31.054611, avg_loss=0.188210, seen=165, correct=156, accuracy=0.945455
2025-11-11 16:33:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1294MB allocated=1276MB
2025-11-11 16:33:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:33:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:33:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.369457, avg_loss=0.134236, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:33:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:33:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:33:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1294MB allocated=1276MB
2025-11-11 16:33:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:33:51 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:33:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=787, total=3146)
2025-11-11 16:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:51 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:33:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:33:51 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:33:51 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=394, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:33:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:33:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:34:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=33.419922, avg_loss=0.202545, seen=165, correct=155, accuracy=0.939394
2025-11-11 16:34:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.900197, avg_loss=0.097505, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:04 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:34:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=23.695841, avg_loss=0.143611, seen=165, correct=157, accuracy=0.951515
2025-11-11 16:34:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.408703, avg_loss=0.110218, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:34:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:34:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:34:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:34:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:34:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:34:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:34:26 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=21.910088, avg_loss=0.132788, seen=165, correct=158, accuracy=0.957576
2025-11-11 16:34:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:28 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.389126, avg_loss=0.184728, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:34:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:29 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.925000
2025-11-11 16:34:36 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:34:36 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:34:36 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:34:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:34:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:34:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=20.436029, avg_loss=0.123855, seen=165, correct=159, accuracy=0.963636
2025-11-11 16:34:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.867786, avg_loss=0.171695, seen=40, correct=36, accuracy=0.900000
2025-11-11 16:34:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:42 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.900000
2025-11-11 16:34:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:34:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:34:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:34:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:34:51 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=28.058998, avg_loss=0.170055, seen=165, correct=153, accuracy=0.927273
2025-11-11 16:34:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:34:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:34:53 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.935651, avg_loss=0.073391, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:34:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:34:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:34:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:34:54 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:35:01 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:35:01 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:35:01 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:35:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:35:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=25.583382, avg_loss=0.155051, seen=165, correct=155, accuracy=0.939394
2025-11-11 16:35:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.969961, avg_loss=0.224249, seen=40, correct=36, accuracy=0.900000
2025-11-11 16:35:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:07 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.900000
2025-11-11 16:35:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:35:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:35:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:35:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:35:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=20.677956, avg_loss=0.125321, seen=165, correct=157, accuracy=0.951515
2025-11-11 16:35:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.173456, avg_loss=0.154336, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:35:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:19 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.925000
2025-11-11 16:35:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:35:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:35:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:35:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:35:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:35:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=24.022938, avg_loss=0.145594, seen=165, correct=156, accuracy=0.945455
2025-11-11 16:35:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.800038, avg_loss=0.095001, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:35:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 16:35:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:35:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:35:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:35:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:35:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=21.256720, avg_loss=0.128829, seen=165, correct=159, accuracy=0.963636
2025-11-11 16:35:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.786456, avg_loss=0.094661, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:35:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.975000, curr=0.950000
2025-11-11 16:35:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:35:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:35:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:35:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=42, total=165)
2025-11-11 16:35:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=42
2025-11-11 16:35:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=165, loss_sum=21.124083, avg_loss=0.128025, seen=165, correct=158, accuracy=0.957576
2025-11-11 16:35:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.659443, avg_loss=0.091486, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:35:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:35:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:57 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:35:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:35:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:35:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:35:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1340MB allocated=1310MB
2025-11-11 16:35:58 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:35:58 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:35:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:35:58 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:35:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:35:58 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:35:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:35:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:35:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:36:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=32.220840, avg_loss=0.161104, seen=200, correct=189, accuracy=0.945000
2025-11-11 16:36:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1314MB allocated=1293MB
2025-11-11 16:36:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.946847, avg_loss=0.098671, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:36:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1314MB allocated=1293MB
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2384, total=9535)
2025-11-11 16:36:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:36:06 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1192, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:36:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:36:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:36:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:36:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.620693, avg_loss=0.103103, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:36:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:36:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.784019, avg_loss=0.069600, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:36:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:36:19 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:36:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:36:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:36:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:36:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=25.158165, avg_loss=0.125791, seen=200, correct=190, accuracy=0.950000
2025-11-11 16:36:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:36:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.814211, avg_loss=0.120355, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:36:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:36:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:36:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:36:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:36:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:36:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=27.931498, avg_loss=0.139657, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:36:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:36:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.694073, avg_loss=0.117352, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:36:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:36:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:36:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:36:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:36:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:36:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:36:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:36:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=25.535858, avg_loss=0.127679, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:36:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:36:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:36:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:36:57 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.334181, avg_loss=0.133355, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:36:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:36:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:36:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:36:58 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:37:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:37:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:37:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:37:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:37:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.371403, avg_loss=0.096857, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:37:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:37:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.004163, avg_loss=0.075104, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:37:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:37:11 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 16:37:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:37:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:37:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:37:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:37:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=23.763739, avg_loss=0.118819, seen=200, correct=188, accuracy=0.940000
2025-11-11 16:37:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:37:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.970348, avg_loss=0.049259, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:37:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:37:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:37:31 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:37:31 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:37:31 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:37:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:37:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=29.825317, avg_loss=0.149127, seen=200, correct=189, accuracy=0.945000
2025-11-11 16:37:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:37:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.385883, avg_loss=0.159647, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:37:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:37:37 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.925000
2025-11-11 16:37:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:37:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:37:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:37:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:37:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.349241, avg_loss=0.091746, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:37:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:37:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:37:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:37:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.890967, avg_loss=0.072274, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:37:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:37:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:37:50 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:37:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:37:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:37:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:37:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:37:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:37:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:38:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=21.191710, avg_loss=0.105959, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:38:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:38:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.189352, avg_loss=0.054734, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:38:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:38:03 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:38:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:38:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:38:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:38:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:38:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.123701, avg_loss=0.100619, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:38:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:38:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.812534, avg_loss=0.045313, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:38:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:38:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:38:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:38:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1388MB allocated=1327MB
2025-11-11 16:38:17 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:38:17 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:38:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:38:17 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:38:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:17 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:38:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:38:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:38:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:38:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=35.428894, avg_loss=0.181687, seen=195, correct=183, accuracy=0.938462
2025-11-11 16:38:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1334MB allocated=1310MB
2025-11-11 16:38:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.361924, avg_loss=0.109048, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:38:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1334MB allocated=1310MB
2025-11-11 16:38:25 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:38:25 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:38:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=929, total=3713)
2025-11-11 16:38:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:25 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:38:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:25 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:38:25 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=465, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:38:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:38:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:38:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:38:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:38:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:38:35 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=35.441071, avg_loss=0.181749, seen=195, correct=184, accuracy=0.943590
2025-11-11 16:38:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:38:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.548018, avg_loss=0.038700, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:38:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:38:38 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:38:45 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:38:45 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:38:45 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:38:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:38:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=27.360767, avg_loss=0.140312, seen=195, correct=183, accuracy=0.938462
2025-11-11 16:38:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:38:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:38:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:38:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.506094, avg_loss=0.062652, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:38:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:38:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:38:51 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:38:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:38:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:38:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:38:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:38:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:38:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:39:01 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=27.697071, avg_loss=0.142036, seen=195, correct=185, accuracy=0.948718
2025-11-11 16:39:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.654301, avg_loss=0.091358, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:39:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:04 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.950000
2025-11-11 16:39:11 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:39:11 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:39:11 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:39:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:39:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:39:14 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=25.414785, avg_loss=0.130332, seen=195, correct=186, accuracy=0.953846
2025-11-11 16:39:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:16 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.515551, avg_loss=0.062889, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:39:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:17 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:39:24 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:39:24 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:39:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:39:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:39:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=26.551098, avg_loss=0.136159, seen=195, correct=183, accuracy=0.938462
2025-11-11 16:39:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.441942, avg_loss=0.061049, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:39:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:30 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:39:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:39:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:39:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:39:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:39:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:39:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=18.335407, avg_loss=0.094028, seen=195, correct=188, accuracy=0.964103
2025-11-11 16:39:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.354802, avg_loss=0.083870, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:39:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:44 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.950000
2025-11-11 16:39:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:39:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:39:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:39:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:39:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:39:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=17.914700, avg_loss=0.091870, seen=195, correct=188, accuracy=0.964103
2025-11-11 16:39:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:39:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:39:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:39:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.549136, avg_loss=0.113728, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:39:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:39:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:39:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:39:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:39:57 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.925000
2025-11-11 16:40:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:40:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:40:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:40:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:40:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:40:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=30.058447, avg_loss=0.154146, seen=195, correct=184, accuracy=0.943590
2025-11-11 16:40:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.137852, avg_loss=0.078446, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:40:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:40:10 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:40:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:40:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:40:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:40:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:40:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=19.638950, avg_loss=0.100713, seen=195, correct=184, accuracy=0.943590
2025-11-11 16:40:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.777112, avg_loss=0.144428, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:40:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:40:23 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=1.000000, curr=0.925000
2025-11-11 16:40:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:40:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:40:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:40:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=195)
2025-11-11 16:40:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-11-11 16:40:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=195, loss_sum=21.318390, avg_loss=0.109325, seen=195, correct=185, accuracy=0.948718
2025-11-11 16:40:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:40:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.573858, avg_loss=0.114346, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:40:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:40:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=1.000000, curr=0.925000
2025-11-11 16:40:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:40:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:40:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1400MB allocated=1344MB
2025-11-11 16:40:37 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:40:37 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:40:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:40:37 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:40:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:37 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:40:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:40:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=28.569016, avg_loss=2.380751, seen=12, correct=1, accuracy=0.083333
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1354MB allocated=1327MB
2025-11-11 16:40:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=91.163406, avg_loss=2.279085, seen=40, correct=3, accuracy=0.075000
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1354MB allocated=1327MB
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.075000
2025-11-11 16:40:41 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:40:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=58, total=229)
2025-11-11 16:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:42 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:40:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:42 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:40:42 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=29, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:40:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:40:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:40:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=2.981875, avg_loss=0.248490, seen=12, correct=12, accuracy=1.000000
2025-11-11 16:40:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:40:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:40:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:40:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.783646, avg_loss=0.244591, seen=40, correct=36, accuracy=0.900000
2025-11-11 16:40:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:40:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:40:52 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.900000
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:40:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=1.153227, avg_loss=0.096102, seen=12, correct=12, accuracy=1.000000
2025-11-11 16:40:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:40:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.665288, avg_loss=0.141632, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:41:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:41:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:41:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:41:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:41:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:41:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=1.373934, avg_loss=0.114494, seen=12, correct=12, accuracy=1.000000
2025-11-11 16:41:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.969734, avg_loss=0.124243, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:41:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:13 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:41:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=1.281864, avg_loss=0.106822, seen=12, correct=11, accuracy=0.916667
2025-11-11 16:41:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.826520, avg_loss=0.120663, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:41:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=2.440088, avg_loss=0.203341, seen=12, correct=11, accuracy=0.916667
2025-11-11 16:41:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.803516, avg_loss=0.145088, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:41:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=0.945583, avg_loss=0.078799, seen=12, correct=12, accuracy=1.000000
2025-11-11 16:41:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.783930, avg_loss=0.144598, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:41:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:45 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 16:41:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:41:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:41:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:41:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:41:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=1.712886, avg_loss=0.142740, seen=12, correct=11, accuracy=0.916667
2025-11-11 16:41:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:41:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:41:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.518503, avg_loss=0.187963, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:41:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:41:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:41:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:41:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.950000, curr=0.925000
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=1.405613, avg_loss=0.117134, seen=12, correct=11, accuracy=0.916667
2025-11-11 16:42:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:42:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.805795, avg_loss=0.170145, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:42:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:42:06 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:42:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:42:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:42:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=0.801458, avg_loss=0.066788, seen=12, correct=11, accuracy=0.916667
2025-11-11 16:42:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:42:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.531335, avg_loss=0.213283, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:42:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:42:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=12)
2025-11-11 16:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=12, loss_sum=1.452130, avg_loss=0.121011, seen=12, correct=11, accuracy=0.916667
2025-11-11 16:42:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:42:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.188683, avg_loss=0.154717, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:42:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:42:27 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.950000, curr=0.925000
2025-11-11 16:42:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:42:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:42:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1402MB allocated=1360MB
2025-11-11 16:42:28 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:42:28 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:42:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:42:28 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:42:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:28 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:42:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:42:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:42:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:42:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=526.166260, avg_loss=2.630831, seen=200, correct=10, accuracy=0.050000
2025-11-11 16:42:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1374MB allocated=1344MB
2025-11-11 16:42:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=99.716606, avg_loss=2.492915, seen=40, correct=2, accuracy=0.050000
2025-11-11 16:42:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1374MB allocated=1344MB
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.050000
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=2350, total=9397)
2025-11-11 16:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:42:35 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=1175, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:42:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:42:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:42:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:42:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:42:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:42:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=35.403603, avg_loss=0.177018, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:42:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:42:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:42:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.876438, avg_loss=0.221911, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:42:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:42:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:42:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:42:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:42:58 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.952129, avg_loss=0.094761, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:42:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:42:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:42:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.689868, avg_loss=0.117247, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:43:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:01 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:43:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:43:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:43:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:43:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:43:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:43:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=27.141010, avg_loss=0.135705, seen=200, correct=188, accuracy=0.940000
2025-11-11 16:43:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.204995, avg_loss=0.105125, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:43:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:14 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:43:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:43:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:43:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:43:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:43:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=17.897793, avg_loss=0.089489, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:43:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:27 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.797450, avg_loss=0.119936, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:43:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:28 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:43:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:43:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:43:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:43:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:43:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:43:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=21.590382, avg_loss=0.107952, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:43:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.120696, avg_loss=0.153017, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:43:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:41 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:43:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:43:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:43:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:43:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:43:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:43:51 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.114830, avg_loss=0.090574, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:43:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:43:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:43:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:43:53 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.297723, avg_loss=0.107443, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:43:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:43:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:43:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:43:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:43:54 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:44:00 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:44:00 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:44:00 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:44:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:44:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.872410, avg_loss=0.094362, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:44:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.386339, avg_loss=0.109658, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:44:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:07 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:44:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:44:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:44:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:44:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:44:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:44:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=20.102623, avg_loss=0.100513, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:44:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.904939, avg_loss=0.122623, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:44:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:20 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:44:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=23.498610, avg_loss=0.117493, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:44:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.429427, avg_loss=0.085736, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:44:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:33 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:44:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:44:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:44:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:44:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:44:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:44:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=30.480007, avg_loss=0.152400, seen=200, correct=187, accuracy=0.935000
2025-11-11 16:44:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.266531, avg_loss=0.106663, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:44:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:46 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:44:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:44:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1438MB allocated=1377MB
2025-11-11 16:44:47 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:44:47 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:44:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:44:47 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:44:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:47 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:44:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:44:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:44:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:44:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=4.760715, avg_loss=0.183104, seen=26, correct=25, accuracy=0.961538
2025-11-11 16:44:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1394MB allocated=1360MB
2025-11-11 16:44:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:44:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:44:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.841953, avg_loss=0.121049, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:44:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:44:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1394MB allocated=1360MB
2025-11-11 16:44:52 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:44:52 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:44:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=126, total=503)
2025-11-11 16:44:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:52 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:44:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:52 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:44:52 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=63, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:44:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=2.261801, avg_loss=0.086992, seen=26, correct=25, accuracy=0.961538
2025-11-11 16:44:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.725942, avg_loss=0.043149, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:45:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:03 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:45:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:45:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:45:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:45:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:45:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=1.973460, avg_loss=0.075902, seen=26, correct=25, accuracy=0.961538
2025-11-11 16:45:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.239209, avg_loss=0.030980, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:45:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:13 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:45:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:45:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:45:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:45:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:45:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:45:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=0.671617, avg_loss=0.025831, seen=26, correct=26, accuracy=1.000000
2025-11-11 16:45:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.584257, avg_loss=0.014606, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:45:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:45:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:45:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:45:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:45:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:45:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:45:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=1.733945, avg_loss=0.066690, seen=26, correct=25, accuracy=0.961538
2025-11-11 16:45:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.252482, avg_loss=0.031312, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:45:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:45:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:45:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:45:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:45:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:45:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:45:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=0.519710, avg_loss=0.019989, seen=26, correct=26, accuracy=1.000000
2025-11-11 16:45:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.730811, avg_loss=0.018270, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:45:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:45:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:45:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:45:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:45:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:45:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:45:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=3.786486, avg_loss=0.145634, seen=26, correct=25, accuracy=0.961538
2025-11-11 16:45:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:45:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:45:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:45:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.533705, avg_loss=0.038343, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:45:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:45:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:45:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:45:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:45:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:46:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=1.210720, avg_loss=0.046566, seen=26, correct=25, accuracy=0.961538
2025-11-11 16:46:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.892701, avg_loss=0.047318, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:46:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:46:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:46:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:46:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:46:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:46:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=0.333702, avg_loss=0.012835, seen=26, correct=26, accuracy=1.000000
2025-11-11 16:46:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.506836, avg_loss=0.012671, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:46:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:46:23 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:46:23 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:46:23 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:46:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:46:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:46:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=0.804587, avg_loss=0.030946, seen=26, correct=26, accuracy=1.000000
2025-11-11 16:46:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.630151, avg_loss=0.015754, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:46:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:27 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:46:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:46:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:46:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:46:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=7, total=26)
2025-11-11 16:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=7
2025-11-11 16:46:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=26, loss_sum=1.589482, avg_loss=0.061134, seen=26, correct=25, accuracy=0.961538
2025-11-11 16:46:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.297721, avg_loss=0.032443, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:46:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:37 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:46:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:46:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1464MB allocated=1394MB
2025-11-11 16:46:38 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:46:38 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:46:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:46:38 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:46:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:38 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:46:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:46:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:46:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:46:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=30.771042, avg_loss=0.153855, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:46:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1394MB allocated=1377MB
2025-11-11 16:46:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.963204, avg_loss=0.124080, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:46:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1394MB allocated=1377MB
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4561, total=18241)
2025-11-11 16:46:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:46:46 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=2281, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:46:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:46:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:46:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:46:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:46:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:46:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=27.505449, avg_loss=0.137527, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:46:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:46:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:46:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:46:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.326070, avg_loss=0.108152, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:46:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:46:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:46:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:46:59 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:47:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:47:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:47:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:47:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:47:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=25.662144, avg_loss=0.128311, seen=200, correct=190, accuracy=0.950000
2025-11-11 16:47:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:47:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.181716, avg_loss=0.104543, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:47:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:47:13 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:47:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:47:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:47:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:47:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:47:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=21.958273, avg_loss=0.109791, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:47:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:47:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.428171, avg_loss=0.035704, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:47:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:47:26 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:47:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:47:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:47:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:47:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:47:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:47:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.856609, avg_loss=0.084283, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:47:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:47:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.879264, avg_loss=0.071982, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:47:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:47:39 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:47:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:47:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:47:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:47:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:47:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=23.471489, avg_loss=0.117357, seen=200, correct=192, accuracy=0.960000
2025-11-11 16:47:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:47:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.266645, avg_loss=0.031666, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:47:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:47:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:47:52 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:47:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:47:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:47:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:47:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:47:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:47:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:48:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.995913, avg_loss=0.099980, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:48:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.249445, avg_loss=0.031236, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:48:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:06 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:48:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:48:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:48:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:48:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:48:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:48:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=27.565933, avg_loss=0.137830, seen=200, correct=184, accuracy=0.920000
2025-11-11 16:48:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.120685, avg_loss=0.178017, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:48:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:19 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.925000
2025-11-11 16:48:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:48:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:48:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:48:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:48:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:48:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=18.894596, avg_loss=0.094473, seen=200, correct=190, accuracy=0.950000
2025-11-11 16:48:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.453098, avg_loss=0.136327, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:48:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:32 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.950000
2025-11-11 16:48:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:48:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:48:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:48:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:48:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=23.346773, avg_loss=0.116734, seen=200, correct=189, accuracy=0.945000
2025-11-11 16:48:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.693536, avg_loss=0.092338, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:48:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:46 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.950000
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:48:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.080135, avg_loss=0.095401, seen=200, correct=190, accuracy=0.950000
2025-11-11 16:48:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:48:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:48:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:48:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.626661, avg_loss=0.115667, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:48:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:48:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:48:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:48:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:48:59 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=1.000000, curr=0.950000
2025-11-11 16:48:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:48:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:48:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1442MB allocated=1411MB
2025-11-11 16:49:00 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:49:00 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:49:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:49:00 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:49:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:00 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:49:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:01 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:49:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:49:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:49:04 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=28.891743, avg_loss=0.144459, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:49:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1416MB allocated=1394MB
2025-11-11 16:49:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.771169, avg_loss=0.169279, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:49:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1416MB allocated=1394MB
2025-11-11 16:49:07 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:49:07 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:49:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1106, total=4423)
2025-11-11 16:49:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:08 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:49:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:08 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:49:08 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=553, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:49:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:49:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:49:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:49:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:49:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:49:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=16.300119, avg_loss=0.081501, seen=200, correct=195, accuracy=0.975000
2025-11-11 16:49:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:49:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.906824, avg_loss=0.147671, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:49:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:49:21 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:49:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:49:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:49:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:49:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:49:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=15.506683, avg_loss=0.077533, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:49:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:49:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.432778, avg_loss=0.135819, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:49:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:49:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:49:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:49:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:49:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:49:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:49:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=17.592249, avg_loss=0.087961, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:49:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:49:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:49:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.518096, avg_loss=0.187952, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:49:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:49:47 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:49:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:49:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:49:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:49:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:49:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:49:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:49:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.090630, avg_loss=0.095453, seen=200, correct=193, accuracy=0.965000
2025-11-11 16:49:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:49:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:49:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:49:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:49:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.629192, avg_loss=0.115730, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:50:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:01 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:50:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:50:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=19.769405, avg_loss=0.098847, seen=200, correct=194, accuracy=0.970000
2025-11-11 16:50:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.407060, avg_loss=0.135177, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:50:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:13 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 16:50:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:50:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:50:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:50:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:50:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:50:23 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=44.375626, avg_loss=0.221878, seen=200, correct=186, accuracy=0.930000
2025-11-11 16:50:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=10.186270, avg_loss=0.254657, seen=40, correct=35, accuracy=0.875000
2025-11-11 16:50:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:26 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:27 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.875000
2025-11-11 16:50:33 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:50:33 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:50:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:50:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=14.373257, avg_loss=0.071866, seen=200, correct=197, accuracy=0.985000
2025-11-11 16:50:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.035855, avg_loss=0.125896, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:50:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:40 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 16:50:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:50:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:50:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:50:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:50:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=13.680834, avg_loss=0.068404, seen=200, correct=196, accuracy=0.980000
2025-11-11 16:50:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:50:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:50:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:50:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.646843, avg_loss=0.141171, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:50:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:50:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:50:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:50:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:50:53 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.975000, curr=0.950000
2025-11-11 16:51:00 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:51:00 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:51:00 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:51:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:51:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=13.216159, avg_loss=0.066081, seen=200, correct=196, accuracy=0.980000
2025-11-11 16:51:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:51:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:06 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.036694, avg_loss=0.125917, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:51:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:51:07 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=0.975000, curr=0.950000
2025-11-11 16:51:13 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:51:13 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:51:13 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:51:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-11-11 16:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-11-11 16:51:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=200, loss_sum=23.247467, avg_loss=0.116237, seen=200, correct=191, accuracy=0.955000
2025-11-11 16:51:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:51:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.285540, avg_loss=0.132139, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:51:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:51:20 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=0.975000, curr=0.950000
2025-11-11 16:51:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:51:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:51:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1462MB allocated=1428MB
2025-11-11 16:51:21 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:51:21 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:51:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:51:21 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:51:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=22, num_train_batch_last_epoch=12, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:21 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:51:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:51:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:51:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=22, num_train_batch_last_epoch=12, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:51:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=5.470221, avg_loss=2.735111, seen=2, correct=0, accuracy=0.000000
2025-11-11 16:51:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1436MB allocated=1411MB
2025-11-11 16:51:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=22, num_train_batch_last_epoch=12, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=109.889069, avg_loss=2.747227, seen=40, correct=0, accuracy=0.000000
2025-11-11 16:51:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1436MB allocated=1411MB
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.000000
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=11, total=43)
2025-11-11 16:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=22, num_train_batch_last_epoch=12, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:51:25 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=6, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:51:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:51:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:51:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:51:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:51:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:51:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.281344, avg_loss=0.140672, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:51:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:51:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.141331, avg_loss=0.178533, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:51:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:51:35 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:51:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:51:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:51:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:51:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:51:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.034395, avg_loss=0.017197, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:51:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:51:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.389225, avg_loss=0.059731, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:51:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:51:46 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:51:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:51:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:51:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:51:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:51:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.012561, avg_loss=0.006280, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:51:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:51:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:51:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:51:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.772158, avg_loss=0.044304, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:51:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:51:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:51:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:51:56 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:52:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:52:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:52:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:52:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:52:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.006163, avg_loss=0.003082, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:52:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.578062, avg_loss=0.039452, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:52:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:52:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:52:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:52:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:52:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:52:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:52:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.004456, avg_loss=0.002228, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:52:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.606109, avg_loss=0.040153, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:52:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:16 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:52:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.002382, avg_loss=0.001191, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:52:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:25 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.017384, avg_loss=0.050435, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:52:25 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:26 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:26 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=1.000000, curr=0.975000
2025-11-11 16:52:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:52:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:52:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:52:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:52:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:52:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.001694, avg_loss=0.000847, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:52:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.177639, avg_loss=0.029441, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:52:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=5/25), best=1.000000, curr=0.975000
2025-11-11 16:52:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:52:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:52:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:52:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:52:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.001102, avg_loss=0.000551, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:52:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.363459, avg_loss=0.034086, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:52:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:46 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=6/25), best=1.000000, curr=0.975000
2025-11-11 16:52:52 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:52:52 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:52:52 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:52:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:52:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:52:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.000850, avg_loss=0.000425, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:52:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:52:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:52:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:52:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.388211, avg_loss=0.034705, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:52:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:52:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:52:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:52:55 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=7/25), best=1.000000, curr=0.975000
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1, total=2)
2025-11-11 16:53:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=1
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=2, loss_sum=0.000941, avg_loss=0.000470, seen=2, correct=2, accuracy=1.000000
2025-11-11 16:53:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:53:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.205976, avg_loss=0.055149, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:53:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:53:06 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=8/25), best=1.000000, curr=0.975000
2025-11-11 16:53:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:53:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:53:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1510MB allocated=1444MB
2025-11-11 16:53:06 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:53:06 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:53:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:53:07 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:53:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:07 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:53:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:53:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:53:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:53:10 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=23.069944, avg_loss=0.165971, seen=139, correct=134, accuracy=0.964029
2025-11-11 16:53:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1456MB allocated=1428MB
2025-11-11 16:53:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.400978, avg_loss=0.185024, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:53:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1456MB allocated=1428MB
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=664, total=2655)
2025-11-11 16:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:53:13 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=332, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:53:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:53:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:53:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:53:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:53:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:53:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=17.451239, avg_loss=0.125548, seen=139, correct=132, accuracy=0.949640
2025-11-11 16:53:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:53:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.759093, avg_loss=0.218977, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:53:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:53:25 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:53:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:53:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:53:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:53:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:53:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=18.321102, avg_loss=0.131806, seen=139, correct=133, accuracy=0.956835
2025-11-11 16:53:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:53:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:37 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.124106, avg_loss=0.178103, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:53:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:53:38 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 16:53:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:53:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:53:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:53:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:53:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:53:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=18.296711, avg_loss=0.131631, seen=139, correct=133, accuracy=0.956835
2025-11-11 16:53:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:53:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:53:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.601632, avg_loss=0.165041, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:53:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:53:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:53:50 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.950000, curr=0.925000
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:53:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:53:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:53:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=14.428679, avg_loss=0.103803, seen=139, correct=133, accuracy=0.956835
2025-11-11 16:53:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.069678, avg_loss=0.176742, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:54:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:54:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:54:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=14.243793, avg_loss=0.102473, seen=139, correct=132, accuracy=0.949640
2025-11-11 16:54:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.569731, avg_loss=0.239243, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:54:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:15 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:54:22 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:54:22 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:54:22 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:54:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:54:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=13.099778, avg_loss=0.094243, seen=139, correct=135, accuracy=0.971223
2025-11-11 16:54:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=11.066913, avg_loss=0.276673, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:54:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:27 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:54:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:54:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=21.741739, avg_loss=0.156415, seen=139, correct=130, accuracy=0.935252
2025-11-11 16:54:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=12.162542, avg_loss=0.304064, seen=40, correct=37, accuracy=0.925000
2025-11-11 16:54:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:39 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 16:54:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:54:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:54:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:54:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:54:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=23.623842, avg_loss=0.169956, seen=139, correct=128, accuracy=0.920863
2025-11-11 16:54:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:54:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:54:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.792360, avg_loss=0.169809, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:54:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:54:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:54:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:54:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:54:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:54:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:54:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:54:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:54:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:55:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=15.016729, avg_loss=0.108034, seen=139, correct=133, accuracy=0.956835
2025-11-11 16:55:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:55:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:03 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.031253, avg_loss=0.150781, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:55:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:55:03 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:55:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:55:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:55:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:55:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=139)
2025-11-11 16:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-11-11 16:55:12 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=139, loss_sum=12.776513, avg_loss=0.091917, seen=139, correct=133, accuracy=0.956835
2025-11-11 16:55:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:55:14 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:14 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.837655, avg_loss=0.220941, seen=40, correct=38, accuracy=0.950000
2025-11-11 16:55:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:55:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 16:55:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:55:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:55:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1532MB allocated=1461MB
2025-11-11 16:55:16 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:55:16 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:55:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:55:16 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:55:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:17 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:55:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:55:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:55:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:55:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=414.345886, avg_loss=2.573577, seen=161, correct=15, accuracy=0.093168
2025-11-11 16:55:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1476MB allocated=1444MB
2025-11-11 16:55:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=109.535629, avg_loss=2.738391, seen=40, correct=1, accuracy=0.025000
2025-11-11 16:55:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1476MB allocated=1444MB
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.025000
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=767, total=3066)
2025-11-11 16:55:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:55:23 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=384, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:55:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:55:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:55:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:55:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:55:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:55:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=30.348064, avg_loss=0.188497, seen=161, correct=156, accuracy=0.968944
2025-11-11 16:55:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:55:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.492652, avg_loss=0.137316, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:55:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:55:36 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:55:43 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:55:43 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:55:43 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:55:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:55:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:55:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=18.569729, avg_loss=0.115340, seen=161, correct=155, accuracy=0.962733
2025-11-11 16:55:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:55:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:55:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.366552, avg_loss=0.059164, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:55:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:55:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:55:49 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 16:55:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:55:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:55:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:55:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:55:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:55:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:55:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:55:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=17.208332, avg_loss=0.106884, seen=161, correct=155, accuracy=0.962733
2025-11-11 16:55:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.417522, avg_loss=0.035438, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:56:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:56:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:56:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:56:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:56:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:56:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:56:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=14.634761, avg_loss=0.090899, seen=161, correct=156, accuracy=0.968944
2025-11-11 16:56:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:14 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.726887, avg_loss=0.043172, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:56:14 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:15 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:56:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:56:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:56:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:56:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:56:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:56:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=18.935040, avg_loss=0.117609, seen=161, correct=154, accuracy=0.956522
2025-11-11 16:56:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.814542, avg_loss=0.020364, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:56:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:27 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:56:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:56:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:56:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:56:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:56:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:56:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=15.713311, avg_loss=0.097598, seen=161, correct=155, accuracy=0.962733
2025-11-11 16:56:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.033792, avg_loss=0.050845, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:56:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:40 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:56:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:56:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:56:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:56:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:56:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:56:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=16.026142, avg_loss=0.099541, seen=161, correct=155, accuracy=0.962733
2025-11-11 16:56:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:56:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:56:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.903329, avg_loss=0.022583, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:56:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:56:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:56:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:56:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:56:53 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:56:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:56:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:56:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:57:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:57:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:57:02 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=15.449369, avg_loss=0.095959, seen=161, correct=155, accuracy=0.962733
2025-11-11 16:57:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:57:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:04 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.396956, avg_loss=0.034924, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:57:04 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:05 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:57:05 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:57:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:57:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:57:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:57:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:57:15 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=12.837111, avg_loss=0.079734, seen=161, correct=157, accuracy=0.975155
2025-11-11 16:57:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:15 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:57:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:17 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.894203, avg_loss=0.047355, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:57:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:57:18 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:57:25 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:57:25 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:57:25 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:57:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-11-11 16:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-11-11 16:57:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=161, loss_sum=13.212582, avg_loss=0.082066, seen=161, correct=157, accuracy=0.975155
2025-11-11 16:57:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:57:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:30 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.581945, avg_loss=0.064549, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:57:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:57:31 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=1.000000, curr=0.975000
2025-11-11 16:57:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:57:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:57:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1520MB allocated=1478MB
2025-11-11 16:57:32 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:57:32 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:57:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:57:32 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:57:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:32 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:57:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:33 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:57:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:57:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:57:34 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=249.605377, avg_loss=2.773393, seen=90, correct=6, accuracy=0.066667
2025-11-11 16:57:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1496MB allocated=1461MB
2025-11-11 16:57:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:36 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=113.799698, avg_loss=2.844992, seen=40, correct=1, accuracy=0.025000
2025-11-11 16:57:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1496MB allocated=1461MB
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.025000
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=431, total=1724)
2025-11-11 16:57:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:57:37 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=216, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:57:44 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:57:44 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:57:44 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:57:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:57:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:57:46 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=17.242508, avg_loss=0.191583, seen=90, correct=85, accuracy=0.944444
2025-11-11 16:57:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:57:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:57:48 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.864577, avg_loss=0.146614, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:57:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:57:49 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:57:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:57:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:57:56 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:57:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:57:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:57:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:57:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=11.333294, avg_loss=0.125925, seen=90, correct=85, accuracy=0.944444
2025-11-11 16:57:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:57:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:57:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:57:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:57:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:57:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.392096, avg_loss=0.059802, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:00 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:58:07 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 16:58:07 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 16:58:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 16:58:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:58:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:58:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=11.116709, avg_loss=0.123519, seen=90, correct=85, accuracy=0.944444
2025-11-11 16:58:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:11 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.323449, avg_loss=0.033086, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:58:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:12 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=1.000000, curr=0.975000
2025-11-11 16:58:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 16:58:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 16:58:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 16:58:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:58:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:58:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=11.124784, avg_loss=0.123609, seen=90, correct=86, accuracy=0.955556
2025-11-11 16:58:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.923716, avg_loss=0.023093, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:58:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:58:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 16:58:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 16:58:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 16:58:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:58:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=14.621226, avg_loss=0.162458, seen=90, correct=84, accuracy=0.933333
2025-11-11 16:58:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.233026, avg_loss=0.030826, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:58:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 16:58:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 16:58:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 16:58:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:58:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:58:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=9.969239, avg_loss=0.110769, seen=90, correct=85, accuracy=0.944444
2025-11-11 16:58:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.042500, avg_loss=0.026063, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:58:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:47 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:58:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 16:58:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 16:58:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 16:58:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:58:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=9.660518, avg_loss=0.107339, seen=90, correct=86, accuracy=0.955556
2025-11-11 16:58:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:58:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:58:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:58:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.203732, avg_loss=0.030093, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:58:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:58:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:58:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:58:59 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:59:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 16:59:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 16:59:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 16:59:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:59:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:59:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=12.871403, avg_loss=0.143016, seen=90, correct=86, accuracy=0.955556
2025-11-11 16:59:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:59:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.024165, avg_loss=0.025604, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:59:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:59:10 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 16:59:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 16:59:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 16:59:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 16:59:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:59:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=10.699858, avg_loss=0.118887, seen=90, correct=86, accuracy=0.955556
2025-11-11 16:59:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:59:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.801863, avg_loss=0.020047, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:59:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:59:22 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:59:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 16:59:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 16:59:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 16:59:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=90)
2025-11-11 16:59:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-11-11 16:59:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=90, loss_sum=10.492037, avg_loss=0.116578, seen=90, correct=86, accuracy=0.955556
2025-11-11 16:59:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:59:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.110979, avg_loss=0.027774, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:59:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:59:33 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:59:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 16:59:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 16:59:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1542MB allocated=1495MB
2025-11-11 16:59:34 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 16:59:34 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {}}
2025-11-11 16:59:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 16:59:34 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 16:59:35 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=88, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:35 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 16:59:35 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:35 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 16:59:36 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:59:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:36 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=88, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:59:36 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=27.423513, avg_loss=3.047057, seen=9, correct=0, accuracy=0.000000
2025-11-11 16:59:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:37 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1496MB allocated=1478MB
2025-11-11 16:59:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=88, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:38 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=105.187721, avg_loss=2.629693, seen=40, correct=0, accuracy=0.000000
2025-11-11 16:59:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1496MB allocated=1478MB
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.000000
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=44, total=176)
2025-11-11 16:59:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=88, num_train_batch_last_epoch=12, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 16:59:39 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=22, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 16:59:46 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 16:59:46 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 16:59:46 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 16:59:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:59:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:59:47 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=2.078612, avg_loss=0.230957, seen=9, correct=8, accuracy=0.888889
2025-11-11 16:59:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:59:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:49 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.686408, avg_loss=0.167160, seen=40, correct=40, accuracy=1.000000
2025-11-11 16:59:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:59:50 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 16:59:56 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 16:59:56 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 16:59:57 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 16:59:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 16:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 16:59:57 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=1.029684, avg_loss=0.114409, seen=9, correct=8, accuracy=0.888889
2025-11-11 16:59:57 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 16:59:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 16:59:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 16:59:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 16:59:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 16:59:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 16:59:59 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.523159, avg_loss=0.063079, seen=40, correct=39, accuracy=0.975000
2025-11-11 16:59:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 16:59:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:00 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 17:00:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=0.459718, avg_loss=0.051080, seen=9, correct=9, accuracy=1.000000
2025-11-11 17:00:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.411625, avg_loss=0.035291, seen=40, correct=40, accuracy=1.000000
2025-11-11 17:00:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:11 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 17:00:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 17:00:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 17:00:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 17:00:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 17:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 17:00:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=0.194462, avg_loss=0.021607, seen=9, correct=9, accuracy=1.000000
2025-11-11 17:00:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.662573, avg_loss=0.016564, seen=40, correct=40, accuracy=1.000000
2025-11-11 17:00:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:21 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 17:00:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 17:00:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 17:00:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 17:00:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 17:00:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 17:00:28 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=0.091373, avg_loss=0.010153, seen=9, correct=9, accuracy=1.000000
2025-11-11 17:00:28 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.484636, avg_loss=0.012116, seen=40, correct=40, accuracy=1.000000
2025-11-11 17:00:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:32 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 17:00:38 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 17:00:38 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 17:00:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 17:00:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=0.065988, avg_loss=0.007332, seen=9, correct=9, accuracy=1.000000
2025-11-11 17:00:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:41 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.128766, avg_loss=0.028219, seen=40, correct=39, accuracy=0.975000
2025-11-11 17:00:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:42 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 17:00:49 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 17:00:49 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 17:00:49 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 17:00:49 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 17:00:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:49 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 17:00:50 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=0.019264, avg_loss=0.002140, seen=9, correct=9, accuracy=1.000000
2025-11-11 17:00:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:00:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:00:52 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.661262, avg_loss=0.016532, seen=40, correct=40, accuracy=1.000000
2025-11-11 17:00:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:00:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:00:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:00:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:00:53 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 17:01:00 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 17:01:00 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 17:01:00 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 17:01:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 17:01:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 17:01:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=0.020983, avg_loss=0.002331, seen=9, correct=9, accuracy=1.000000
2025-11-11 17:01:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:01:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:01:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:01:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.288442, avg_loss=0.057211, seen=40, correct=39, accuracy=0.975000
2025-11-11 17:01:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:01:03 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=1.000000, curr=0.975000
2025-11-11 17:01:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 17:01:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 17:01:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 17:01:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 17:01:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 17:01:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=0.011479, avg_loss=0.001275, seen=9, correct=9, accuracy=1.000000
2025-11-11 17:01:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:01:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:01:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.440080, avg_loss=0.011002, seen=40, correct=40, accuracy=1.000000
2025-11-11 17:01:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:01:14 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 17:01:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 17:01:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 17:01:20 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 17:01:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=9)
2025-11-11 17:01:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-11-11 17:01:21 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=9, loss_sum=0.008517, avg_loss=0.000946, seen=9, correct=9, accuracy=1.000000
2025-11-11 17:01:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:01:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:01:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:01:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=0.277467, avg_loss=0.006937, seen=40, correct=40, accuracy=1.000000
2025-11-11 17:01:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:24 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:01:24 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=1.000000
2025-11-11 17:01:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 17:01:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 17:01:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1566MB allocated=1511MB
2025-11-11 17:01:25 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 17:01:25 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {}}
2025-11-11 17:01:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:01:25 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 17:01:25 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:25 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 17:01:25 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 17:01:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:01:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:01:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=5.430985, avg_loss=0.175193, seen=31, correct=29, accuracy=0.935484
2025-11-11 17:01:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1516MB allocated=1495MB
2025-11-11 17:01:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:01:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.209961, avg_loss=0.230249, seen=40, correct=35, accuracy=0.875000
2025-11-11 17:01:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:29 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1516MB allocated=1495MB
2025-11-11 17:01:30 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.875000
2025-11-11 17:01:30 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 17:01:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=151, total=601)
2025-11-11 17:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:30 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 17:01:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:30 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 17:01:30 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=76, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 17:01:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 17:01:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 17:01:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 17:01:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:01:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=2.219039, avg_loss=0.071582, seen=31, correct=30, accuracy=0.967742
2025-11-11 17:01:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:01:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:01:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:01:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.380978, avg_loss=0.209524, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:01:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:01:41 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:01:47 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 17:01:47 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 17:01:47 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 17:01:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:01:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:01:48 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=3.927271, avg_loss=0.126686, seen=31, correct=29, accuracy=0.935484
2025-11-11 17:01:48 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:49 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:49 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:01:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:01:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:01:50 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=11.364008, avg_loss=0.284100, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:01:50 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:51 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:01:51 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:01:51 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:01:58 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 17:01:58 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 17:01:58 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 17:01:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:01:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:01:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:01:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:01:59 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=2.220480, avg_loss=0.071628, seen=31, correct=29, accuracy=0.935484
2025-11-11 17:01:59 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:00 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:02:01 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=10.841767, avg_loss=0.271044, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:02:01 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:02 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:02 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:02 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:02:09 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 17:02:09 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 17:02:09 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 17:02:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:02:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:02:09 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=2.925446, avg_loss=0.094369, seen=31, correct=30, accuracy=0.967742
2025-11-11 17:02:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:11 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:11 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:02:12 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=11.121624, avg_loss=0.278041, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:02:12 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:13 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:13 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:02:19 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 17:02:19 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 17:02:19 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 17:02:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:02:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:02:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=1.329726, avg_loss=0.042894, seen=31, correct=31, accuracy=1.000000
2025-11-11 17:02:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:02:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.357412, avg_loss=0.233935, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:02:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:02:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 17:02:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 17:02:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 17:02:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:02:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:02:31 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=4.736437, avg_loss=0.152788, seen=31, correct=29, accuracy=0.935484
2025-11-11 17:02:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:02:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:02:33 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=11.541610, avg_loss=0.288540, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:02:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:34 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:02:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 17:02:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 17:02:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 17:02:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:02:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:02:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=0.618893, avg_loss=0.019964, seen=31, correct=31, accuracy=1.000000
2025-11-11 17:02:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:02:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:02:44 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.566183, avg_loss=0.214155, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:02:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:02:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 17:02:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 17:02:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 17:02:52 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:52 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:02:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=0.880458, avg_loss=0.028402, seen=31, correct=31, accuracy=1.000000
2025-11-11 17:02:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:53 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:02:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:02:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:02:54 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.534954, avg_loss=0.188374, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:02:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:02:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:02:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:02:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:02:55 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:03:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 17:03:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 17:03:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 17:03:02 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:02 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:03:03 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=1.414983, avg_loss=0.045645, seen=31, correct=30, accuracy=0.967742
2025-11-11 17:03:03 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:04 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:04 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:03:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:03:05 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.717951, avg_loss=0.242949, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:03:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:05 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:03:06 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:03:12 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 17:03:12 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 17:03:12 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 17:03:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=31)
2025-11-11 17:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-11-11 17:03:13 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=31, loss_sum=0.641361, avg_loss=0.020689, seen=31, correct=31, accuracy=1.000000
2025-11-11 17:03:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:15 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:03:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:03:15 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=9.088352, avg_loss=0.227209, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:03:15 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:16 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:16 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:03:16 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:03:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 17:03:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 17:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1582MB allocated=1528MB
2025-11-11 17:03:17 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 17:03:17 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {}}
2025-11-11 17:03:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:03:17 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 17:03:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:17 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 17:03:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:03:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 17:03:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:03:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:03:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=19.337568, avg_loss=0.158505, seen=122, correct=117, accuracy=0.959016
2025-11-11 17:03:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1536MB allocated=1511MB
2025-11-11 17:03:22 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:22 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:03:23 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.611701, avg_loss=0.190293, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:03:23 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1536MB allocated=1511MB
2025-11-11 17:03:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:03:23 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 17:03:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=583, total=2331)
2025-11-11 17:03:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:24 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 17:03:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:24 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 17:03:24 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=292, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 17:03:30 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 17:03:30 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 17:03:30 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 17:03:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:03:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:03:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=22.760944, avg_loss=0.186565, seen=122, correct=115, accuracy=0.942623
2025-11-11 17:03:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:03:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:03:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:03:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=13.133079, avg_loss=0.328327, seen=40, correct=34, accuracy=0.850000
2025-11-11 17:03:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:03:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.925000, curr=0.850000
2025-11-11 17:03:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 17:03:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 17:03:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 17:03:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:03:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=15.573666, avg_loss=0.127653, seen=122, correct=117, accuracy=0.959016
2025-11-11 17:03:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:03:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:03:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:03:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=11.770950, avg_loss=0.294274, seen=40, correct=34, accuracy=0.850000
2025-11-11 17:03:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:47 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:03:47 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.925000, curr=0.850000
2025-11-11 17:03:54 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 17:03:54 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 17:03:54 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 17:03:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:03:56 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=18.764763, avg_loss=0.153810, seen=122, correct=116, accuracy=0.950820
2025-11-11 17:03:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:03:58 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:03:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:58 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:03:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:03:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=15.199233, avg_loss=0.379981, seen=40, correct=34, accuracy=0.850000
2025-11-11 17:03:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:03:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:03:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:03:59 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:03:59 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.925000, curr=0.850000
2025-11-11 17:04:06 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 17:04:06 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 17:04:06 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 17:04:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:04:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:04:08 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=13.767396, avg_loss=0.112848, seen=122, correct=116, accuracy=0.950820
2025-11-11 17:04:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:04:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:04:10 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=10.391307, avg_loss=0.259783, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:04:10 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:11 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:11 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:11 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:04:18 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 17:04:18 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 17:04:18 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 17:04:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:04:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:04:20 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=13.442847, avg_loss=0.110187, seen=122, correct=114, accuracy=0.934426
2025-11-11 17:04:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:04:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:04:22 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=10.023253, avg_loss=0.250581, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:04:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:23 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:04:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 17:04:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 17:04:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 17:04:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:04:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:30 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:04:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=10.390478, avg_loss=0.085168, seen=122, correct=117, accuracy=0.959016
2025-11-11 17:04:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:33 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:04:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:33 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:04:34 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=11.568325, avg_loss=0.289208, seen=40, correct=35, accuracy=0.875000
2025-11-11 17:04:34 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:34 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:35 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.925000, curr=0.875000
2025-11-11 17:04:41 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 17:04:41 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 17:04:41 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 17:04:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:04:44 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=19.060844, avg_loss=0.156236, seen=122, correct=114, accuracy=0.934426
2025-11-11 17:04:44 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:04:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:04:46 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=14.024859, avg_loss=0.350621, seen=40, correct=34, accuracy=0.850000
2025-11-11 17:04:46 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:47 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.925000, curr=0.850000
2025-11-11 17:04:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 17:04:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 17:04:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 17:04:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:04:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:04:55 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=21.506371, avg_loss=0.176282, seen=122, correct=114, accuracy=0.934426
2025-11-11 17:04:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:57 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:04:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:57 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:04:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:04:58 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=13.719009, avg_loss=0.342975, seen=40, correct=33, accuracy=0.825000
2025-11-11 17:04:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:04:58 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:04:58 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:04:58 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.925000, curr=0.825000
2025-11-11 17:05:05 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 17:05:05 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 17:05:05 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 17:05:05 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:05:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:05 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:05:07 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=12.367559, avg_loss=0.101373, seen=122, correct=118, accuracy=0.967213
2025-11-11 17:05:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:05:09 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:05:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:09 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:05:09 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=8.080142, avg_loss=0.202004, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:05:09 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:10 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:10 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:05:10 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:05:17 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 17:05:17 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 17:05:17 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 17:05:17 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=122)
2025-11-11 17:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:17 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-11-11 17:05:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=122, loss_sum=15.627682, avg_loss=0.128096, seen=122, correct=115, accuracy=0.942623
2025-11-11 17:05:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:05:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:05:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:05:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=10.501031, avg_loss=0.262526, seen=40, correct=36, accuracy=0.900000
2025-11-11 17:05:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:05:22 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.925000, curr=0.900000
2025-11-11 17:05:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 17:05:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 17:05:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:23 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1580MB allocated=1545MB
2025-11-11 17:05:23 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 17:05:23 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {}}
2025-11-11 17:05:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:05:23 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 17:05:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:23 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 17:05:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:24 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 17:05:24 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:24 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:05:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=514.651611, avg_loss=2.752148, seen=187, correct=8, accuracy=0.042781
2025-11-11 17:05:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:29 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1556MB allocated=1528MB
2025-11-11 17:05:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:05:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:05:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=109.709702, avg_loss=2.742743, seen=40, correct=3, accuracy=0.075000
2025-11-11 17:05:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1556MB allocated=1528MB
2025-11-11 17:05:30 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.075000
2025-11-11 17:05:30 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 17:05:30 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=890, total=3560)
2025-11-11 17:05:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:31 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 17:05:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:31 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 17:05:31 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=445, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 17:05:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 17:05:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 17:05:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 17:05:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:05:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:05:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=31.063364, avg_loss=0.166114, seen=187, correct=179, accuracy=0.957219
2025-11-11 17:05:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:42 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:05:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:05:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:05:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.958459, avg_loss=0.173961, seen=40, correct=39, accuracy=0.975000
2025-11-11 17:05:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:05:44 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 17:05:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 17:05:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 17:05:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 17:05:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:05:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:05:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=19.420208, avg_loss=0.103851, seen=187, correct=180, accuracy=0.962567
2025-11-11 17:05:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:05:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:05:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:05:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.115256, avg_loss=0.102881, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:05:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:05:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:05:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:05:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:05:57 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 17:06:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 17:06:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 17:06:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 17:06:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:06:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=20.631620, avg_loss=0.110330, seen=187, correct=180, accuracy=0.962567
2025-11-11 17:06:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:06:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:06:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:06:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:06:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.024915, avg_loss=0.075623, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:06:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:06:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:06:09 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 17:06:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 17:06:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 17:06:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 17:06:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:06:19 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=21.448284, avg_loss=0.114697, seen=187, correct=182, accuracy=0.973262
2025-11-11 17:06:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:06:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:06:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:06:21 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.530088, avg_loss=0.063252, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:06:21 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:06:22 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:06:22 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 17:06:29 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 17:06:29 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 17:06:29 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 17:06:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:06:32 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=16.624201, avg_loss=0.088899, seen=187, correct=181, accuracy=0.967914
2025-11-11 17:06:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:06:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:06:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:06:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.307953, avg_loss=0.082699, seen=40, correct=39, accuracy=0.975000
2025-11-11 17:06:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:06:35 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:06:35 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 17:06:42 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 17:06:42 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 17:06:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 17:06:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:06:45 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=14.334986, avg_loss=0.076658, seen=187, correct=183, accuracy=0.978610
2025-11-11 17:06:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:06:47 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:06:47 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:47 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:06:47 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.696877, avg_loss=0.117422, seen=40, correct=39, accuracy=0.975000
2025-11-11 17:06:47 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:48 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:06:48 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:06:48 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 17:06:55 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 17:06:55 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 17:06:55 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 17:06:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:06:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:06:58 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=17.002275, avg_loss=0.090921, seen=187, correct=181, accuracy=0.967914
2025-11-11 17:06:58 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:06:59 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:00 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:00 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:07:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:00 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:07:00 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.804901, avg_loss=0.045123, seen=40, correct=39, accuracy=0.975000
2025-11-11 17:07:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:01 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 17:07:08 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 17:07:08 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 17:07:08 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 17:07:08 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:08 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:07:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=23.169880, avg_loss=0.123903, seen=187, correct=178, accuracy=0.951872
2025-11-11 17:07:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:13 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:13 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:07:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.180786, avg_loss=0.054520, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:07:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:14 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:14 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.975000, curr=0.950000
2025-11-11 17:07:21 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 17:07:21 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 17:07:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 17:07:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:07:24 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=15.736644, avg_loss=0.084153, seen=187, correct=179, accuracy=0.957219
2025-11-11 17:07:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:25 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:07:26 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.713925, avg_loss=0.067848, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:07:26 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:27 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:27 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:27 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.975000, curr=0.950000
2025-11-11 17:07:34 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 17:07:34 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 17:07:34 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 17:07:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=187)
2025-11-11 17:07:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-11-11 17:07:37 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=187, loss_sum=21.833248, avg_loss=0.116755, seen=187, correct=180, accuracy=0.962567
2025-11-11 17:07:37 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:38 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:38 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:07:39 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.835925, avg_loss=0.070898, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:07:39 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:40 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:40 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:40 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.975000, curr=0.950000
2025-11-11 17:07:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 17:07:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 17:07:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1602MB allocated=1562MB
2025-11-11 17:07:41 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 17:07:41 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {}}
2025-11-11 17:07:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:07:41 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 17:07:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:41 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 17:07:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:07:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:42 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 17:07:42 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:42 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:07:43 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=13.005170, avg_loss=0.245381, seen=53, correct=49, accuracy=0.924528
2025-11-11 17:07:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1578MB allocated=1545MB
2025-11-11 17:07:45 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:45 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:07:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.378676, avg_loss=0.184467, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:07:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1578MB allocated=1545MB
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.925000
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=256, total=1021)
2025-11-11 17:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 17:07:46 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=128, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 17:07:53 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 17:07:53 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 17:07:53 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 17:07:53 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:07:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:53 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:07:54 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=14.266348, avg_loss=0.269176, seen=53, correct=48, accuracy=0.905660
2025-11-11 17:07:54 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:07:56 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:07:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:56 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:07:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:07:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.917781, avg_loss=0.122945, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:07:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:07:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:07:57 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:07:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:07:57 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:08:04 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 17:08:04 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 17:08:04 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 17:08:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:08:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=11.881262, avg_loss=0.224175, seen=53, correct=50, accuracy=0.943396
2025-11-11 17:08:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:08:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.155149, avg_loss=0.153879, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:08:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:08 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:08 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:08:15 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 17:08:15 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 17:08:15 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 17:08:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:08:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:08:16 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=9.677760, avg_loss=0.182599, seen=53, correct=51, accuracy=0.962264
2025-11-11 17:08:16 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:17 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:17 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:18 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:08:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:18 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:08:18 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.023540, avg_loss=0.100588, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:08:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:19 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:08:26 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 17:08:26 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 17:08:26 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 17:08:26 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:08:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:26 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:08:27 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=11.795490, avg_loss=0.222556, seen=53, correct=49, accuracy=0.924528
2025-11-11 17:08:27 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:28 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:28 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:29 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:29 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:08:29 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.545441, avg_loss=0.113636, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:08:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:30 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:30 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:08:37 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 17:08:37 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 17:08:37 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 17:08:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:08:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:08:38 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=9.625403, avg_loss=0.181611, seen=53, correct=50, accuracy=0.943396
2025-11-11 17:08:38 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:39 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:39 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:08:40 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.363401, avg_loss=0.109085, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:08:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:41 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 17:08:48 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 17:08:48 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 17:08:48 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 17:08:48 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:48 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:08:49 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=9.974382, avg_loss=0.188196, seen=53, correct=50, accuracy=0.943396
2025-11-11 17:08:49 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:50 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:50 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:08:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:08:51 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.269677, avg_loss=0.106742, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:08:51 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:52 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:08:52 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:08:52 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.950000, curr=0.925000
2025-11-11 17:08:59 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 17:08:59 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 17:08:59 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 17:08:59 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:08:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:08:59 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:09:00 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=9.614365, avg_loss=0.181403, seen=53, correct=49, accuracy=0.924528
2025-11-11 17:09:00 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:01 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:01 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:01 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:09:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:01 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:09:02 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.977814, avg_loss=0.124445, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:09:02 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:03 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:03 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:03 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=3/25), best=0.950000, curr=0.925000
2025-11-11 17:09:10 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 17:09:10 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 17:09:10 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 17:09:10 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:09:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:10 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:09:11 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=16.607977, avg_loss=0.313358, seen=53, correct=50, accuracy=0.943396
2025-11-11 17:09:11 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:12 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:12 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:12 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:09:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:12 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:09:13 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.600098, avg_loss=0.190002, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:09:13 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:13 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:14 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:14 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=4/25), best=0.950000, curr=0.925000
2025-11-11 17:09:20 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 17:09:20 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 17:09:21 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 17:09:21 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:21 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:09:22 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=11.425132, avg_loss=0.215569, seen=53, correct=50, accuracy=0.943396
2025-11-11 17:09:22 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:22 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:23 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:23 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:23 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:09:24 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.711463, avg_loss=0.142787, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:09:24 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:24 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:25 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:25 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:09:32 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 17:09:32 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 17:09:32 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 17:09:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=53)
2025-11-11 17:09:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-11-11 17:09:33 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=53, loss_sum=31.741291, avg_loss=0.598892, seen=53, correct=47, accuracy=0.886792
2025-11-11 17:09:33 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:34 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:34 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:34 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:09:35 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=13.486374, avg_loss=0.337159, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:09:35 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:35 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:36 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:36 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 17:09:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 17:09:36 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 17:09:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:36 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:37 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1579MB
2025-11-11 17:09:37 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 17:09:37 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {}}
2025-11-11 17:09:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:09:37 (federatedscope.llm.trainer.trainer:391) INFO: [mid-eval] every_n_train_steps=10
2025-11-11 17:09:37 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:37 (federatedscope.llm.trainer.trainer:1473) INFO: [STAT] rank=0 emb_dev=cuda:0  will_send_to=cuda:0
2025-11-11 17:09:37 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:09:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:38 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-11-11 17:09:38 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:38 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:09:40 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=26.702015, avg_loss=0.182891, seen=146, correct=135, accuracy=0.924658
2025-11-11 17:09:40 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:41 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:41 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1598MB allocated=1562MB
2025-11-11 17:09:41 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:09:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:41 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:09:42 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=7.071116, avg_loss=0.176778, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:09:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1598MB allocated=1562MB
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:432) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-11-11 17:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:818) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:860) INFO: [force-step-schedule] epoch=1, num_batches=100, grad_accum_step=2 (=> total micro-batches = 200)
2025-11-11 17:09:43 (federatedscope.llm.trainer.trainer:552) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-11-11 17:09:50 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-11-11 17:09:50 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=10
2025-11-11 17:09:50 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-11-11 17:09:50 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:09:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:50 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:09:52 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=18.758163, avg_loss=0.128481, seen=146, correct=137, accuracy=0.938356
2025-11-11 17:09:52 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:53 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:54 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:09:54 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:54 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:09:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:09:55 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.180604, avg_loss=0.104515, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:09:55 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:09:55 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:09:56 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:09:56 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:10:02 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-11-11 17:10:02 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=20
2025-11-11 17:10:02 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-11-11 17:10:03 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:10:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:03 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:10:05 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=17.253580, avg_loss=0.118175, seen=146, correct=136, accuracy=0.931507
2025-11-11 17:10:05 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:06 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:06 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:06 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:10:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:06 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:10:07 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=5.212849, avg_loss=0.130321, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:10:07 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:08 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:08 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 17:10:14 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-11-11 17:10:14 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=30
2025-11-11 17:10:14 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-11-11 17:10:15 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:15 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:10:17 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=17.404591, avg_loss=0.119210, seen=146, correct=139, accuracy=0.952055
2025-11-11 17:10:17 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:18 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:18 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:19 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:19 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:10:19 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=4.251919, avg_loss=0.106298, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:10:19 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:20 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:20 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:20 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:10:27 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-11-11 17:10:27 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=40
2025-11-11 17:10:27 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-11-11 17:10:27 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:10:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:27 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:10:29 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=15.654932, avg_loss=0.107226, seen=146, correct=139, accuracy=0.952055
2025-11-11 17:10:29 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:30 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:31 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:31 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:31 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:10:31 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.619314, avg_loss=0.090483, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:10:31 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:32 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:32 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:10:39 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-11-11 17:10:39 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=50
2025-11-11 17:10:39 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-11-11 17:10:39 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:10:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:39 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:10:41 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=20.401644, avg_loss=0.139737, seen=146, correct=131, accuracy=0.897260
2025-11-11 17:10:41 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:42 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:43 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:43 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:43 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:10:43 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.126271, avg_loss=0.153157, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:10:43 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:44 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:44 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 17:10:51 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-11-11 17:10:51 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=60
2025-11-11 17:10:51 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-11-11 17:10:51 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:10:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:51 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:10:53 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=14.284297, avg_loss=0.097838, seen=146, correct=142, accuracy=0.972603
2025-11-11 17:10:53 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:54 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:55 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:55 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:55 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:10:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:10:56 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=2.867927, avg_loss=0.071698, seen=40, correct=38, accuracy=0.950000
2025-11-11 17:10:56 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:10:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:10:56 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:10:57 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:10:57 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.950000
2025-11-11 17:11:03 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-11-11 17:11:03 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=70
2025-11-11 17:11:03 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-11-11 17:11:04 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:11:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:04 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:11:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:11:06 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=21.421526, avg_loss=0.146723, seen=146, correct=135, accuracy=0.924658
2025-11-11 17:11:06 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:11:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:07 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:07 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:07 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:11:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:07 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:11:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:11:08 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=6.421150, avg_loss=0.160529, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:11:08 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:11:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:09 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:09 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:09 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=1/25), best=0.950000, curr=0.925000
2025-11-11 17:11:16 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-11-11 17:11:16 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=80
2025-11-11 17:11:16 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-11-11 17:11:16 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:16 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:11:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:11:18 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=22.151602, avg_loss=0.151723, seen=146, correct=137, accuracy=0.938356
2025-11-11 17:11:18 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:19 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:19 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:20 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:20 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:11:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:11:20 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=3.825461, avg_loss=0.095637, seen=40, correct=37, accuracy=0.925000
2025-11-11 17:11:20 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:11:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:21 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:21 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:21 (federatedscope.llm.trainer.trainer:1676) INFO: [EarlyStop] no improvement (wait=2/25), best=0.950000, curr=0.925000
2025-11-11 17:11:28 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-11-11 17:11:28 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=90
2025-11-11 17:11:28 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-11-11 17:11:28 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:11:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:28 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:11:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:11:30 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=11.814306, avg_loss=0.080920, seen=146, correct=142, accuracy=0.972603
2025-11-11 17:11:30 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:31 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:32 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:32 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:11:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:32 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:11:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:11:32 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.746987, avg_loss=0.043675, seen=40, correct=39, accuracy=0.975000
2025-11-11 17:11:32 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:11:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:33 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:33 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:33 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 17:11:40 (federatedscope.llm.trainer.trainer:1113) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-11-11 17:11:40 (federatedscope.llm.trainer.trainer:1227) INFO: [MID-EVAL-TRIGGER] at optimizer_step=100
2025-11-11 17:11:40 (federatedscope.llm.trainer.trainer:1612) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-11-11 17:11:40 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-11-11 17:11:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:40 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:11:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-11-11 17:11:42 (federatedscope.llm.trainer.trainer:1265) INFO: [val|final] total=146, loss_sum=11.702421, avg_loss=0.080154, seen=146, correct=142, accuracy=0.972603
2025-11-11 17:11:42 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:11:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:43 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:44 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:44 (federatedscope.llm.trainer.trainer:317) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-11-11 17:11:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:44 (federatedscope.llm.trainer.trainer:839) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-11-11 17:11:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-11-11 17:11:45 (federatedscope.llm.trainer.trainer:1265) INFO: [test|final] total=40, loss_sum=1.955885, avg_loss=0.048897, seen=40, correct=39, accuracy=0.975000
2025-11-11 17:11:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'data_batch']
2025-11-11 17:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:45 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:45 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:45 (federatedscope.llm.trainer.trainer:1671) INFO: [EarlyStop] new best test_acc=0.975000
2025-11-11 17:11:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-11-11 17:11:45 (federatedscope.llm.trainer.trainer:1293) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-11-11 17:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=54425288704 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-11-11 17:11:46 (federatedscope.llm.trainer.trainer:1317) INFO: Accelerator memory has been freed (object preserved).
2025-11-11 17:11:46 (federatedscope.llm.trainer.trainer:1340) INFO: [VRAM] round=0 reserved=1642MB allocated=1595MB
2025-11-11 17:11:46 (federatedscope.core.workers.client:457) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-11-11 17:11:46 (federatedscope.core.workers.client:636) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {}}
2025-11-11 17:11:46 (federatedscope.core.workers.server:550) INFO: [in-place aggregation ] round=0
2025-11-11 17:11:47 (federatedscope.core.workers.server:434) INFO: Server: Training is finished! (skip final evaluation)
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 76.53119258333332, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 3336096, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 1 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 76.53154931666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 2 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 76.49154863333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 3 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 76.46035655, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 4 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 76.43004048333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 5 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 76.39951945, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 6 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 76.36806005, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 7 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 76.33667626666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 8 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 76.30500396666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 9 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 76.27331101666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 10 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 76.24118466666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 11 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 76.20941235000001, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 12 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 76.17773615, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 13 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 76.14611918333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 14 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 76.11438533333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 15 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 76.08336441666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 16 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 76.05139028333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 17 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 76.01925983333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 18 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 75.98782541666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 19 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 75.95611873333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 20 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 75.92460028333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 21 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 75.89297658333334, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 22 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 75.86141395, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 23 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 75.82991396666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 24 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 75.79832676666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 25 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 75.7673832, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 26 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 75.73575745, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 27 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 75.70417598333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 28 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 75.67264279999999, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 29 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 75.64079216666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 30 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 75.60916446666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 31 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 75.57869386666667, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 32 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 75.54832486666666, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 33 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 75.5186588, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 34 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 75.48703915, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.workers.client:836) INFO: ================= client 35 received finish message =================
2025-11-11 17:11:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=336 skipped=0 missing=291 unexpected=0
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 75.45650628333333, 'total_model_size': 498172032, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 112928, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 76.00390070185186, 'sys_avg/total_model_size': '461.9M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '197.72K', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-11-11 17:11:48 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.32646856323190815, 'sys_std/total_model_size': '78.07M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '517.27K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
