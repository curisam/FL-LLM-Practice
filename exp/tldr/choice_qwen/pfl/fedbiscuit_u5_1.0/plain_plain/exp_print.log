2025-09-14 02:08:21 (root:426) INFO: [logger] file handler -> exp/tldr/choice_qwen/pfl/fedbiscuit_u5_1.0/plain_plain/exp_print.log
2025-09-14 02:08:21 (root:51) INFO: [main] outdir=exp/tldr/choice_qwen/pfl/fedbiscuit_u5_1.0/plain_plain
2025-09-14 02:08:45 (federatedscope.core.data.base_translator:234) INFO: Main process: Completion file found. Skipping generation.
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:264) INFO: [Final Split Summary][loaded][server=0][rank=0/4] Train=92858, Val=33082, Test=50715, Total=176655
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=1][rank=0/4] Train=2793, Val=146, Test=40, Total=2979
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=2][rank=0/4] Train=214, Val=11, Test=40, Total=265
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=3][rank=0/4] Train=691, Val=36, Test=40, Total=767
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=4][rank=0/4] Train=213, Val=11, Test=40, Total=264
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=5][rank=0/4] Train=285, Val=14, Test=40, Total=339
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=6][rank=0/4] Train=2547, Val=134, Test=40, Total=2721
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=7][rank=0/4] Train=1088, Val=57, Test=40, Total=1185
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=8][rank=0/4] Train=1316, Val=69, Test=40, Total=1425
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=9][rank=0/4] Train=3572, Val=188, Test=40, Total=3800
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=10][rank=0/4] Train=1209, Val=63, Test=40, Total=1312
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=11][rank=0/4] Train=621, Val=32, Test=40, Total=693
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=12][rank=0/4] Train=2605, Val=137, Test=40, Total=2782
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=13][rank=0/4] Train=1372, Val=72, Test=40, Total=1484
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=14][rank=0/4] Train=3055, Val=160, Test=40, Total=3255
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=15][rank=0/4] Train=14550, Val=200, Test=40, Total=14790
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=16][rank=0/4] Train=2589, Val=136, Test=40, Total=2765
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=17][rank=0/4] Train=5883, Val=200, Test=40, Total=6123
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=18][rank=0/4] Train=2576, Val=135, Test=40, Total=2751
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=19][rank=0/4] Train=2102, Val=110, Test=40, Total=2252
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=20][rank=0/4] Train=2399, Val=126, Test=40, Total=2565
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=21][rank=0/4] Train=2915, Val=153, Test=40, Total=3108
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=22][rank=0/4] Train=224, Val=11, Test=40, Total=275
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=23][rank=0/4] Train=583, Val=30, Test=40, Total=653
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=24][rank=0/4] Train=4944, Val=200, Test=40, Total=5184
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=25][rank=0/4] Train=4647, Val=200, Test=40, Total=4887
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=26][rank=0/4] Train=3063, Val=161, Test=40, Total=3264
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=27][rank=0/4] Train=2342, Val=123, Test=40, Total=2505
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=28][rank=0/4] Train=1434, Val=75, Test=40, Total=1549
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=29][rank=0/4] Train=6191, Val=200, Test=40, Total=6431
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=30][rank=0/4] Train=3247, Val=170, Test=40, Total=3457
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=31][rank=0/4] Train=3679, Val=193, Test=40, Total=3912
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=32][rank=0/4] Train=2144, Val=112, Test=40, Total=2296
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=33][rank=0/4] Train=1409, Val=74, Test=40, Total=1523
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=34][rank=0/4] Train=4486, Val=200, Test=40, Total=4726
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=35][rank=0/4] Train=4736, Val=200, Test=40, Total=4976
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=36][rank=0/4] Train=1030, Val=54, Test=40, Total=1124
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=37][rank=0/4] Train=4273, Val=200, Test=40, Total=4513
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=38][rank=0/4] Train=6171, Val=200, Test=40, Total=6411
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=39][rank=0/4] Train=1594, Val=83, Test=40, Total=1717
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=40][rank=0/4] Train=4005, Val=200, Test=40, Total=4245
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=41][rank=0/4] Train=2275, Val=119, Test=40, Total=2434
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=42][rank=0/4] Train=5772, Val=200, Test=40, Total=6012
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=43][rank=0/4] Train=1694, Val=89, Test=40, Total=1823
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=44][rank=0/4] Train=7916, Val=200, Test=40, Total=8156
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=45][rank=0/4] Train=1901, Val=100, Test=40, Total=2041
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=46][rank=0/4] Train=2100, Val=110, Test=40, Total=2250
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=47][rank=0/4] Train=2812, Val=147, Test=40, Total=2999
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=48][rank=0/4] Train=880, Val=46, Test=40, Total=966
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=49][rank=0/4] Train=2521, Val=132, Test=40, Total=2693
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=50][rank=0/4] Train=2527, Val=133, Test=40, Total=2700
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=51][rank=0/4] Train=1580, Val=83, Test=40, Total=1703
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=52][rank=0/4] Train=3589, Val=188, Test=40, Total=3817
2025-09-14 02:09:27 (federatedscope.core.data.base_translator:273) INFO: [Final Split Summary][loaded][client=53][rank=0/4] Train=6791, Val=200, Test=40, Total=7031
2025-09-14 02:09:28 (federatedscope.core.configs.config:256) INFO: the used configs are: 
adapter:
  use: False
aggregator:
  BFT_args:
    
  byzantine_node_num: 0
  inside_weight: 1.0
  num_agg_groups: 1
  num_agg_topk: []
  outside_weight: 0.0
  robust_rule: fedavg
asyn:
  use: False
attack:
  alpha_TV: 0.001
  alpha_prop_loss: 0
  attack_method: 
  attacker_id: -1
  classifier_PIA: randomforest
  edge_num: 100
  edge_path: edge_data/
  freq: 10
  info_diff_type: l2
  inject_round: 0
  insert_round: 100000
  label_type: dirty
  max_ite: 400
  mean: [0.9637]
  mia_is_simulate_in: False
  mia_simulate_in_round: 20
  pgd_eps: 2
  pgd_lr: 0.1
  pgd_poisoning: False
  poison_ratio: 0.5
  reconstruct_lr: 0.01
  reconstruct_optim: Adam
  scale_para: 1.0
  scale_poisoning: False
  self_epoch: 6
  self_lr: 0.05
  self_opt: False
  setting: fix
  std: [0.1592]
  target_label_ind: -1
  trigger_path: trigger/
  trigger_type: edge
backend: torch
cfg_file: 
check_completeness: False
criterion:
  type: CrossEntropyLoss
data:
  args: []
  batch_size: 64
  cSBM_phi: [0.5, 0.5, 0.5]
  cache_dir: 
  consistent_label_distribution: True
  drop_last: False
  file_path: 
  hetero_data_name: []
  hetero_synth_batch_size: 32
  hetero_synth_feat_dim: 128
  hetero_synth_prim_weight: 0.5
  is_debug: False
  load_splits: False
  loader: 
  max_query_len: 128
  max_seq_len: 384
  max_tgt_len: 128
  num_contrast: 0
  num_of_client_for_data: []
  num_steps: 30
  num_workers: 0
  pre_transform: []
  quadratic:
    dim: 1
    max_curv: 12.5
    min_curv: 0.02
  root: data/
  save_data: False
  save_splits: False
  server_holds_all: False
  shuffle: True
  sizes: [10, 5]
  splits: [0.9, 0.09, 0.01]
  splits_path: ./final_data_splits
  splitter: meta
  splitter_args: []
  subsample: 1.0
  target_transform: []
  test_pre_transform: []
  test_target_transform: []
  test_transform: []
  transform: []
  trunc_stride: 128
  type: reddit-tldr-comparison-choice@llm
  val_pre_transform: []
  val_target_transform: []
  val_transform: []
  walk_length: 2
dataloader:
  batch_size: 2
  drop_last: False
  num_steps: 30
  num_workers: 0
  pin_memory: False
  shuffle: True
  sizes: [10, 5]
  theta: -1
  type: base
  walk_length: 2
device: 0
distribute:
  use: False
early_stop:
  delta: 0.0
  improve_indicator_mode: best
  patience: 0
eval:
  baseline_before_ft: True
  best_res_update_round_wise_key: val_loss
  count_flops: False
  every_n_train_steps: 10
  freq: 1
  metrics: ['loss', 'acc']
  monitoring: []
  outdir: exp/tldr/choice_qwen/pfl/fedbiscuit_u5_1.0/plain_plain/raw
  report: ['weighted_avg', 'avg', 'fairness', 'raw']
  split: ['val', 'test']
expname: 
expname_tag: 
feat_engr:
  num_bins: 5
  scenario: hfl
  secure:
    dp:
      
    encrypt:
      type: dummy
    key_size: 3072
    type: encrypt
  selec_threshold: 0.05
  selec_woe_binning: quantile
  type: 
federate:
  atc_load_from: 
  atc_vanilla: False
  client_idx_for_local_train: 0
  client_num: 53
  data_weighted_aggr: False
  ignore_weight: True
  join_in_info: []
  make_global_eval: False
  master_addr: 127.0.0.1
  master_port: 29500
  merge_test_data: False
  merge_val_data: False
  method: FedAvg
  mode: standalone
  online_aggr: False
  process_num: 1
  resource_info_file: 
  restore_from: 
  sample_client_num: 53
  sample_client_rate: -1.0
  sampler: uniform
  save_client_model: False
  save_freq: -1
  save_to: 
  share_local_model: True
  total_round_num: 1
  unseen_clients_rate: 0.0
  use_diff: False
  use_ss: False
fedopt:
  use: False
fedprox:
  use: False
fedsageplus:
  a: 1.0
  b: 1.0
  c: 1.0
  fedgen_epoch: 200
  gen_hidden: 128
  hide_portion: 0.5
  loc_epoch: 1
  num_pred: 5
fedswa:
  use: False
finetune:
  batch_or_epoch: epoch
  before_eval: False
  epoch_linear: 10
  freeze_param: 
  local_param: []
  local_update_steps: 1
  lr_linear: 0.005
  optimizer:
    lr: 0.1
    type: SGD
  scheduler:
    type: 
    warmup_ratio: 0.0
  simple_tuning: False
  weight_decay: 0.0
flitplus:
  factor_ema: 0.8
  lambdavat: 0.5
  tmpFed: 0.5
  weightReg: 1.0
gcflplus:
  EPS_1: 0.05
  EPS_2: 0.1
  seq_length: 5
  standardize: False
grad:
  grad_accum_count: 1
  grad_clip: -1.0
hpo:
  fedex:
    cutoff: 0.0
    diff: False
    eta0: -1.0
    flatten_ss: True
    gamma: 0.0
    pi_lr: 0.01
    psn: False
    sched: auto
    ss: 
    use: False
  fts:
    M: 100
    M_target: 200
    allow_load_existing_info: True
    diff: False
    fed_bo_max_iter: 50
    g_var: 1e-06
    gp_opt_schedule: 1
    local_bo_epochs: 50
    local_bo_max_iter: 50
    ls: 1.0
    obs_noise: 1e-06
    ss: 
    target_clients: []
    use: False
    v_kernel: 1.0
    var: 0.1
  init_cand_num: 16
  larger_better: False
  metric: client_summarized_weighted_avg.val_loss
  num_workers: 0
  pbt:
    max_stage: 5
    perf_threshold: 0.1
  pfedhpo:
    discrete: False
    ss: 
    target_fl_total_round: 1000
    train_anchor: False
    train_fl: False
    use: False
  scheduler: rs
  sha:
    budgets: []
    elim_rate: 3
    iter: 0
  ss: 
  table:
    eps: 0.1
    idx: 0
    num: 27
  trial_index: 0
  working_folder: hpo
llm:
  accelerator:
    config: 
    use: True
  adapter:
    args: [{'adapter_package': 'peft', 'adapter_method': 'lora', 'r': 8, 'lora_alpha': 16, 'lora_dropout': 0.05, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']}]
    balance: False
    count: 5
    grouping:
      round: 0
      use: True
    local_only: False
    mv_to_cpu: False
    use: True
    warmup:
      round: 10
      use: False
  cache:
    model: 
  chat:
    max_history_len: 10
    max_len: 1024
  deepspeed:
    ds_config: 
    use: False
  fedrlhf:
    config_file: 
    frequency: 100
    pretrained: False
    train:
      batch_or_epoch: batch
      local_update_steps: 10
    use: False
  grad_accum_step: 2
  max_new_token: 60
  num_completions: 2
  offsite_tuning:
    emu_align:
      data:
        root: data
        splits: [0.8, 0.1, 0.1]
        type: alpaca@llm
      exit_after_align: False
      init_enable_ground_truth: False
      initial_only: True
      kl_divergence: raw
      layerwise_distill: False
      restore_from: 
      save_to: 
      sim_loss: l2
      train:
        batch_or_epoch: batch
        enable_ground_truth: False
        initial_update_rounds: 50
        kd_loss_weight: 0.9
        lm_loss_weight: 0.1
        local_update_steps: 10
        optimizer:
          lr: 0.01
          type: SGD
      use: False
    emu_l: 1
    emu_r: 10
    eval_type: emu
    kwargs: [{}]
    llm_generated:
      ratio: 0.1
      use: False
    save_full_model: False
    strategy: drop_layer
    use: False
  retry_on_nan_loss: False
  reward_coeff: 0.1
  rlhf: False
  tok_len: 1024
model:
  contrast_temp: 1.0
  contrast_topk: 100
  downstream_tasks: []
  dropout: 0.5
  embed_size: 8
  gamma: 0
  graph_pooling: mean
  hidden: 256
  in_channels: 0
  input_shape: ()
  label_smoothing: 0.1
  lambda_: 0.1
  layer: 2
  length_penalty: 2.0
  llm_kwargs: [{}]
  llm_type: CausalLM
  load_from_local_pretrained_fs_config: 
  load_from_local_pretrained_model_path: checkpoints_1.0/final_tldr_choice_qwen_fedbiscuit_u5_plain_round_175.ckpt
  max_answer_len: 30
  max_length: 200
  max_tree_depth: 3
  min_length: 1
  model_num_per_trainer: 1
  model_type: google/bert_uncased_L-2_H-128_A-2
  n_best_size: 20
  no_repeat_ngram_size: 3
  null_score_diff_threshold: 0.0
  num_beams: 5
  num_item: 0
  num_labels: 1
  num_of_trees: 10
  num_user: 0
  out_channels: 1
  pretrain_tasks: []
  stage: 
  task: node
  type: Qwen/Qwen2-0.5B@huggingface_llm
  use_bias: True
  use_contrastive_loss: False
nbafl:
  use: False
outdir: exp/tldr/choice_qwen/pfl/fedbiscuit_u5_1.0/plain_plain
personalization:
  K: 5
  beta: 1.0
  epoch_feature: 1
  epoch_linear: 2
  local_param: []
  local_update_steps: 100
  lr: 1e-05
  lr_feature: 0.1
  lr_linear: 0.1
  regular_weight: 0.1
  share_non_trainable_para: False
  weight_decay: 0.0
print_decimal_digits: 6
quantization:
  method: none
  nbits: 8
regularizer:
  mu: 0.0
  type: 
seed: 0
sgdmf:
  use: False
train:
  batch_or_epoch: batch
  data_para_dids: []
  is_enable_half: True
  local_update_steps: 100
  optimizer:
    betas: (0.9, 0.95)
    lr: 1e-05
    type: AdamW
  scheduler:
    type: 
    warmup_ratio: 0.0
trainer:
  choices: ['A', 'B']
  disp_freq: 50
  local_entropy:
    alpha: 0.75
    eps: 0.0001
    gamma: 0.03
    inc_factor: 1.0
  sam:
    adaptive: False
    eta: 0.0
    rho: 1.0
  type: llmrewardchoicetrainer
  val_freq: 100000000
use_gpu: True
verbose: 1
vertical:
  use: False
wandb:
  use: False
2025-09-14 02:09:29 (federatedscope.core.auxiliaries.utils:175) INFO: The device information file is not provided
2025-09-14 02:09:29 (federatedscope.core.auxiliaries.model_builder:139) WARNING: The input shape is None. Please specify the `data.input_shape`(a tuple) or give the representative data to `get_model` if necessary
2025-09-14 02:09:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-build][rank=0] tok_len=151643 | base=Qwen2ForCausalLM | in_emb=(Embedding) num=151646 ptr=139866382622784 | out_emb=(Linear) num=151646 ptr=139866382622784 | lora_ptr=None
2025-09-14 02:09:43 (federatedscope.llm.model.model_builder:187) INFO: [Warmup-Init] loaded from checkpoints_1.0/final_tldr_choice_qwen_fedbiscuit_u5_plain_round_175.ckpt (round=175) | missing=291 unexpected=0
2025-09-14 02:09:43 (federatedscope.core.fed_runner:211) INFO: Server has been set up ... 
2025-09-14 02:09:44 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:09:47 (federatedscope.core.fed_runner:275) INFO: Client 1 has been set up ... 
2025-09-14 02:09:47 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:09:49 (federatedscope.core.fed_runner:275) INFO: Client 2 has been set up ... 
2025-09-14 02:09:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:09:52 (federatedscope.core.fed_runner:275) INFO: Client 3 has been set up ... 
2025-09-14 02:09:52 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:09:54 (federatedscope.core.fed_runner:275) INFO: Client 4 has been set up ... 
2025-09-14 02:09:54 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:09:57 (federatedscope.core.fed_runner:275) INFO: Client 5 has been set up ... 
2025-09-14 02:09:57 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:09:59 (federatedscope.core.fed_runner:275) INFO: Client 6 has been set up ... 
2025-09-14 02:09:59 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:02 (federatedscope.core.fed_runner:275) INFO: Client 7 has been set up ... 
2025-09-14 02:10:02 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:04 (federatedscope.core.fed_runner:275) INFO: Client 8 has been set up ... 
2025-09-14 02:10:04 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:06 (federatedscope.core.fed_runner:275) INFO: Client 9 has been set up ... 
2025-09-14 02:10:07 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:09 (federatedscope.core.fed_runner:275) INFO: Client 10 has been set up ... 
2025-09-14 02:10:09 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:11 (federatedscope.core.fed_runner:275) INFO: Client 11 has been set up ... 
2025-09-14 02:10:11 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:14 (federatedscope.core.fed_runner:275) INFO: Client 12 has been set up ... 
2025-09-14 02:10:14 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:17 (federatedscope.core.fed_runner:275) INFO: Client 13 has been set up ... 
2025-09-14 02:10:17 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:20 (federatedscope.core.fed_runner:275) INFO: Client 14 has been set up ... 
2025-09-14 02:10:20 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:22 (federatedscope.core.fed_runner:275) INFO: Client 15 has been set up ... 
2025-09-14 02:10:22 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:25 (federatedscope.core.fed_runner:275) INFO: Client 16 has been set up ... 
2025-09-14 02:10:25 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:27 (federatedscope.core.fed_runner:275) INFO: Client 17 has been set up ... 
2025-09-14 02:10:27 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:30 (federatedscope.core.fed_runner:275) INFO: Client 18 has been set up ... 
2025-09-14 02:10:30 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:32 (federatedscope.core.fed_runner:275) INFO: Client 19 has been set up ... 
2025-09-14 02:10:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:34 (federatedscope.core.fed_runner:275) INFO: Client 20 has been set up ... 
2025-09-14 02:10:35 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:37 (federatedscope.core.fed_runner:275) INFO: Client 21 has been set up ... 
2025-09-14 02:10:37 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:39 (federatedscope.core.fed_runner:275) INFO: Client 22 has been set up ... 
2025-09-14 02:10:39 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:42 (federatedscope.core.fed_runner:275) INFO: Client 23 has been set up ... 
2025-09-14 02:10:42 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:44 (federatedscope.core.fed_runner:275) INFO: Client 24 has been set up ... 
2025-09-14 02:10:45 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:47 (federatedscope.core.fed_runner:275) INFO: Client 25 has been set up ... 
2025-09-14 02:10:47 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:50 (federatedscope.core.fed_runner:275) INFO: Client 26 has been set up ... 
2025-09-14 02:10:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:53 (federatedscope.core.fed_runner:275) INFO: Client 27 has been set up ... 
2025-09-14 02:10:53 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:55 (federatedscope.core.fed_runner:275) INFO: Client 28 has been set up ... 
2025-09-14 02:10:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:10:58 (federatedscope.core.fed_runner:275) INFO: Client 29 has been set up ... 
2025-09-14 02:10:58 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:00 (federatedscope.core.fed_runner:275) INFO: Client 30 has been set up ... 
2025-09-14 02:11:00 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:03 (federatedscope.core.fed_runner:275) INFO: Client 31 has been set up ... 
2025-09-14 02:11:03 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:05 (federatedscope.core.fed_runner:275) INFO: Client 32 has been set up ... 
2025-09-14 02:11:05 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:08 (federatedscope.core.fed_runner:275) INFO: Client 33 has been set up ... 
2025-09-14 02:11:08 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:10 (federatedscope.core.fed_runner:275) INFO: Client 34 has been set up ... 
2025-09-14 02:11:10 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:13 (federatedscope.core.fed_runner:275) INFO: Client 35 has been set up ... 
2025-09-14 02:11:13 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:15 (federatedscope.core.fed_runner:275) INFO: Client 36 has been set up ... 
2025-09-14 02:11:16 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:18 (federatedscope.core.fed_runner:275) INFO: Client 37 has been set up ... 
2025-09-14 02:11:18 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:20 (federatedscope.core.fed_runner:275) INFO: Client 38 has been set up ... 
2025-09-14 02:11:21 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:24 (federatedscope.core.fed_runner:275) INFO: Client 39 has been set up ... 
2025-09-14 02:11:24 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:27 (federatedscope.core.fed_runner:275) INFO: Client 40 has been set up ... 
2025-09-14 02:11:27 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:29 (federatedscope.core.fed_runner:275) INFO: Client 41 has been set up ... 
2025-09-14 02:11:29 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:32 (federatedscope.core.fed_runner:275) INFO: Client 42 has been set up ... 
2025-09-14 02:11:32 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:34 (federatedscope.core.fed_runner:275) INFO: Client 43 has been set up ... 
2025-09-14 02:11:34 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:37 (federatedscope.core.fed_runner:275) INFO: Client 44 has been set up ... 
2025-09-14 02:11:37 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:39 (federatedscope.core.fed_runner:275) INFO: Client 45 has been set up ... 
2025-09-14 02:11:40 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:42 (federatedscope.core.fed_runner:275) INFO: Client 46 has been set up ... 
2025-09-14 02:11:42 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:44 (federatedscope.core.fed_runner:275) INFO: Client 47 has been set up ... 
2025-09-14 02:11:45 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:47 (federatedscope.core.fed_runner:275) INFO: Client 48 has been set up ... 
2025-09-14 02:11:47 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:50 (federatedscope.core.fed_runner:275) INFO: Client 49 has been set up ... 
2025-09-14 02:11:50 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:52 (federatedscope.core.fed_runner:275) INFO: Client 50 has been set up ... 
2025-09-14 02:11:52 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:55 (federatedscope.core.fed_runner:275) INFO: Client 51 has been set up ... 
2025-09-14 02:11:55 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:11:57 (federatedscope.core.fed_runner:275) INFO: Client 52 has been set up ... 
2025-09-14 02:11:57 (federatedscope.llm.trainer.trainer:179) INFO: Choice token IDs: [362, 425]
2025-09-14 02:12:01 (federatedscope.core.fed_runner:275) INFO: Client 53 has been set up ... 
2025-09-14 02:12:01 (federatedscope.core.trainers.trainer:569) INFO: Model meta-info: <class 'federatedscope.llm.model.adapter_builder.AdapterModel'>.
2025-09-14 02:12:01 (federatedscope.core.trainers.trainer:584) INFO: Num of original para names: 2016.
2025-09-14 02:12:01 (federatedscope.core.trainers.trainer:585) INFO: Num of original trainable para names: 2306.
2025-09-14 02:12:01 (federatedscope.core.trainers.trainer:587) INFO: Num of preserved para names in local update: 2016. 
Preserved para names in local update: {'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.22.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.16.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.18.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.23.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.16.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.18.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.20.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.17.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.o_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.21.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.22.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.22.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.21.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.23.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.23.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.16.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.mlp.gate_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.17.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.20.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.o_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.18.mlp.up_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.v_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.16.self_attn.q_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.20.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.23.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.20.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.21.self_attn.q_proj.lora_B.Adapter_3.weight', 'base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.19.mlp.gate_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.18.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.22.mlp.up_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.Adapter_4.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.19.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.17.mlp.gate_proj.lora_B.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_2.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.Adapter_0.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.Adapter_1.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.21.mlp.down_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.19.self_attn.o_proj.lora_B.Adapter_4.weight', 'base_model.model.model.layers.20.self_attn.k_proj.lora_A.Adapter_0.weight', 'base_model.model.model.layers.17.self_attn.q_proj.lora_A.Adapter_3.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.Adapter_2.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.Adapter_0.weight'}.
2025-09-14 02:12:01 (federatedscope.core.trainers.trainer:591) INFO: Num of filtered para names in local update: 0. 
Filtered para names in local update: set().
2025-09-14 02:12:01 (federatedscope.core.trainers.trainer:599) INFO: After register default hooks,
	the hooks_in_train is:
	{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init",
	    "_hook_on_fit_start_calculate_model_size"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward",
	    "_hook_on_batch_forward_regularizer",
	    "_hook_on_batch_forward_flop_count"
	  ],
	  "on_batch_backward": [
	    "_hook_on_batch_backward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	};
	the hooks_in_eval is:
            t{
	  "on_fit_start": [
	    "_hook_on_fit_start_numerical_precision",
	    "_hook_on_data_parallel_init",
	    "_hook_on_fit_start_init"
	  ],
	  "on_batch_start": [
	    "_hook_on_batch_start_init"
	  ],
	  "on_batch_forward": [
	    "_hook_on_batch_forward"
	  ],
	  "on_batch_end": [
	    "_hook_on_batch_end"
	  ],
	  "on_fit_end": [
	    "_hook_on_fit_end",
	    "_hook_on_fit_end_free_space"
	  ]
	}
2025-09-14 02:12:01 (federatedscope.llm.llm_local.server:103) INFO: Waited all clients join, start now...
2025-09-14 02:12:01 (federatedscope.llm.llm_local.server:111) INFO: ----------- Starting training (Round #0) -------------
2025-09-14 02:12:01 (federatedscope.llm.llm_local.server:114) INFO: Server: Performing a grouping step...
2025-09-14 02:12:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:12:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 02:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139866382622784 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:12:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=98.315788, avg_loss=0.673396, seen=146, correct=82, accuracy=0.561644
2025-09-14 02:12:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2040MB allocated=1785MB
2025-09-14 02:12:21 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 0 with val results: {'val_total': 146, 'val_loss': 98.31578826904297, 'val_avg_loss': 0.6733958100619382, 'val_seen': 146, 'val_correct': 82, 'val_acc': 0.5616438356164384}
2025-09-14 02:12:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:12:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:12:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.688755, avg_loss=0.617219, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:12:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2040MB allocated=1785MB
2025-09-14 02:12:23 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.68875503540039, 'test_avg_loss': 0.6172188758850098, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:12:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 02:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:12:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.267143, avg_loss=0.659364, seen=146, correct=87, accuracy=0.595890
2025-09-14 02:12:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1743MB
2025-09-14 02:12:29 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 1 with val results: {'val_total': 146, 'val_loss': 96.26714324951172, 'val_avg_loss': 0.6593639948596693, 'val_seen': 146, 'val_correct': 87, 'val_acc': 0.5958904109589042}
2025-09-14 02:12:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:12:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:12:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.258568, avg_loss=0.656464, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:12:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1743MB
2025-09-14 02:12:31 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.258567810058594, 'test_avg_loss': 0.6564641952514648, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:12:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 02:12:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:12:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=99.842880, avg_loss=0.683855, seen=146, correct=77, accuracy=0.527397
2025-09-14 02:12:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1743MB
2025-09-14 02:12:37 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 2 with val results: {'val_total': 146, 'val_loss': 99.84288024902344, 'val_avg_loss': 0.6838553441713934, 'val_seen': 146, 'val_correct': 77, 'val_acc': 0.5273972602739726}
2025-09-14 02:12:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:12:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:12:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.998550, avg_loss=0.674964, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:12:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1743MB
2025-09-14 02:12:40 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.998550415039062, 'test_avg_loss': 0.6749637603759766, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:12:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 02:12:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:12:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=104.681641, avg_loss=0.716998, seen=146, correct=81, accuracy=0.554795
2025-09-14 02:12:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1743MB
2025-09-14 02:12:46 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 3 with val results: {'val_total': 146, 'val_loss': 104.681640625, 'val_avg_loss': 0.7169975385273972, 'val_seen': 146, 'val_correct': 81, 'val_acc': 0.5547945205479452}
2025-09-14 02:12:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:12:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:12:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.221863, avg_loss=0.680547, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:12:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1743MB
2025-09-14 02:12:49 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 3 with test results: {'test_total': 40, 'test_loss': 27.22186279296875, 'test_avg_loss': 0.6805465698242188, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:12:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 02:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:12:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=106.340790, avg_loss=0.728362, seen=146, correct=78, accuracy=0.534247
2025-09-14 02:12:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1743MB
2025-09-14 02:12:55 (federatedscope.llm.llm_local.client:480) INFO: Client 1 Adapter 4 with val results: {'val_total': 146, 'val_loss': 106.34078979492188, 'val_avg_loss': 0.7283615739378211, 'val_seen': 146, 'val_correct': 78, 'val_acc': 0.5342465753424658}
2025-09-14 02:12:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:12:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:12:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:12:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.409985, avg_loss=0.660250, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:12:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:12:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:12:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:12:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1743MB
2025-09-14 02:12:58 (federatedscope.llm.llm_local.client:501) INFO: Client 1 Adapter 4 with test results: {'test_total': 40, 'test_loss': 26.409984588623047, 'test_avg_loss': 0.6602496147155762, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:12:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:12:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:12:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:13:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.466173, avg_loss=0.769652, seen=11, correct=6, accuracy=0.545455
2025-09-14 02:13:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1794MB
2025-09-14 02:13:01 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 0 with val results: {'val_total': 11, 'val_loss': 8.46617317199707, 'val_avg_loss': 0.7696521065451882, 'val_seen': 11, 'val_correct': 6, 'val_acc': 0.5454545454545454}
2025-09-14 02:13:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.172293, avg_loss=0.654307, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:13:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1794MB
2025-09-14 02:13:04 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.172292709350586, 'test_avg_loss': 0.6543073177337646, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:13:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:13:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:13:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.833826, avg_loss=0.712166, seen=11, correct=7, accuracy=0.636364
2025-09-14 02:13:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1752MB
2025-09-14 02:13:06 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.833825588226318, 'val_avg_loss': 0.7121659625660289, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 02:13:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.350964, avg_loss=0.658774, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:13:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1752MB
2025-09-14 02:13:09 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.350963592529297, 'test_avg_loss': 0.6587740898132324, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:13:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:13:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.420978, avg_loss=0.674634, seen=11, correct=7, accuracy=0.636364
2025-09-14 02:13:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1752MB
2025-09-14 02:13:11 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 2 with val results: {'val_total': 11, 'val_loss': 7.420977592468262, 'val_avg_loss': 0.6746343265880238, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 02:13:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.547169, avg_loss=0.638679, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:13:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1752MB
2025-09-14 02:13:13 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.547168731689453, 'test_avg_loss': 0.6386792182922363, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:13:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:13:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.612449, avg_loss=0.692041, seen=11, correct=7, accuracy=0.636364
2025-09-14 02:13:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1752MB
2025-09-14 02:13:15 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 3 with val results: {'val_total': 11, 'val_loss': 7.612448692321777, 'val_avg_loss': 0.6920407902110707, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 02:13:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.635529, avg_loss=0.615888, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:13:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1752MB
2025-09-14 02:13:17 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 3 with test results: {'test_total': 40, 'test_loss': 24.635528564453125, 'test_avg_loss': 0.6158882141113281, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:13:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:13:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.896717, avg_loss=0.808792, seen=11, correct=5, accuracy=0.454545
2025-09-14 02:13:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1752MB
2025-09-14 02:13:19 (federatedscope.llm.llm_local.client:480) INFO: Client 2 Adapter 4 with val results: {'val_total': 11, 'val_loss': 8.896717071533203, 'val_avg_loss': 0.808792461048473, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-14 02:13:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=33.153549, avg_loss=0.828839, seen=40, correct=13, accuracy=0.325000
2025-09-14 02:13:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1752MB
2025-09-14 02:13:21 (federatedscope.llm.llm_local.client:501) INFO: Client 2 Adapter 4 with test results: {'test_total': 40, 'test_loss': 33.15354919433594, 'test_avg_loss': 0.8288387298583985, 'test_seen': 40, 'test_correct': 13, 'test_acc': 0.325}
2025-09-14 02:13:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:13:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 02:13:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 02:13:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=30.516857, avg_loss=0.847690, seen=36, correct=17, accuracy=0.472222
2025-09-14 02:13:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1802MB
2025-09-14 02:13:25 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 0 with val results: {'val_total': 36, 'val_loss': 30.516857147216797, 'val_avg_loss': 0.8476904763115777, 'val_seen': 36, 'val_correct': 17, 'val_acc': 0.4722222222222222}
2025-09-14 02:13:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.807199, avg_loss=0.745180, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:13:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2040MB allocated=1802MB
2025-09-14 02:13:27 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.807199478149414, 'test_avg_loss': 0.7451799869537353, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:13:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 02:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 02:13:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=27.644581, avg_loss=0.767905, seen=36, correct=16, accuracy=0.444444
2025-09-14 02:13:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1760MB
2025-09-14 02:13:30 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 1 with val results: {'val_total': 36, 'val_loss': 27.644580841064453, 'val_avg_loss': 0.7679050233629015, 'val_seen': 36, 'val_correct': 16, 'val_acc': 0.4444444444444444}
2025-09-14 02:13:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.614550, avg_loss=0.715364, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:13:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1760MB
2025-09-14 02:13:33 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.61454963684082, 'test_avg_loss': 0.7153637409210205, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:13:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 02:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 02:13:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.661469, avg_loss=0.740596, seen=36, correct=14, accuracy=0.388889
2025-09-14 02:13:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1760MB
2025-09-14 02:13:37 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 2 with val results: {'val_total': 36, 'val_loss': 26.661468505859375, 'val_avg_loss': 0.7405963473849826, 'val_seen': 36, 'val_correct': 14, 'val_acc': 0.3888888888888889}
2025-09-14 02:13:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.097450, avg_loss=0.702436, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:13:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1760MB
2025-09-14 02:13:39 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.097450256347656, 'test_avg_loss': 0.7024362564086915, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:13:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 02:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 02:13:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=29.929306, avg_loss=0.831370, seen=36, correct=15, accuracy=0.416667
2025-09-14 02:13:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1760MB
2025-09-14 02:13:42 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 3 with val results: {'val_total': 36, 'val_loss': 29.929306030273438, 'val_avg_loss': 0.83136961195204, 'val_seen': 36, 'val_correct': 15, 'val_acc': 0.4166666666666667}
2025-09-14 02:13:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.800713, avg_loss=0.745018, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:13:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1760MB
2025-09-14 02:13:44 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 3 with test results: {'test_total': 40, 'test_loss': 29.80071258544922, 'test_avg_loss': 0.7450178146362305, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:13:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 02:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 02:13:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.983858, avg_loss=0.721774, seen=36, correct=21, accuracy=0.583333
2025-09-14 02:13:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1760MB
2025-09-14 02:13:47 (federatedscope.llm.llm_local.client:480) INFO: Client 3 Adapter 4 with val results: {'val_total': 36, 'val_loss': 25.983858108520508, 'val_avg_loss': 0.7217738363477919, 'val_seen': 36, 'val_correct': 21, 'val_acc': 0.5833333333333334}
2025-09-14 02:13:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.077810, avg_loss=0.676945, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:13:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1760MB
2025-09-14 02:13:50 (federatedscope.llm.llm_local.client:501) INFO: Client 3 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.077810287475586, 'test_avg_loss': 0.6769452571868897, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:13:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:13:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:13:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.094566, avg_loss=0.826779, seen=11, correct=4, accuracy=0.363636
2025-09-14 02:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1811MB
2025-09-14 02:13:52 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 0 with val results: {'val_total': 11, 'val_loss': 9.094566345214844, 'val_avg_loss': 0.8267787586558949, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-14 02:13:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.355743, avg_loss=0.658894, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:13:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2040MB allocated=1811MB
2025-09-14 02:13:54 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.355743408203125, 'test_avg_loss': 0.6588935852050781, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:13:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:13:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:13:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.809393, avg_loss=0.709945, seen=11, correct=5, accuracy=0.454545
2025-09-14 02:13:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:13:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1769MB
2025-09-14 02:13:57 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 1 with val results: {'val_total': 11, 'val_loss': 7.809393405914307, 'val_avg_loss': 0.7099448550831188, 'val_seen': 11, 'val_correct': 5, 'val_acc': 0.45454545454545453}
2025-09-14 02:13:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:13:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:13:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:13:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.769901, avg_loss=0.644248, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:13:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:13:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:13:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1769MB
2025-09-14 02:14:00 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.769901275634766, 'test_avg_loss': 0.6442475318908691, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:14:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:14:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:14:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.682822, avg_loss=0.789347, seen=11, correct=4, accuracy=0.363636
2025-09-14 02:14:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1769MB
2025-09-14 02:14:02 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.682822227478027, 'val_avg_loss': 0.7893474752252753, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-14 02:14:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.193617, avg_loss=0.629840, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:14:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1769MB
2025-09-14 02:14:05 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.19361686706543, 'test_avg_loss': 0.6298404216766358, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:14:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:14:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:14:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.520996, avg_loss=0.865545, seen=11, correct=4, accuracy=0.363636
2025-09-14 02:14:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1769MB
2025-09-14 02:14:07 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 3 with val results: {'val_total': 11, 'val_loss': 9.52099609375, 'val_avg_loss': 0.8655450994318182, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-14 02:14:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.602676, avg_loss=0.640067, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:14:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1769MB
2025-09-14 02:14:09 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 3 with test results: {'test_total': 40, 'test_loss': 25.602676391601562, 'test_avg_loss': 0.6400669097900391, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:14:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:14:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.269842, avg_loss=0.660895, seen=11, correct=7, accuracy=0.636364
2025-09-14 02:14:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1769MB
2025-09-14 02:14:11 (federatedscope.llm.llm_local.client:480) INFO: Client 4 Adapter 4 with val results: {'val_total': 11, 'val_loss': 7.269842147827148, 'val_avg_loss': 0.660894740711559, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 02:14:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.471088, avg_loss=0.761777, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:14:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1769MB
2025-09-14 02:14:13 (federatedscope.llm.llm_local.client:501) INFO: Client 4 Adapter 4 with test results: {'test_total': 40, 'test_loss': 30.471088409423828, 'test_avg_loss': 0.7617772102355957, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:14:13 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:14:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 02:14:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 02:14:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=10.092851, avg_loss=0.720918, seen=14, correct=8, accuracy=0.571429
2025-09-14 02:14:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1819MB
2025-09-14 02:14:16 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 0 with val results: {'val_total': 14, 'val_loss': 10.092850685119629, 'val_avg_loss': 0.7209179060799735, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-14 02:14:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.469072, avg_loss=0.736727, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:14:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1819MB
2025-09-14 02:14:19 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.469072341918945, 'test_avg_loss': 0.7367268085479737, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:14:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 02:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 02:14:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.448443, avg_loss=0.674889, seen=14, correct=9, accuracy=0.642857
2025-09-14 02:14:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1777MB
2025-09-14 02:14:21 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 1 with val results: {'val_total': 14, 'val_loss': 9.448443412780762, 'val_avg_loss': 0.6748888151986259, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-14 02:14:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.812710, avg_loss=0.670318, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:14:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1777MB
2025-09-14 02:14:23 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.81270980834961, 'test_avg_loss': 0.6703177452087402, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:14:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 02:14:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 02:14:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.838845, avg_loss=0.702775, seen=14, correct=7, accuracy=0.500000
2025-09-14 02:14:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1777MB
2025-09-14 02:14:26 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 2 with val results: {'val_total': 14, 'val_loss': 9.838845252990723, 'val_avg_loss': 0.7027746609279087, 'val_seen': 14, 'val_correct': 7, 'val_acc': 0.5}
2025-09-14 02:14:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.429176, avg_loss=0.710729, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:14:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1777MB
2025-09-14 02:14:29 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.429176330566406, 'test_avg_loss': 0.7107294082641602, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:14:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 02:14:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 02:14:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=10.553414, avg_loss=0.753815, seen=14, correct=8, accuracy=0.571429
2025-09-14 02:14:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1777MB
2025-09-14 02:14:31 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 3 with val results: {'val_total': 14, 'val_loss': 10.553414344787598, 'val_avg_loss': 0.7538153103419712, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-14 02:14:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.876556, avg_loss=0.721914, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:14:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1777MB
2025-09-14 02:14:34 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 3 with test results: {'test_total': 40, 'test_loss': 28.876556396484375, 'test_avg_loss': 0.7219139099121094, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:14:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 02:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 02:14:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.825736, avg_loss=0.701838, seen=14, correct=8, accuracy=0.571429
2025-09-14 02:14:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1777MB
2025-09-14 02:14:36 (federatedscope.llm.llm_local.client:480) INFO: Client 5 Adapter 4 with val results: {'val_total': 14, 'val_loss': 9.825736045837402, 'val_avg_loss': 0.7018382889883858, 'val_seen': 14, 'val_correct': 8, 'val_acc': 0.5714285714285714}
2025-09-14 02:14:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.217396, avg_loss=0.655435, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:14:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1777MB
2025-09-14 02:14:39 (federatedscope.llm.llm_local.client:501) INFO: Client 5 Adapter 4 with test results: {'test_total': 40, 'test_loss': 26.217395782470703, 'test_avg_loss': 0.6554348945617676, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:14:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:14:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 02:14:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:14:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=93.477493, avg_loss=0.697593, seen=134, correct=81, accuracy=0.604478
2025-09-14 02:14:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1828MB
2025-09-14 02:14:46 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 0 with val results: {'val_total': 134, 'val_loss': 93.47749328613281, 'val_avg_loss': 0.697593233478603, 'val_seen': 134, 'val_correct': 81, 'val_acc': 0.6044776119402985}
2025-09-14 02:14:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.631985, avg_loss=0.790800, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:14:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1828MB
2025-09-14 02:14:49 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.63198471069336, 'test_avg_loss': 0.790799617767334, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:14:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 02:14:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:14:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.282585, avg_loss=0.643900, seen=134, correct=88, accuracy=0.656716
2025-09-14 02:14:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1786MB
2025-09-14 02:14:55 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 1 with val results: {'val_total': 134, 'val_loss': 86.28258514404297, 'val_avg_loss': 0.643899889134649, 'val_seen': 134, 'val_correct': 88, 'val_acc': 0.6567164179104478}
2025-09-14 02:14:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:14:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:14:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:14:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.395231, avg_loss=0.734881, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:14:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:14:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:14:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1786MB
2025-09-14 02:14:57 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.395231246948242, 'test_avg_loss': 0.734880781173706, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:14:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 02:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:14:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:15:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=90.807205, avg_loss=0.677666, seen=134, correct=73, accuracy=0.544776
2025-09-14 02:15:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1786MB
2025-09-14 02:15:03 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 2 with val results: {'val_total': 134, 'val_loss': 90.80720520019531, 'val_avg_loss': 0.6776657104492188, 'val_seen': 134, 'val_correct': 73, 'val_acc': 0.5447761194029851}
2025-09-14 02:15:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.436464, avg_loss=0.710912, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:15:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1786MB
2025-09-14 02:15:05 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.436464309692383, 'test_avg_loss': 0.7109116077423095, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:15:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 02:15:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:15:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=91.230934, avg_loss=0.680828, seen=134, correct=78, accuracy=0.582090
2025-09-14 02:15:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1786MB
2025-09-14 02:15:11 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 3 with val results: {'val_total': 134, 'val_loss': 91.2309341430664, 'val_avg_loss': 0.6808278667393015, 'val_seen': 134, 'val_correct': 78, 'val_acc': 0.582089552238806}
2025-09-14 02:15:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.794016, avg_loss=0.794850, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:15:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1786MB
2025-09-14 02:15:13 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 3 with test results: {'test_total': 40, 'test_loss': 31.794015884399414, 'test_avg_loss': 0.7948503971099854, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:15:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 02:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:15:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=96.923088, avg_loss=0.723307, seen=134, correct=75, accuracy=0.559701
2025-09-14 02:15:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1786MB
2025-09-14 02:15:20 (federatedscope.llm.llm_local.client:480) INFO: Client 6 Adapter 4 with val results: {'val_total': 134, 'val_loss': 96.92308807373047, 'val_avg_loss': 0.723306627415899, 'val_seen': 134, 'val_correct': 75, 'val_acc': 0.5597014925373134}
2025-09-14 02:15:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.052366, avg_loss=0.776309, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:15:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1786MB
2025-09-14 02:15:22 (federatedscope.llm.llm_local.client:501) INFO: Client 6 Adapter 4 with test results: {'test_total': 40, 'test_loss': 31.052366256713867, 'test_avg_loss': 0.7763091564178467, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:15:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:15:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 02:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 02:15:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.177021, avg_loss=0.687316, seen=57, correct=31, accuracy=0.543860
2025-09-14 02:15:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1836MB
2025-09-14 02:15:26 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 0 with val results: {'val_total': 57, 'val_loss': 39.17702102661133, 'val_avg_loss': 0.6873161583616022, 'val_seen': 57, 'val_correct': 31, 'val_acc': 0.543859649122807}
2025-09-14 02:15:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.833549, avg_loss=0.570839, seen=40, correct=29, accuracy=0.725000
2025-09-14 02:15:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1836MB
2025-09-14 02:15:29 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 0 with test results: {'test_total': 40, 'test_loss': 22.83354949951172, 'test_avg_loss': 0.570838737487793, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:15:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 02:15:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 02:15:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.800781, avg_loss=0.645628, seen=57, correct=36, accuracy=0.631579
2025-09-14 02:15:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1794MB
2025-09-14 02:15:32 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 1 with val results: {'val_total': 57, 'val_loss': 36.80078125, 'val_avg_loss': 0.6456277412280702, 'val_seen': 57, 'val_correct': 36, 'val_acc': 0.631578947368421}
2025-09-14 02:15:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.477499, avg_loss=0.611937, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:15:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1794MB
2025-09-14 02:15:35 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.47749900817871, 'test_avg_loss': 0.6119374752044677, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:15:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 02:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 02:15:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.545162, avg_loss=0.658687, seen=57, correct=37, accuracy=0.649123
2025-09-14 02:15:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1794MB
2025-09-14 02:15:38 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 2 with val results: {'val_total': 57, 'val_loss': 37.545162200927734, 'val_avg_loss': 0.6586870561566269, 'val_seen': 57, 'val_correct': 37, 'val_acc': 0.6491228070175439}
2025-09-14 02:15:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.790897, avg_loss=0.644772, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:15:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1794MB
2025-09-14 02:15:41 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.790897369384766, 'test_avg_loss': 0.6447724342346192, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:15:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 02:15:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 02:15:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=40.177933, avg_loss=0.704876, seen=57, correct=33, accuracy=0.578947
2025-09-14 02:15:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1794MB
2025-09-14 02:15:44 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 3 with val results: {'val_total': 57, 'val_loss': 40.17793273925781, 'val_avg_loss': 0.7048760129694354, 'val_seen': 57, 'val_correct': 33, 'val_acc': 0.5789473684210527}
2025-09-14 02:15:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.751009, avg_loss=0.643775, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:15:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1794MB
2025-09-14 02:15:47 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 3 with test results: {'test_total': 40, 'test_loss': 25.751008987426758, 'test_avg_loss': 0.643775224685669, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:15:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 02:15:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 02:15:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.819386, avg_loss=0.681042, seen=57, correct=32, accuracy=0.561404
2025-09-14 02:15:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1794MB
2025-09-14 02:15:51 (federatedscope.llm.llm_local.client:480) INFO: Client 7 Adapter 4 with val results: {'val_total': 57, 'val_loss': 38.81938552856445, 'val_avg_loss': 0.6810418513783237, 'val_seen': 57, 'val_correct': 32, 'val_acc': 0.5614035087719298}
2025-09-14 02:15:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.698263, avg_loss=0.692457, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:15:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1794MB
2025-09-14 02:15:53 (federatedscope.llm.llm_local.client:501) INFO: Client 7 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.69826316833496, 'test_avg_loss': 0.692456579208374, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:15:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:15:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 02:15:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:15:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.398510, avg_loss=0.672442, seen=69, correct=41, accuracy=0.594203
2025-09-14 02:15:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:15:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1844MB
2025-09-14 02:15:58 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 0 with val results: {'val_total': 69, 'val_loss': 46.39850997924805, 'val_avg_loss': 0.6724421736122905, 'val_seen': 69, 'val_correct': 41, 'val_acc': 0.5942028985507246}
2025-09-14 02:15:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:15:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:15:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:15:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:15:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.204552, avg_loss=0.780114, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:15:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:15:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1844MB
2025-09-14 02:16:00 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.204551696777344, 'test_avg_loss': 0.7801137924194336, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:16:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 02:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:16:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=48.061886, avg_loss=0.696549, seen=69, correct=35, accuracy=0.507246
2025-09-14 02:16:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1803MB
2025-09-14 02:16:04 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 1 with val results: {'val_total': 69, 'val_loss': 48.061885833740234, 'val_avg_loss': 0.6965490700542063, 'val_seen': 69, 'val_correct': 35, 'val_acc': 0.5072463768115942}
2025-09-14 02:16:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:16:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.269461, avg_loss=0.706737, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:16:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1803MB
2025-09-14 02:16:06 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.269460678100586, 'test_avg_loss': 0.7067365169525146, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:16:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 02:16:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:16:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.721558, avg_loss=0.677124, seen=69, correct=39, accuracy=0.565217
2025-09-14 02:16:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1803MB
2025-09-14 02:16:10 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 2 with val results: {'val_total': 69, 'val_loss': 46.7215576171875, 'val_avg_loss': 0.6771240234375, 'val_seen': 69, 'val_correct': 39, 'val_acc': 0.5652173913043478}
2025-09-14 02:16:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:16:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:16:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.830374, avg_loss=0.745759, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:16:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1803MB
2025-09-14 02:16:13 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.830373764038086, 'test_avg_loss': 0.7457593441009521, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:16:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 02:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:16:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.058575, avg_loss=0.682008, seen=69, correct=36, accuracy=0.521739
2025-09-14 02:16:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1803MB
2025-09-14 02:16:17 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 3 with val results: {'val_total': 69, 'val_loss': 47.05857467651367, 'val_avg_loss': 0.6820083286451257, 'val_seen': 69, 'val_correct': 36, 'val_acc': 0.5217391304347826}
2025-09-14 02:16:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:16:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.526226, avg_loss=0.788156, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:16:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1803MB
2025-09-14 02:16:19 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 3 with test results: {'test_total': 40, 'test_loss': 31.526226043701172, 'test_avg_loss': 0.7881556510925293, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:16:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 02:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:16:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=49.920101, avg_loss=0.723480, seen=69, correct=37, accuracy=0.536232
2025-09-14 02:16:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1803MB
2025-09-14 02:16:23 (federatedscope.llm.llm_local.client:480) INFO: Client 8 Adapter 4 with val results: {'val_total': 69, 'val_loss': 49.920101165771484, 'val_avg_loss': 0.7234797270401664, 'val_seen': 69, 'val_correct': 37, 'val_acc': 0.5362318840579711}
2025-09-14 02:16:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:16:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:16:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.728817, avg_loss=0.743220, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:16:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1803MB
2025-09-14 02:16:25 (federatedscope.llm.llm_local.client:501) INFO: Client 8 Adapter 4 with test results: {'test_total': 40, 'test_loss': 29.728816986083984, 'test_avg_loss': 0.7432204246520996, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:16:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:16:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:16:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=127.371384, avg_loss=0.677507, seen=188, correct=116, accuracy=0.617021
2025-09-14 02:16:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1853MB
2025-09-14 02:16:33 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 0 with val results: {'val_total': 188, 'val_loss': 127.37138366699219, 'val_avg_loss': 0.6775073599308095, 'val_seen': 188, 'val_correct': 116, 'val_acc': 0.6170212765957447}
2025-09-14 02:16:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:16:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:16:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.734896, avg_loss=0.643372, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:16:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1853MB
2025-09-14 02:16:35 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.734895706176758, 'test_avg_loss': 0.6433723926544189, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:16:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:16:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=127.907501, avg_loss=0.680359, seen=188, correct=104, accuracy=0.553191
2025-09-14 02:16:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1811MB
2025-09-14 02:16:42 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 1 with val results: {'val_total': 188, 'val_loss': 127.90750122070312, 'val_avg_loss': 0.6803590490462932, 'val_seen': 188, 'val_correct': 104, 'val_acc': 0.5531914893617021}
2025-09-14 02:16:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:16:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.942083, avg_loss=0.673552, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:16:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1811MB
2025-09-14 02:16:44 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.94208335876465, 'test_avg_loss': 0.6735520839691163, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:16:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:16:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:16:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=130.282440, avg_loss=0.692992, seen=188, correct=107, accuracy=0.569149
2025-09-14 02:16:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1811MB
2025-09-14 02:16:53 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 2 with val results: {'val_total': 188, 'val_loss': 130.28244018554688, 'val_avg_loss': 0.692991703114611, 'val_seen': 188, 'val_correct': 107, 'val_acc': 0.5691489361702128}
2025-09-14 02:16:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:16:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:16:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.559942, avg_loss=0.663999, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:16:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:16:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1811MB
2025-09-14 02:16:55 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.5599422454834, 'test_avg_loss': 0.6639985561370849, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:16:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:16:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:16:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:17:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=132.878433, avg_loss=0.706800, seen=188, correct=111, accuracy=0.590426
2025-09-14 02:17:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1811MB
2025-09-14 02:17:03 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 3 with val results: {'val_total': 188, 'val_loss': 132.87843322753906, 'val_avg_loss': 0.7068001767422291, 'val_seen': 188, 'val_correct': 111, 'val_acc': 0.5904255319148937}
2025-09-14 02:17:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.742153, avg_loss=0.643554, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:17:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1811MB
2025-09-14 02:17:05 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 3 with test results: {'test_total': 40, 'test_loss': 25.74215316772461, 'test_avg_loss': 0.6435538291931152, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:17:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:17:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:17:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=135.275284, avg_loss=0.719549, seen=188, correct=101, accuracy=0.537234
2025-09-14 02:17:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1811MB
2025-09-14 02:17:13 (federatedscope.llm.llm_local.client:480) INFO: Client 9 Adapter 4 with val results: {'val_total': 188, 'val_loss': 135.27528381347656, 'val_avg_loss': 0.7195493819865775, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-14 02:17:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.244307, avg_loss=0.756108, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:17:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1811MB
2025-09-14 02:17:15 (federatedscope.llm.llm_local.client:501) INFO: Client 9 Adapter 4 with test results: {'test_total': 40, 'test_loss': 30.244306564331055, 'test_avg_loss': 0.7561076641082763, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:17:15 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:17:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 02:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 02:17:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=39.924397, avg_loss=0.633721, seen=63, correct=43, accuracy=0.682540
2025-09-14 02:17:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1862MB
2025-09-14 02:17:19 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 0 with val results: {'val_total': 63, 'val_loss': 39.92439651489258, 'val_avg_loss': 0.6337205796014695, 'val_seen': 63, 'val_correct': 43, 'val_acc': 0.6825396825396826}
2025-09-14 02:17:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.562874, avg_loss=0.564072, seen=40, correct=29, accuracy=0.725000
2025-09-14 02:17:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1862MB
2025-09-14 02:17:21 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 0 with test results: {'test_total': 40, 'test_loss': 22.56287384033203, 'test_avg_loss': 0.5640718460083007, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:17:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 02:17:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 02:17:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.637424, avg_loss=0.676785, seen=63, correct=33, accuracy=0.523810
2025-09-14 02:17:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1820MB
2025-09-14 02:17:25 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 1 with val results: {'val_total': 63, 'val_loss': 42.63742446899414, 'val_avg_loss': 0.6767845153808594, 'val_seen': 63, 'val_correct': 33, 'val_acc': 0.5238095238095238}
2025-09-14 02:17:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.012653, avg_loss=0.675316, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:17:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1820MB
2025-09-14 02:17:27 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.012653350830078, 'test_avg_loss': 0.675316333770752, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:17:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 02:17:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 02:17:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.747761, avg_loss=0.678536, seen=63, correct=33, accuracy=0.523810
2025-09-14 02:17:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1820MB
2025-09-14 02:17:31 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 2 with val results: {'val_total': 63, 'val_loss': 42.74776077270508, 'val_avg_loss': 0.678535885281033, 'val_seen': 63, 'val_correct': 33, 'val_acc': 0.5238095238095238}
2025-09-14 02:17:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.158714, avg_loss=0.678968, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:17:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1820MB
2025-09-14 02:17:34 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.158714294433594, 'test_avg_loss': 0.6789678573608399, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:17:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 02:17:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 02:17:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.141384, avg_loss=0.684784, seen=63, correct=36, accuracy=0.571429
2025-09-14 02:17:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1820MB
2025-09-14 02:17:37 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 3 with val results: {'val_total': 63, 'val_loss': 43.14138412475586, 'val_avg_loss': 0.6847838749961247, 'val_seen': 63, 'val_correct': 36, 'val_acc': 0.5714285714285714}
2025-09-14 02:17:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.490894, avg_loss=0.712272, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:17:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1820MB
2025-09-14 02:17:40 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 3 with test results: {'test_total': 40, 'test_loss': 28.490894317626953, 'test_avg_loss': 0.7122723579406738, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:17:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 02:17:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 02:17:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=51.415520, avg_loss=0.816119, seen=63, correct=30, accuracy=0.476190
2025-09-14 02:17:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1820MB
2025-09-14 02:17:43 (federatedscope.llm.llm_local.client:480) INFO: Client 10 Adapter 4 with val results: {'val_total': 63, 'val_loss': 51.41551971435547, 'val_avg_loss': 0.8161193605453249, 'val_seen': 63, 'val_correct': 30, 'val_acc': 0.47619047619047616}
2025-09-14 02:17:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.912508, avg_loss=0.697813, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:17:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1820MB
2025-09-14 02:17:46 (federatedscope.llm.llm_local.client:501) INFO: Client 10 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.912508010864258, 'test_avg_loss': 0.6978127002716065, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:17:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:17:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 02:17:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:17:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=20.024311, avg_loss=0.625760, seen=32, correct=17, accuracy=0.531250
2025-09-14 02:17:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1870MB
2025-09-14 02:17:49 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 0 with val results: {'val_total': 32, 'val_loss': 20.024311065673828, 'val_avg_loss': 0.6257597208023071, 'val_seen': 32, 'val_correct': 17, 'val_acc': 0.53125}
2025-09-14 02:17:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.305054, avg_loss=0.607626, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:17:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1870MB
2025-09-14 02:17:51 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.3050537109375, 'test_avg_loss': 0.6076263427734375, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:17:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 02:17:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:17:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=20.739010, avg_loss=0.648094, seen=32, correct=22, accuracy=0.687500
2025-09-14 02:17:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1828MB
2025-09-14 02:17:54 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 1 with val results: {'val_total': 32, 'val_loss': 20.739009857177734, 'val_avg_loss': 0.6480940580368042, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}
2025-09-14 02:17:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:17:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:17:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.352182, avg_loss=0.683805, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:17:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1828MB
2025-09-14 02:17:56 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.352182388305664, 'test_avg_loss': 0.6838045597076416, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:17:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 02:17:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:17:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:17:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=21.579666, avg_loss=0.674365, seen=32, correct=16, accuracy=0.500000
2025-09-14 02:17:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:17:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:17:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:17:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1828MB
2025-09-14 02:17:59 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 2 with val results: {'val_total': 32, 'val_loss': 21.579666137695312, 'val_avg_loss': 0.6743645668029785, 'val_seen': 32, 'val_correct': 16, 'val_acc': 0.5}
2025-09-14 02:17:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.720156, avg_loss=0.643004, seen=40, correct=29, accuracy=0.725000
2025-09-14 02:18:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1828MB
2025-09-14 02:18:02 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.720155715942383, 'test_avg_loss': 0.6430038928985595, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:18:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 02:18:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:18:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=21.739645, avg_loss=0.679364, seen=32, correct=19, accuracy=0.593750
2025-09-14 02:18:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1828MB
2025-09-14 02:18:05 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 3 with val results: {'val_total': 32, 'val_loss': 21.73964500427246, 'val_avg_loss': 0.6793639063835144, 'val_seen': 32, 'val_correct': 19, 'val_acc': 0.59375}
2025-09-14 02:18:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.358791, avg_loss=0.658970, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:18:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1828MB
2025-09-14 02:18:07 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 3 with test results: {'test_total': 40, 'test_loss': 26.35879135131836, 'test_avg_loss': 0.658969783782959, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:18:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 02:18:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:18:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=23.230209, avg_loss=0.725944, seen=32, correct=20, accuracy=0.625000
2025-09-14 02:18:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1828MB
2025-09-14 02:18:10 (federatedscope.llm.llm_local.client:480) INFO: Client 11 Adapter 4 with val results: {'val_total': 32, 'val_loss': 23.230209350585938, 'val_avg_loss': 0.7259440422058105, 'val_seen': 32, 'val_correct': 20, 'val_acc': 0.625}
2025-09-14 02:18:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.956585, avg_loss=0.748915, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:18:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1828MB
2025-09-14 02:18:12 (federatedscope.llm.llm_local.client:501) INFO: Client 11 Adapter 4 with test results: {'test_total': 40, 'test_loss': 29.956584930419922, 'test_avg_loss': 0.7489146232604981, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:18:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:18:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 02:18:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 02:18:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=92.867561, avg_loss=0.677865, seen=137, correct=85, accuracy=0.620438
2025-09-14 02:18:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1879MB
2025-09-14 02:18:18 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 0 with val results: {'val_total': 137, 'val_loss': 92.86756134033203, 'val_avg_loss': 0.6778654112432995, 'val_seen': 137, 'val_correct': 85, 'val_acc': 0.6204379562043796}
2025-09-14 02:18:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.385178, avg_loss=0.659629, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:18:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2040MB allocated=1879MB
2025-09-14 02:18:20 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.385177612304688, 'test_avg_loss': 0.6596294403076172, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:18:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 02:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 02:18:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=93.341995, avg_loss=0.681328, seen=137, correct=75, accuracy=0.547445
2025-09-14 02:18:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1837MB
2025-09-14 02:18:25 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 1 with val results: {'val_total': 137, 'val_loss': 93.34199523925781, 'val_avg_loss': 0.6813284324033417, 'val_seen': 137, 'val_correct': 75, 'val_acc': 0.5474452554744526}
2025-09-14 02:18:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.627748, avg_loss=0.640694, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:18:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1837MB
2025-09-14 02:18:28 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.627748489379883, 'test_avg_loss': 0.6406937122344971, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:18:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 02:18:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 02:18:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=95.780716, avg_loss=0.699129, seen=137, correct=66, accuracy=0.481752
2025-09-14 02:18:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2040MB allocated=1837MB
2025-09-14 02:18:33 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 2 with val results: {'val_total': 137, 'val_loss': 95.78071594238281, 'val_avg_loss': 0.6991293134480497, 'val_seen': 137, 'val_correct': 66, 'val_acc': 0.48175182481751827}
2025-09-14 02:18:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.496016, avg_loss=0.637400, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:18:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1837MB
2025-09-14 02:18:36 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.496015548706055, 'test_avg_loss': 0.6374003887176514, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:18:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 02:18:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 02:18:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=100.242683, avg_loss=0.731698, seen=137, correct=74, accuracy=0.540146
2025-09-14 02:18:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1837MB
2025-09-14 02:18:42 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 3 with val results: {'val_total': 137, 'val_loss': 100.24268341064453, 'val_avg_loss': 0.7316984190557995, 'val_seen': 137, 'val_correct': 74, 'val_acc': 0.5401459854014599}
2025-09-14 02:18:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.712032, avg_loss=0.642801, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:18:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1837MB
2025-09-14 02:18:45 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 3 with test results: {'test_total': 40, 'test_loss': 25.712032318115234, 'test_avg_loss': 0.6428008079528809, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:18:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 02:18:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 02:18:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=95.304947, avg_loss=0.695657, seen=137, correct=82, accuracy=0.598540
2025-09-14 02:18:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1837MB
2025-09-14 02:18:50 (federatedscope.llm.llm_local.client:480) INFO: Client 12 Adapter 4 with val results: {'val_total': 137, 'val_loss': 95.30494689941406, 'val_avg_loss': 0.6956565467110516, 'val_seen': 137, 'val_correct': 82, 'val_acc': 0.5985401459854015}
2025-09-14 02:18:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.489010, avg_loss=0.687225, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:18:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1837MB
2025-09-14 02:18:52 (federatedscope.llm.llm_local.client:501) INFO: Client 12 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.489009857177734, 'test_avg_loss': 0.6872252464294434, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:18:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:18:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 02:18:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:18:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=50.249153, avg_loss=0.697905, seen=72, correct=41, accuracy=0.569444
2025-09-14 02:18:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1887MB
2025-09-14 02:18:56 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 0 with val results: {'val_total': 72, 'val_loss': 50.24915313720703, 'val_avg_loss': 0.697904904683431, 'val_seen': 72, 'val_correct': 41, 'val_acc': 0.5694444444444444}
2025-09-14 02:18:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:18:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:18:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:18:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.031128, avg_loss=0.675778, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:18:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:18:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:18:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1887MB
2025-09-14 02:18:58 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.0311279296875, 'test_avg_loss': 0.6757781982421875, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:18:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 02:18:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:18:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:19:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.656120, avg_loss=0.675779, seen=72, correct=38, accuracy=0.527778
2025-09-14 02:19:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1845MB
2025-09-14 02:19:02 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 1 with val results: {'val_total': 72, 'val_loss': 48.65612030029297, 'val_avg_loss': 0.6757794486151801, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-14 02:19:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:19:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:19:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.046883, avg_loss=0.651172, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:19:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1845MB
2025-09-14 02:19:05 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.04688262939453, 'test_avg_loss': 0.6511720657348633, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:19:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 02:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:19:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=50.454300, avg_loss=0.700754, seen=72, correct=38, accuracy=0.527778
2025-09-14 02:19:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1845MB
2025-09-14 02:19:09 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 2 with val results: {'val_total': 72, 'val_loss': 50.45429992675781, 'val_avg_loss': 0.7007541656494141, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-14 02:19:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:19:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.427458, avg_loss=0.660686, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:19:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1845MB
2025-09-14 02:19:11 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.427457809448242, 'test_avg_loss': 0.660686445236206, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:19:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 02:19:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:19:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=51.838715, avg_loss=0.719982, seen=72, correct=40, accuracy=0.555556
2025-09-14 02:19:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1845MB
2025-09-14 02:19:15 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 3 with val results: {'val_total': 72, 'val_loss': 51.838714599609375, 'val_avg_loss': 0.7199821472167969, 'val_seen': 72, 'val_correct': 40, 'val_acc': 0.5555555555555556}
2025-09-14 02:19:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:19:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:19:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.353832, avg_loss=0.658846, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:19:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1845MB
2025-09-14 02:19:17 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 3 with test results: {'test_total': 40, 'test_loss': 26.353832244873047, 'test_avg_loss': 0.6588458061218262, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:19:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 02:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 02:19:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=50.049957, avg_loss=0.695138, seen=72, correct=37, accuracy=0.513889
2025-09-14 02:19:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1845MB
2025-09-14 02:19:21 (federatedscope.llm.llm_local.client:480) INFO: Client 13 Adapter 4 with val results: {'val_total': 72, 'val_loss': 50.049957275390625, 'val_avg_loss': 0.6951382954915365, 'val_seen': 72, 'val_correct': 37, 'val_acc': 0.5138888888888888}
2025-09-14 02:19:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:19:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.326496, avg_loss=0.733162, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:19:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1845MB
2025-09-14 02:19:24 (federatedscope.llm.llm_local.client:501) INFO: Client 13 Adapter 4 with test results: {'test_total': 40, 'test_loss': 29.326496124267578, 'test_avg_loss': 0.7331624031066895, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:19:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:19:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 02:19:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 02:19:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.715950, avg_loss=0.666975, seen=160, correct=99, accuracy=0.618750
2025-09-14 02:19:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1895MB
2025-09-14 02:19:30 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 0 with val results: {'val_total': 160, 'val_loss': 106.71595001220703, 'val_avg_loss': 0.666974687576294, 'val_seen': 160, 'val_correct': 99, 'val_acc': 0.61875}
2025-09-14 02:19:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:19:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.343418, avg_loss=0.608585, seen=40, correct=29, accuracy=0.725000
2025-09-14 02:19:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1895MB
2025-09-14 02:19:33 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.34341812133789, 'test_avg_loss': 0.6085854530334472, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:19:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 02:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 02:19:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=108.176369, avg_loss=0.676102, seen=160, correct=92, accuracy=0.575000
2025-09-14 02:19:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1853MB
2025-09-14 02:19:39 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 1 with val results: {'val_total': 160, 'val_loss': 108.1763687133789, 'val_avg_loss': 0.6761023044586182, 'val_seen': 160, 'val_correct': 92, 'val_acc': 0.575}
2025-09-14 02:19:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:19:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.733955, avg_loss=0.618349, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:19:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1853MB
2025-09-14 02:19:41 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.73395538330078, 'test_avg_loss': 0.6183488845825196, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:19:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 02:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 02:19:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=111.469406, avg_loss=0.696684, seen=160, correct=87, accuracy=0.543750
2025-09-14 02:19:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1853MB
2025-09-14 02:19:48 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 2 with val results: {'val_total': 160, 'val_loss': 111.46940612792969, 'val_avg_loss': 0.6966837882995606, 'val_seen': 160, 'val_correct': 87, 'val_acc': 0.54375}
2025-09-14 02:19:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:19:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:19:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.804916, avg_loss=0.645123, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:19:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1853MB
2025-09-14 02:19:50 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.804916381835938, 'test_avg_loss': 0.6451229095458985, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:19:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 02:19:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 02:19:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=112.427490, avg_loss=0.702672, seen=160, correct=99, accuracy=0.618750
2025-09-14 02:19:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1853MB
2025-09-14 02:19:56 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 3 with val results: {'val_total': 160, 'val_loss': 112.427490234375, 'val_avg_loss': 0.7026718139648438, 'val_seen': 160, 'val_correct': 99, 'val_acc': 0.61875}
2025-09-14 02:19:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:19:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:19:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:19:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.781935, avg_loss=0.594548, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:19:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:19:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:19:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1853MB
2025-09-14 02:19:59 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 3 with test results: {'test_total': 40, 'test_loss': 23.78193473815918, 'test_avg_loss': 0.5945483684539795, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:19:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 02:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:19:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 02:20:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=111.498772, avg_loss=0.696867, seen=160, correct=89, accuracy=0.556250
2025-09-14 02:20:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1853MB
2025-09-14 02:20:04 (federatedscope.llm.llm_local.client:480) INFO: Client 14 Adapter 4 with val results: {'val_total': 160, 'val_loss': 111.49877166748047, 'val_avg_loss': 0.6968673229217529, 'val_seen': 160, 'val_correct': 89, 'val_acc': 0.55625}
2025-09-14 02:20:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:20:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.483185, avg_loss=0.712080, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:20:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1853MB
2025-09-14 02:20:07 (federatedscope.llm.llm_local.client:501) INFO: Client 14 Adapter 4 with test results: {'test_total': 40, 'test_loss': 28.483184814453125, 'test_avg_loss': 0.7120796203613281, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:20:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:20:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:20:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:20:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.940552, avg_loss=0.649703, seen=200, correct=118, accuracy=0.590000
2025-09-14 02:20:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1904MB
2025-09-14 02:20:15 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 0 with val results: {'val_total': 200, 'val_loss': 129.9405517578125, 'val_avg_loss': 0.6497027587890625, 'val_seen': 200, 'val_correct': 118, 'val_acc': 0.59}
2025-09-14 02:20:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:20:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.334936, avg_loss=0.683373, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:20:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1904MB
2025-09-14 02:20:17 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.334936141967773, 'test_avg_loss': 0.6833734035491943, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:20:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:20:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:20:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.198059, avg_loss=0.660990, seen=200, correct=117, accuracy=0.585000
2025-09-14 02:20:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1862MB
2025-09-14 02:20:25 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 1 with val results: {'val_total': 200, 'val_loss': 132.19805908203125, 'val_avg_loss': 0.6609902954101563, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 02:20:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:20:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:20:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.384285, avg_loss=0.659607, seen=40, correct=29, accuracy=0.725000
2025-09-14 02:20:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1862MB
2025-09-14 02:20:28 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.38428497314453, 'test_avg_loss': 0.6596071243286132, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:20:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:20:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:20:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.651154, avg_loss=0.668256, seen=200, correct=114, accuracy=0.570000
2025-09-14 02:20:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1862MB
2025-09-14 02:20:35 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 2 with val results: {'val_total': 200, 'val_loss': 133.65115356445312, 'val_avg_loss': 0.6682557678222656, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-14 02:20:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:20:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:20:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.194239, avg_loss=0.679856, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:20:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1862MB
2025-09-14 02:20:38 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.194238662719727, 'test_avg_loss': 0.6798559665679932, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:20:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:20:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:20:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.448715, avg_loss=0.657244, seen=200, correct=130, accuracy=0.650000
2025-09-14 02:20:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1862MB
2025-09-14 02:20:46 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 3 with val results: {'val_total': 200, 'val_loss': 131.44871520996094, 'val_avg_loss': 0.6572435760498047, 'val_seen': 200, 'val_correct': 130, 'val_acc': 0.65}
2025-09-14 02:20:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:20:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:20:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.080893, avg_loss=0.727022, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:20:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1862MB
2025-09-14 02:20:48 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 3 with test results: {'test_total': 40, 'test_loss': 29.08089256286621, 'test_avg_loss': 0.7270223140716553, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:20:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:20:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=146.250381, avg_loss=0.731252, seen=200, correct=95, accuracy=0.475000
2025-09-14 02:20:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1862MB
2025-09-14 02:20:56 (federatedscope.llm.llm_local.client:480) INFO: Client 15 Adapter 4 with val results: {'val_total': 200, 'val_loss': 146.25038146972656, 'val_avg_loss': 0.7312519073486328, 'val_seen': 200, 'val_correct': 95, 'val_acc': 0.475}
2025-09-14 02:20:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:20:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:20:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:20:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.089840, avg_loss=0.652246, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:20:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:20:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:20:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1862MB
2025-09-14 02:20:58 (federatedscope.llm.llm_local.client:501) INFO: Client 15 Adapter 4 with test results: {'test_total': 40, 'test_loss': 26.089839935302734, 'test_avg_loss': 0.6522459983825684, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:20:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:20:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 02:20:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:20:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:21:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.407951, avg_loss=0.657411, seen=136, correct=86, accuracy=0.632353
2025-09-14 02:21:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1912MB
2025-09-14 02:21:04 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 0 with val results: {'val_total': 136, 'val_loss': 89.40795135498047, 'val_avg_loss': 0.6574114070219153, 'val_seen': 136, 'val_correct': 86, 'val_acc': 0.6323529411764706}
2025-09-14 02:21:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:21:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:21:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.316982, avg_loss=0.582925, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:21:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1912MB
2025-09-14 02:21:07 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.31698226928711, 'test_avg_loss': 0.5829245567321777, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:21:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 02:21:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:21:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=88.860985, avg_loss=0.653390, seen=136, correct=82, accuracy=0.602941
2025-09-14 02:21:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1870MB
2025-09-14 02:21:13 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 1 with val results: {'val_total': 136, 'val_loss': 88.8609848022461, 'val_avg_loss': 0.6533895941341624, 'val_seen': 136, 'val_correct': 82, 'val_acc': 0.6029411764705882}
2025-09-14 02:21:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:21:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:21:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.079582, avg_loss=0.676990, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:21:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1870MB
2025-09-14 02:21:15 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.07958221435547, 'test_avg_loss': 0.6769895553588867, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:21:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 02:21:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:21:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=91.771332, avg_loss=0.674789, seen=136, correct=77, accuracy=0.566176
2025-09-14 02:21:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1870MB
2025-09-14 02:21:21 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 2 with val results: {'val_total': 136, 'val_loss': 91.77133178710938, 'val_avg_loss': 0.6747892043169808, 'val_seen': 136, 'val_correct': 77, 'val_acc': 0.5661764705882353}
2025-09-14 02:21:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:21:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.829212, avg_loss=0.695730, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:21:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1870MB
2025-09-14 02:21:24 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.829212188720703, 'test_avg_loss': 0.6957303047180176, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:21:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 02:21:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:21:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=93.309097, avg_loss=0.686096, seen=136, correct=80, accuracy=0.588235
2025-09-14 02:21:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1870MB
2025-09-14 02:21:30 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 3 with val results: {'val_total': 136, 'val_loss': 93.30909729003906, 'val_avg_loss': 0.6860963036032284, 'val_seen': 136, 'val_correct': 80, 'val_acc': 0.5882352941176471}
2025-09-14 02:21:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:21:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:21:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.388447, avg_loss=0.734711, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:21:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1870MB
2025-09-14 02:21:32 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 3 with test results: {'test_total': 40, 'test_loss': 29.388446807861328, 'test_avg_loss': 0.7347111701965332, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:21:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 02:21:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:21:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=92.230049, avg_loss=0.678162, seen=136, correct=76, accuracy=0.558824
2025-09-14 02:21:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1870MB
2025-09-14 02:21:38 (federatedscope.llm.llm_local.client:480) INFO: Client 16 Adapter 4 with val results: {'val_total': 136, 'val_loss': 92.23004913330078, 'val_avg_loss': 0.6781621259801528, 'val_seen': 136, 'val_correct': 76, 'val_acc': 0.5588235294117647}
2025-09-14 02:21:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:21:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.329607, avg_loss=0.683240, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:21:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1870MB
2025-09-14 02:21:41 (federatedscope.llm.llm_local.client:501) INFO: Client 16 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.329607009887695, 'test_avg_loss': 0.6832401752471924, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:21:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:21:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:21:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:21:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.997833, avg_loss=0.639989, seen=200, correct=132, accuracy=0.660000
2025-09-14 02:21:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1921MB
2025-09-14 02:21:49 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 0 with val results: {'val_total': 200, 'val_loss': 127.99783325195312, 'val_avg_loss': 0.6399891662597657, 'val_seen': 200, 'val_correct': 132, 'val_acc': 0.66}
2025-09-14 02:21:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:21:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:21:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.385239, avg_loss=0.659631, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:21:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:21:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1921MB
2025-09-14 02:21:51 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.385238647460938, 'test_avg_loss': 0.6596309661865234, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:21:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:21:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:21:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.138611, avg_loss=0.650693, seen=200, correct=125, accuracy=0.625000
2025-09-14 02:21:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:21:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1879MB
2025-09-14 02:22:00 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 1 with val results: {'val_total': 200, 'val_loss': 130.13861083984375, 'val_avg_loss': 0.6506930541992187, 'val_seen': 200, 'val_correct': 125, 'val_acc': 0.625}
2025-09-14 02:22:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:22:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:22:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.855343, avg_loss=0.696384, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:22:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1879MB
2025-09-14 02:22:02 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.855342864990234, 'test_avg_loss': 0.6963835716247558, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:22:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:22:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:22:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.789963, avg_loss=0.678950, seen=200, correct=113, accuracy=0.565000
2025-09-14 02:22:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1879MB
2025-09-14 02:22:10 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.7899627685547, 'val_avg_loss': 0.6789498138427734, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-14 02:22:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:22:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:22:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.488903, avg_loss=0.687223, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:22:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1879MB
2025-09-14 02:22:12 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.488903045654297, 'test_avg_loss': 0.6872225761413574, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:22:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:22:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.353607, avg_loss=0.681768, seen=200, correct=109, accuracy=0.545000
2025-09-14 02:22:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1879MB
2025-09-14 02:22:20 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 3 with val results: {'val_total': 200, 'val_loss': 136.35360717773438, 'val_avg_loss': 0.6817680358886719, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:22:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:22:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:22:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.053947, avg_loss=0.726349, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:22:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1879MB
2025-09-14 02:22:23 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 3 with test results: {'test_total': 40, 'test_loss': 29.05394744873047, 'test_avg_loss': 0.7263486862182618, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:22:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:22:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:22:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.144775, avg_loss=0.690724, seen=200, correct=114, accuracy=0.570000
2025-09-14 02:22:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1879MB
2025-09-14 02:22:31 (federatedscope.llm.llm_local.client:480) INFO: Client 17 Adapter 4 with val results: {'val_total': 200, 'val_loss': 138.144775390625, 'val_avg_loss': 0.690723876953125, 'val_seen': 200, 'val_correct': 114, 'val_acc': 0.57}
2025-09-14 02:22:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:22:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:22:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.753212, avg_loss=0.718830, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:22:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1879MB
2025-09-14 02:22:33 (federatedscope.llm.llm_local.client:501) INFO: Client 17 Adapter 4 with test results: {'test_total': 40, 'test_loss': 28.753211975097656, 'test_avg_loss': 0.7188302993774414, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:22:33 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:22:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 02:22:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:22:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.370171, avg_loss=0.706446, seen=135, correct=83, accuracy=0.614815
2025-09-14 02:22:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2040MB allocated=1929MB
2025-09-14 02:22:39 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 0 with val results: {'val_total': 135, 'val_loss': 95.37017059326172, 'val_avg_loss': 0.7064457080982349, 'val_seen': 135, 'val_correct': 83, 'val_acc': 0.6148148148148148}
2025-09-14 02:22:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:22:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.951885, avg_loss=0.573797, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:22:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1929MB
2025-09-14 02:22:41 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 0 with test results: {'test_total': 40, 'test_loss': 22.951885223388672, 'test_avg_loss': 0.5737971305847168, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:22:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 02:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:22:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.245239, avg_loss=0.698113, seen=135, correct=74, accuracy=0.548148
2025-09-14 02:22:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1888MB
2025-09-14 02:22:48 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 1 with val results: {'val_total': 135, 'val_loss': 94.2452392578125, 'val_avg_loss': 0.6981128833912037, 'val_seen': 135, 'val_correct': 74, 'val_acc': 0.5481481481481482}
2025-09-14 02:22:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:22:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:22:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.385050, avg_loss=0.609626, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:22:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1888MB
2025-09-14 02:22:50 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.38504981994629, 'test_avg_loss': 0.6096262454986572, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:22:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 02:22:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:22:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.237648, avg_loss=0.705464, seen=135, correct=71, accuracy=0.525926
2025-09-14 02:22:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1888MB
2025-09-14 02:22:57 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 2 with val results: {'val_total': 135, 'val_loss': 95.2376480102539, 'val_avg_loss': 0.7054640593352142, 'val_seen': 135, 'val_correct': 71, 'val_acc': 0.5259259259259259}
2025-09-14 02:22:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:22:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:22:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.237120, avg_loss=0.630928, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:22:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:22:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:22:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1888MB
2025-09-14 02:22:59 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.237119674682617, 'test_avg_loss': 0.6309279918670654, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:23:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 02:23:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:23:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=98.375153, avg_loss=0.728705, seen=135, correct=76, accuracy=0.562963
2025-09-14 02:23:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1888MB
2025-09-14 02:23:05 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 3 with val results: {'val_total': 135, 'val_loss': 98.37515258789062, 'val_avg_loss': 0.728704833984375, 'val_seen': 135, 'val_correct': 76, 'val_acc': 0.562962962962963}
2025-09-14 02:23:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:23:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:23:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.875675, avg_loss=0.571892, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:23:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1888MB
2025-09-14 02:23:07 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 3 with test results: {'test_total': 40, 'test_loss': 22.875675201416016, 'test_avg_loss': 0.5718918800354004, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:23:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 02:23:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:23:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=100.909019, avg_loss=0.747474, seen=135, correct=63, accuracy=0.466667
2025-09-14 02:23:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1888MB
2025-09-14 02:23:14 (federatedscope.llm.llm_local.client:480) INFO: Client 18 Adapter 4 with val results: {'val_total': 135, 'val_loss': 100.90901947021484, 'val_avg_loss': 0.7474742182978877, 'val_seen': 135, 'val_correct': 63, 'val_acc': 0.4666666666666667}
2025-09-14 02:23:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:23:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:23:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=33.513496, avg_loss=0.837837, seen=40, correct=15, accuracy=0.375000
2025-09-14 02:23:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1888MB
2025-09-14 02:23:16 (federatedscope.llm.llm_local.client:501) INFO: Client 18 Adapter 4 with test results: {'test_total': 40, 'test_loss': 33.51349639892578, 'test_avg_loss': 0.8378374099731445, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-14 02:23:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:23:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:23:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.405388, avg_loss=0.685504, seen=110, correct=62, accuracy=0.563636
2025-09-14 02:23:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2040MB allocated=1938MB
2025-09-14 02:23:21 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 0 with val results: {'val_total': 110, 'val_loss': 75.40538787841797, 'val_avg_loss': 0.685503526167436, 'val_seen': 110, 'val_correct': 62, 'val_acc': 0.5636363636363636}
2025-09-14 02:23:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:23:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:23:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.896290, avg_loss=0.797407, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:23:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1938MB
2025-09-14 02:23:24 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.896289825439453, 'test_avg_loss': 0.7974072456359863, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:23:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:23:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:23:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.292328, avg_loss=0.675385, seen=110, correct=60, accuracy=0.545455
2025-09-14 02:23:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1896MB
2025-09-14 02:23:28 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 1 with val results: {'val_total': 110, 'val_loss': 74.29232788085938, 'val_avg_loss': 0.6753847989169034, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-14 02:23:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:23:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:23:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.987919, avg_loss=0.749698, seen=40, correct=16, accuracy=0.400000
2025-09-14 02:23:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1896MB
2025-09-14 02:23:31 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.987918853759766, 'test_avg_loss': 0.7496979713439942, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-14 02:23:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:23:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:23:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=77.677750, avg_loss=0.706161, seen=110, correct=54, accuracy=0.490909
2025-09-14 02:23:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1896MB
2025-09-14 02:23:36 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 2 with val results: {'val_total': 110, 'val_loss': 77.67774963378906, 'val_avg_loss': 0.7061613603071732, 'val_seen': 110, 'val_correct': 54, 'val_acc': 0.4909090909090909}
2025-09-14 02:23:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:23:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:23:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.424986, avg_loss=0.760625, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:23:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1896MB
2025-09-14 02:23:39 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.424985885620117, 'test_avg_loss': 0.7606246471405029, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:23:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:23:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=78.868881, avg_loss=0.716990, seen=110, correct=58, accuracy=0.527273
2025-09-14 02:23:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1896MB
2025-09-14 02:23:43 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 3 with val results: {'val_total': 110, 'val_loss': 78.86888122558594, 'val_avg_loss': 0.7169898293235085, 'val_seen': 110, 'val_correct': 58, 'val_acc': 0.5272727272727272}
2025-09-14 02:23:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:23:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=34.129250, avg_loss=0.853231, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:23:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1896MB
2025-09-14 02:23:45 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 3 with test results: {'test_total': 40, 'test_loss': 34.129249572753906, 'test_avg_loss': 0.8532312393188477, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:23:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:23:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:23:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=83.485344, avg_loss=0.758958, seen=110, correct=54, accuracy=0.490909
2025-09-14 02:23:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1896MB
2025-09-14 02:23:50 (federatedscope.llm.llm_local.client:480) INFO: Client 19 Adapter 4 with val results: {'val_total': 110, 'val_loss': 83.48534393310547, 'val_avg_loss': 0.7589576721191407, 'val_seen': 110, 'val_correct': 54, 'val_acc': 0.4909090909090909}
2025-09-14 02:23:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:23:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.878677, avg_loss=0.746967, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:23:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1896MB
2025-09-14 02:23:52 (federatedscope.llm.llm_local.client:501) INFO: Client 19 Adapter 4 with test results: {'test_total': 40, 'test_loss': 29.878677368164062, 'test_avg_loss': 0.7469669342041015, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:23:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:23:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 02:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 02:23:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=90.964218, avg_loss=0.721938, seen=126, correct=65, accuracy=0.515873
2025-09-14 02:23:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:23:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1946MB
2025-09-14 02:23:58 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 0 with val results: {'val_total': 126, 'val_loss': 90.96421813964844, 'val_avg_loss': 0.721938239203559, 'val_seen': 126, 'val_correct': 65, 'val_acc': 0.5158730158730159}
2025-09-14 02:23:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:23:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:23:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:23:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:23:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.832848, avg_loss=0.645821, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:23:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1946MB
2025-09-14 02:24:01 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.832847595214844, 'test_avg_loss': 0.6458211898803711, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:24:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 02:24:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 02:24:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.256042, avg_loss=0.676635, seen=126, correct=70, accuracy=0.555556
2025-09-14 02:24:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1904MB
2025-09-14 02:24:07 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 1 with val results: {'val_total': 126, 'val_loss': 85.25604248046875, 'val_avg_loss': 0.676635257781498, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}
2025-09-14 02:24:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:24:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:24:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.611897, avg_loss=0.640297, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:24:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1904MB
2025-09-14 02:24:09 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.611896514892578, 'test_avg_loss': 0.6402974128723145, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:24:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 02:24:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 02:24:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.101196, avg_loss=0.691279, seen=126, correct=60, accuracy=0.476190
2025-09-14 02:24:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1904MB
2025-09-14 02:24:15 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 2 with val results: {'val_total': 126, 'val_loss': 87.1011962890625, 'val_avg_loss': 0.6912793356274801, 'val_seen': 126, 'val_correct': 60, 'val_acc': 0.47619047619047616}
2025-09-14 02:24:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:24:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:24:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.860352, avg_loss=0.671509, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:24:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1904MB
2025-09-14 02:24:18 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.8603515625, 'test_avg_loss': 0.6715087890625, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:24:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 02:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 02:24:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=91.793396, avg_loss=0.728519, seen=126, correct=61, accuracy=0.484127
2025-09-14 02:24:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1904MB
2025-09-14 02:24:24 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 3 with val results: {'val_total': 126, 'val_loss': 91.79339599609375, 'val_avg_loss': 0.7285190158420138, 'val_seen': 126, 'val_correct': 61, 'val_acc': 0.48412698412698413}
2025-09-14 02:24:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:24:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.312731, avg_loss=0.682818, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:24:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1904MB
2025-09-14 02:24:26 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 3 with test results: {'test_total': 40, 'test_loss': 27.31273078918457, 'test_avg_loss': 0.6828182697296142, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:24:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 02:24:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 02:24:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=89.593666, avg_loss=0.711061, seen=126, correct=72, accuracy=0.571429
2025-09-14 02:24:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1904MB
2025-09-14 02:24:32 (federatedscope.llm.llm_local.client:480) INFO: Client 20 Adapter 4 with val results: {'val_total': 126, 'val_loss': 89.59366607666016, 'val_avg_loss': 0.7110608418782552, 'val_seen': 126, 'val_correct': 72, 'val_acc': 0.5714285714285714}
2025-09-14 02:24:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:24:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.464146, avg_loss=0.636604, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:24:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1904MB
2025-09-14 02:24:35 (federatedscope.llm.llm_local.client:501) INFO: Client 20 Adapter 4 with test results: {'test_total': 40, 'test_loss': 25.46414566040039, 'test_avg_loss': 0.6366036415100098, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:24:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:24:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 02:24:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 02:24:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.995621, avg_loss=0.686246, seen=153, correct=90, accuracy=0.588235
2025-09-14 02:24:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1955MB
2025-09-14 02:24:42 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 0 with val results: {'val_total': 153, 'val_loss': 104.99562072753906, 'val_avg_loss': 0.6862458871080984, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-14 02:24:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:24:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:24:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.300140, avg_loss=0.782504, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:24:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1955MB
2025-09-14 02:24:44 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.300140380859375, 'test_avg_loss': 0.7825035095214844, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:24:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 02:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 02:24:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.045158, avg_loss=0.673498, seen=153, correct=90, accuracy=0.588235
2025-09-14 02:24:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1913MB
2025-09-14 02:24:52 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 1 with val results: {'val_total': 153, 'val_loss': 103.04515838623047, 'val_avg_loss': 0.6734977672302646, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-14 02:24:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:24:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:24:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.278633, avg_loss=0.706966, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:24:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:24:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1913MB
2025-09-14 02:24:54 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.27863311767578, 'test_avg_loss': 0.7069658279418946, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:24:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 02:24:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:24:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:24:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 02:24:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.036621, avg_loss=0.679978, seen=153, correct=93, accuracy=0.607843
2025-09-14 02:24:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:24:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1913MB
2025-09-14 02:25:01 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 2 with val results: {'val_total': 153, 'val_loss': 104.03662109375, 'val_avg_loss': 0.6799779156454249, 'val_seen': 153, 'val_correct': 93, 'val_acc': 0.6078431372549019}
2025-09-14 02:25:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.137852, avg_loss=0.753446, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:25:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1913MB
2025-09-14 02:25:03 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 2 with test results: {'test_total': 40, 'test_loss': 30.13785171508789, 'test_avg_loss': 0.7534462928771972, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:25:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 02:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 02:25:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.068573, avg_loss=0.680187, seen=153, correct=88, accuracy=0.575163
2025-09-14 02:25:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1913MB
2025-09-14 02:25:10 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 3 with val results: {'val_total': 153, 'val_loss': 104.06857299804688, 'val_avg_loss': 0.6801867516212214, 'val_seen': 153, 'val_correct': 88, 'val_acc': 0.5751633986928104}
2025-09-14 02:25:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.811031, avg_loss=0.820276, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:25:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1913MB
2025-09-14 02:25:13 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 3 with test results: {'test_total': 40, 'test_loss': 32.811031341552734, 'test_avg_loss': 0.8202757835388184, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:25:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 02:25:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 02:25:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=112.252960, avg_loss=0.733679, seen=153, correct=72, accuracy=0.470588
2025-09-14 02:25:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1913MB
2025-09-14 02:25:19 (federatedscope.llm.llm_local.client:480) INFO: Client 21 Adapter 4 with val results: {'val_total': 153, 'val_loss': 112.25296020507812, 'val_avg_loss': 0.7336794784645629, 'val_seen': 153, 'val_correct': 72, 'val_acc': 0.47058823529411764}
2025-09-14 02:25:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.860622, avg_loss=0.646516, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:25:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1913MB
2025-09-14 02:25:21 (federatedscope.llm.llm_local.client:501) INFO: Client 21 Adapter 4 with test results: {'test_total': 40, 'test_loss': 25.86062240600586, 'test_avg_loss': 0.6465155601501464, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:25:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:25:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:25:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:25:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.603653, avg_loss=0.873059, seen=11, correct=4, accuracy=0.363636
2025-09-14 02:25:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1963MB
2025-09-14 02:25:24 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 0 with val results: {'val_total': 11, 'val_loss': 9.603652954101562, 'val_avg_loss': 0.8730593594637784, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-14 02:25:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.643307, avg_loss=0.691083, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:25:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1963MB
2025-09-14 02:25:27 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 0 with test results: {'test_total': 40, 'test_loss': 27.643306732177734, 'test_avg_loss': 0.6910826683044433, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:25:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:25:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:25:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.971970, avg_loss=0.815634, seen=11, correct=4, accuracy=0.363636
2025-09-14 02:25:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1921MB
2025-09-14 02:25:29 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 1 with val results: {'val_total': 11, 'val_loss': 8.971969604492188, 'val_avg_loss': 0.8156336004083807, 'val_seen': 11, 'val_correct': 4, 'val_acc': 0.36363636363636365}
2025-09-14 02:25:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.881567, avg_loss=0.722039, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:25:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1921MB
2025-09-14 02:25:32 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.881567001342773, 'test_avg_loss': 0.7220391750335693, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:25:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:25:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:25:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.370916, avg_loss=0.760992, seen=11, correct=3, accuracy=0.272727
2025-09-14 02:25:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1921MB
2025-09-14 02:25:35 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 2 with val results: {'val_total': 11, 'val_loss': 8.370916366577148, 'val_avg_loss': 0.760992396961559, 'val_seen': 11, 'val_correct': 3, 'val_acc': 0.2727272727272727}
2025-09-14 02:25:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.989183, avg_loss=0.699730, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:25:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1921MB
2025-09-14 02:25:38 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.98918342590332, 'test_avg_loss': 0.699729585647583, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:25:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:25:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:25:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.668205, avg_loss=0.969837, seen=11, correct=2, accuracy=0.181818
2025-09-14 02:25:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1921MB
2025-09-14 02:25:40 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 3 with val results: {'val_total': 11, 'val_loss': 10.668205261230469, 'val_avg_loss': 0.9698368419300426, 'val_seen': 11, 'val_correct': 2, 'val_acc': 0.18181818181818182}
2025-09-14 02:25:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.021797, avg_loss=0.725545, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:25:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1921MB
2025-09-14 02:25:42 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 3 with test results: {'test_total': 40, 'test_loss': 29.02179718017578, 'test_avg_loss': 0.7255449295043945, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:25:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 02:25:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 02:25:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.796032, avg_loss=0.617821, seen=11, correct=8, accuracy=0.727273
2025-09-14 02:25:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1921MB
2025-09-14 02:25:45 (federatedscope.llm.llm_local.client:480) INFO: Client 22 Adapter 4 with val results: {'val_total': 11, 'val_loss': 6.796031951904297, 'val_avg_loss': 0.6178210865367543, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}
2025-09-14 02:25:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.753204, avg_loss=0.768830, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:25:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1921MB
2025-09-14 02:25:47 (federatedscope.llm.llm_local.client:501) INFO: Client 22 Adapter 4 with test results: {'test_total': 40, 'test_loss': 30.753204345703125, 'test_avg_loss': 0.7688301086425782, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:25:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:25:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 02:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:25:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.810236, avg_loss=0.627008, seen=30, correct=18, accuracy=0.600000
2025-09-14 02:25:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1972MB
2025-09-14 02:25:50 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 0 with val results: {'val_total': 30, 'val_loss': 18.81023597717285, 'val_avg_loss': 0.6270078659057617, 'val_seen': 30, 'val_correct': 18, 'val_acc': 0.6}
2025-09-14 02:25:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.484348, avg_loss=0.637109, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:25:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1972MB
2025-09-14 02:25:53 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.48434829711914, 'test_avg_loss': 0.6371087074279785, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:25:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 02:25:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:25:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=19.193954, avg_loss=0.639798, seen=30, correct=19, accuracy=0.633333
2025-09-14 02:25:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1930MB
2025-09-14 02:25:55 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 1 with val results: {'val_total': 30, 'val_loss': 19.193954467773438, 'val_avg_loss': 0.6397984822591146, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-14 02:25:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:25:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.298595, avg_loss=0.657465, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:25:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:25:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1930MB
2025-09-14 02:25:58 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.298595428466797, 'test_avg_loss': 0.6574648857116699, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:25:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 02:25:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:25:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:25:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:25:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=20.034014, avg_loss=0.667800, seen=30, correct=19, accuracy=0.633333
2025-09-14 02:25:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1930MB
2025-09-14 02:26:01 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 2 with val results: {'val_total': 30, 'val_loss': 20.034013748168945, 'val_avg_loss': 0.6678004582722982, 'val_seen': 30, 'val_correct': 19, 'val_acc': 0.6333333333333333}
2025-09-14 02:26:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:26:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.011469, avg_loss=0.675287, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:26:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1930MB
2025-09-14 02:26:03 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.0114688873291, 'test_avg_loss': 0.6752867221832275, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:26:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 02:26:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:26:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=19.883671, avg_loss=0.662789, seen=30, correct=17, accuracy=0.566667
2025-09-14 02:26:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1930MB
2025-09-14 02:26:07 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 3 with val results: {'val_total': 30, 'val_loss': 19.883670806884766, 'val_avg_loss': 0.6627890268961588, 'val_seen': 30, 'val_correct': 17, 'val_acc': 0.5666666666666667}
2025-09-14 02:26:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:26:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:26:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.576139, avg_loss=0.689403, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:26:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1930MB
2025-09-14 02:26:09 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 3 with test results: {'test_total': 40, 'test_loss': 27.576139450073242, 'test_avg_loss': 0.689403486251831, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:26:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 02:26:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 02:26:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=20.350586, avg_loss=0.678353, seen=30, correct=15, accuracy=0.500000
2025-09-14 02:26:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1930MB
2025-09-14 02:26:11 (federatedscope.llm.llm_local.client:480) INFO: Client 23 Adapter 4 with val results: {'val_total': 30, 'val_loss': 20.3505859375, 'val_avg_loss': 0.6783528645833333, 'val_seen': 30, 'val_correct': 15, 'val_acc': 0.5}
2025-09-14 02:26:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:26:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.735405, avg_loss=0.668385, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:26:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1930MB
2025-09-14 02:26:14 (federatedscope.llm.llm_local.client:501) INFO: Client 23 Adapter 4 with test results: {'test_total': 40, 'test_loss': 26.73540496826172, 'test_avg_loss': 0.668385124206543, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:26:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:26:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:26:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:26:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.682724, avg_loss=0.688414, seen=200, correct=115, accuracy=0.575000
2025-09-14 02:26:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1980MB
2025-09-14 02:26:22 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 0 with val results: {'val_total': 200, 'val_loss': 137.68272399902344, 'val_avg_loss': 0.6884136199951172, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-14 02:26:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:26:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:26:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.279558, avg_loss=0.781989, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:26:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1980MB
2025-09-14 02:26:24 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.279558181762695, 'test_avg_loss': 0.7819889545440674, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:26:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:26:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:26:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.850082, avg_loss=0.679250, seen=200, correct=110, accuracy=0.550000
2025-09-14 02:26:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1938MB
2025-09-14 02:26:32 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 1 with val results: {'val_total': 200, 'val_loss': 135.85008239746094, 'val_avg_loss': 0.6792504119873047, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-14 02:26:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:26:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:26:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.554890, avg_loss=0.713872, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:26:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1938MB
2025-09-14 02:26:35 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.554889678955078, 'test_avg_loss': 0.713872241973877, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:26:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:26:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:26:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.167191, avg_loss=0.680836, seen=200, correct=115, accuracy=0.575000
2025-09-14 02:26:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1938MB
2025-09-14 02:26:43 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 2 with val results: {'val_total': 200, 'val_loss': 136.1671905517578, 'val_avg_loss': 0.680835952758789, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-14 02:26:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:26:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:26:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.634918, avg_loss=0.715873, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:26:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1938MB
2025-09-14 02:26:46 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.634918212890625, 'test_avg_loss': 0.7158729553222656, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:26:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:26:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:26:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.537308, avg_loss=0.697687, seen=200, correct=113, accuracy=0.565000
2025-09-14 02:26:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1938MB
2025-09-14 02:26:53 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 3 with val results: {'val_total': 200, 'val_loss': 139.5373077392578, 'val_avg_loss': 0.697686538696289, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-14 02:26:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:26:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:26:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:26:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.611609, avg_loss=0.790290, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:26:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:26:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:26:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1938MB
2025-09-14 02:26:56 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 3 with test results: {'test_total': 40, 'test_loss': 31.611608505249023, 'test_avg_loss': 0.7902902126312256, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:26:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:26:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:26:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:27:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.180389, avg_loss=0.695902, seen=200, correct=109, accuracy=0.545000
2025-09-14 02:27:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1938MB
2025-09-14 02:27:04 (federatedscope.llm.llm_local.client:480) INFO: Client 24 Adapter 4 with val results: {'val_total': 200, 'val_loss': 139.18038940429688, 'val_avg_loss': 0.6959019470214843, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:27:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:27:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.723948, avg_loss=0.793099, seen=40, correct=15, accuracy=0.375000
2025-09-14 02:27:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1938MB
2025-09-14 02:27:07 (federatedscope.llm.llm_local.client:501) INFO: Client 24 Adapter 4 with test results: {'test_total': 40, 'test_loss': 31.723947525024414, 'test_avg_loss': 0.7930986881256104, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-14 02:27:07 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:27:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:27:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:27:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=148.098618, avg_loss=0.740493, seen=200, correct=108, accuracy=0.540000
2025-09-14 02:27:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1989MB
2025-09-14 02:27:15 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 0 with val results: {'val_total': 200, 'val_loss': 148.09861755371094, 'val_avg_loss': 0.7404930877685547, 'val_seen': 200, 'val_correct': 108, 'val_acc': 0.54}
2025-09-14 02:27:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:27:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:27:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.764786, avg_loss=0.769120, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:27:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1989MB
2025-09-14 02:27:17 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.764785766601562, 'test_avg_loss': 0.7691196441650391, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:27:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:27:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:27:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.120956, avg_loss=0.705605, seen=200, correct=111, accuracy=0.555000
2025-09-14 02:27:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1947MB
2025-09-14 02:27:23 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 1 with val results: {'val_total': 200, 'val_loss': 141.12095642089844, 'val_avg_loss': 0.7056047821044922, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-14 02:27:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:27:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:27:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.888559, avg_loss=0.722214, seen=40, correct=15, accuracy=0.375000
2025-09-14 02:27:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1947MB
2025-09-14 02:27:26 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.888559341430664, 'test_avg_loss': 0.7222139835357666, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-14 02:27:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:27:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:27:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.392181, avg_loss=0.711961, seen=200, correct=102, accuracy=0.510000
2025-09-14 02:27:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1947MB
2025-09-14 02:27:35 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 2 with val results: {'val_total': 200, 'val_loss': 142.39218139648438, 'val_avg_loss': 0.7119609069824219, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-14 02:27:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:27:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:27:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.601406, avg_loss=0.690035, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:27:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1947MB
2025-09-14 02:27:38 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.60140609741211, 'test_avg_loss': 0.6900351524353028, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:27:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:27:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:27:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=155.084854, avg_loss=0.775424, seen=200, correct=99, accuracy=0.495000
2025-09-14 02:27:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1947MB
2025-09-14 02:27:47 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 3 with val results: {'val_total': 200, 'val_loss': 155.08485412597656, 'val_avg_loss': 0.7754242706298828, 'val_seen': 200, 'val_correct': 99, 'val_acc': 0.495}
2025-09-14 02:27:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:27:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:27:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.229019, avg_loss=0.705725, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:27:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1947MB
2025-09-14 02:27:49 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 3 with test results: {'test_total': 40, 'test_loss': 28.229019165039062, 'test_avg_loss': 0.7057254791259766, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:27:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:27:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:27:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=145.610565, avg_loss=0.728053, seen=200, correct=103, accuracy=0.515000
2025-09-14 02:27:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:27:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1947MB
2025-09-14 02:27:57 (federatedscope.llm.llm_local.client:480) INFO: Client 25 Adapter 4 with val results: {'val_total': 200, 'val_loss': 145.61056518554688, 'val_avg_loss': 0.7280528259277343, 'val_seen': 200, 'val_correct': 103, 'val_acc': 0.515}
2025-09-14 02:27:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:27:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:27:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:27:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=35.465637, avg_loss=0.886641, seen=40, correct=15, accuracy=0.375000
2025-09-14 02:27:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:27:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:27:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1947MB
2025-09-14 02:28:00 (federatedscope.llm.llm_local.client:501) INFO: Client 25 Adapter 4 with test results: {'test_total': 40, 'test_loss': 35.46563720703125, 'test_avg_loss': 0.8866409301757813, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-14 02:28:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:28:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 02:28:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 02:28:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=103.734650, avg_loss=0.644315, seen=161, correct=107, accuracy=0.664596
2025-09-14 02:28:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1997MB
2025-09-14 02:28:08 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 0 with val results: {'val_total': 161, 'val_loss': 103.73464965820312, 'val_avg_loss': 0.64431459415033, 'val_seen': 161, 'val_correct': 107, 'val_acc': 0.6645962732919255}
2025-09-14 02:28:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:28:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:28:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.471905, avg_loss=0.611798, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:28:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1997MB
2025-09-14 02:28:10 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 0 with test results: {'test_total': 40, 'test_loss': 24.471904754638672, 'test_avg_loss': 0.6117976188659668, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:28:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 02:28:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 02:28:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=104.510590, avg_loss=0.649134, seen=161, correct=104, accuracy=0.645963
2025-09-14 02:28:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1955MB
2025-09-14 02:28:18 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 1 with val results: {'val_total': 161, 'val_loss': 104.51058959960938, 'val_avg_loss': 0.6491340968919836, 'val_seen': 161, 'val_correct': 104, 'val_acc': 0.6459627329192547}
2025-09-14 02:28:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:28:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:28:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.519415, avg_loss=0.612985, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:28:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1955MB
2025-09-14 02:28:20 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.5194149017334, 'test_avg_loss': 0.612985372543335, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:28:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 02:28:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 02:28:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=104.746666, avg_loss=0.650600, seen=161, correct=101, accuracy=0.627329
2025-09-14 02:28:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1955MB
2025-09-14 02:28:27 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 2 with val results: {'val_total': 161, 'val_loss': 104.74666595458984, 'val_avg_loss': 0.6506004096558375, 'val_seen': 161, 'val_correct': 101, 'val_acc': 0.6273291925465838}
2025-09-14 02:28:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:28:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:28:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.600019, avg_loss=0.640000, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:28:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1955MB
2025-09-14 02:28:29 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.600019454956055, 'test_avg_loss': 0.6400004863739014, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:28:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 02:28:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 02:28:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=105.810684, avg_loss=0.657209, seen=161, correct=101, accuracy=0.627329
2025-09-14 02:28:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1955MB
2025-09-14 02:28:35 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 3 with val results: {'val_total': 161, 'val_loss': 105.81068420410156, 'val_avg_loss': 0.6572092186590159, 'val_seen': 161, 'val_correct': 101, 'val_acc': 0.6273291925465838}
2025-09-14 02:28:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:28:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:28:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.624046, avg_loss=0.665601, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:28:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1955MB
2025-09-14 02:28:38 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 3 with test results: {'test_total': 40, 'test_loss': 26.624046325683594, 'test_avg_loss': 0.6656011581420899, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:28:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 02:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 02:28:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=109.485397, avg_loss=0.680034, seen=161, correct=90, accuracy=0.559006
2025-09-14 02:28:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1955MB
2025-09-14 02:28:45 (federatedscope.llm.llm_local.client:480) INFO: Client 26 Adapter 4 with val results: {'val_total': 161, 'val_loss': 109.48539733886719, 'val_avg_loss': 0.6800335238438956, 'val_seen': 161, 'val_correct': 90, 'val_acc': 0.5590062111801242}
2025-09-14 02:28:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:28:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:28:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.543056, avg_loss=0.663576, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:28:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1955MB
2025-09-14 02:28:47 (federatedscope.llm.llm_local.client:501) INFO: Client 26 Adapter 4 with test results: {'test_total': 40, 'test_loss': 26.54305648803711, 'test_avg_loss': 0.6635764122009278, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:28:48 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:28:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 02:28:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 02:28:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=82.278519, avg_loss=0.668931, seen=123, correct=75, accuracy=0.609756
2025-09-14 02:28:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=2006MB
2025-09-14 02:28:53 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 0 with val results: {'val_total': 123, 'val_loss': 82.27851867675781, 'val_avg_loss': 0.6689310461525025, 'val_seen': 123, 'val_correct': 75, 'val_acc': 0.6097560975609756}
2025-09-14 02:28:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:28:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:28:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:28:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.132446, avg_loss=0.653311, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:28:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:28:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:28:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=2006MB
2025-09-14 02:28:56 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.1324462890625, 'test_avg_loss': 0.6533111572265625, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:28:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 02:28:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:28:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 02:29:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=82.842438, avg_loss=0.673516, seen=123, correct=70, accuracy=0.569106
2025-09-14 02:29:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1964MB
2025-09-14 02:29:01 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 1 with val results: {'val_total': 123, 'val_loss': 82.84243774414062, 'val_avg_loss': 0.6735157540174035, 'val_seen': 123, 'val_correct': 70, 'val_acc': 0.5691056910569106}
2025-09-14 02:29:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:29:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.948898, avg_loss=0.623722, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:29:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1964MB
2025-09-14 02:29:04 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 1 with test results: {'test_total': 40, 'test_loss': 24.948898315429688, 'test_avg_loss': 0.6237224578857422, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:29:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 02:29:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 02:29:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=83.439941, avg_loss=0.678374, seen=123, correct=73, accuracy=0.593496
2025-09-14 02:29:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1964MB
2025-09-14 02:29:10 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 2 with val results: {'val_total': 123, 'val_loss': 83.43994140625, 'val_avg_loss': 0.6783735073678862, 'val_seen': 123, 'val_correct': 73, 'val_acc': 0.5934959349593496}
2025-09-14 02:29:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:29:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.045509, avg_loss=0.651138, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:29:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1964MB
2025-09-14 02:29:13 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.045509338378906, 'test_avg_loss': 0.6511377334594727, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:29:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 02:29:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 02:29:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=87.479477, avg_loss=0.711215, seen=123, correct=72, accuracy=0.585366
2025-09-14 02:29:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1964MB
2025-09-14 02:29:18 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 3 with val results: {'val_total': 123, 'val_loss': 87.47947692871094, 'val_avg_loss': 0.7112152595830158, 'val_seen': 123, 'val_correct': 72, 'val_acc': 0.5853658536585366}
2025-09-14 02:29:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:29:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.993538, avg_loss=0.649838, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:29:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1964MB
2025-09-14 02:29:21 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 3 with test results: {'test_total': 40, 'test_loss': 25.99353790283203, 'test_avg_loss': 0.6498384475708008, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:29:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 02:29:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 02:29:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=88.463120, avg_loss=0.719212, seen=123, correct=58, accuracy=0.471545
2025-09-14 02:29:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1964MB
2025-09-14 02:29:27 (federatedscope.llm.llm_local.client:480) INFO: Client 27 Adapter 4 with val results: {'val_total': 123, 'val_loss': 88.46311950683594, 'val_avg_loss': 0.7192123537141133, 'val_seen': 123, 'val_correct': 58, 'val_acc': 0.4715447154471545}
2025-09-14 02:29:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:29:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.543156, avg_loss=0.813579, seen=40, correct=14, accuracy=0.350000
2025-09-14 02:29:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1964MB
2025-09-14 02:29:29 (federatedscope.llm.llm_local.client:501) INFO: Client 27 Adapter 4 with test results: {'test_total': 40, 'test_loss': 32.543155670166016, 'test_avg_loss': 0.8135788917541504, 'test_seen': 40, 'test_correct': 14, 'test_acc': 0.35}
2025-09-14 02:29:29 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:29:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 02:29:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:29:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=54.735916, avg_loss=0.729812, seen=75, correct=41, accuracy=0.546667
2025-09-14 02:29:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=2014MB
2025-09-14 02:29:34 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 0 with val results: {'val_total': 75, 'val_loss': 54.73591613769531, 'val_avg_loss': 0.7298122151692709, 'val_seen': 75, 'val_correct': 41, 'val_acc': 0.5466666666666666}
2025-09-14 02:29:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:29:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.614189, avg_loss=0.765355, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:29:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=2014MB
2025-09-14 02:29:36 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 0 with test results: {'test_total': 40, 'test_loss': 30.61418914794922, 'test_avg_loss': 0.7653547286987304, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:29:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 02:29:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:29:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.962555, avg_loss=0.652834, seen=75, correct=45, accuracy=0.600000
2025-09-14 02:29:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1972MB
2025-09-14 02:29:40 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 1 with val results: {'val_total': 75, 'val_loss': 48.962554931640625, 'val_avg_loss': 0.6528340657552083, 'val_seen': 75, 'val_correct': 45, 'val_acc': 0.6}
2025-09-14 02:29:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:29:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.138943, avg_loss=0.678474, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:29:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1972MB
2025-09-14 02:29:42 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.13894271850586, 'test_avg_loss': 0.6784735679626465, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:29:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 02:29:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:29:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.628834, avg_loss=0.661718, seen=75, correct=47, accuracy=0.626667
2025-09-14 02:29:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1972MB
2025-09-14 02:29:46 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 2 with val results: {'val_total': 75, 'val_loss': 49.62883377075195, 'val_avg_loss': 0.6617177836100261, 'val_seen': 75, 'val_correct': 47, 'val_acc': 0.6266666666666667}
2025-09-14 02:29:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:29:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.229773, avg_loss=0.705744, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:29:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1972MB
2025-09-14 02:29:49 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.229772567749023, 'test_avg_loss': 0.7057443141937256, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:29:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 02:29:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:29:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=50.816010, avg_loss=0.677547, seen=75, correct=43, accuracy=0.573333
2025-09-14 02:29:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1972MB
2025-09-14 02:29:52 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 3 with val results: {'val_total': 75, 'val_loss': 50.816009521484375, 'val_avg_loss': 0.6775467936197916, 'val_seen': 75, 'val_correct': 43, 'val_acc': 0.5733333333333334}
2025-09-14 02:29:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:29:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.975876, avg_loss=0.724397, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:29:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1972MB
2025-09-14 02:29:55 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 3 with test results: {'test_total': 40, 'test_loss': 28.975875854492188, 'test_avg_loss': 0.7243968963623046, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:29:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 02:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:29:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:29:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=52.741131, avg_loss=0.703215, seen=75, correct=36, accuracy=0.480000
2025-09-14 02:29:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:29:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1972MB
2025-09-14 02:29:59 (federatedscope.llm.llm_local.client:480) INFO: Client 28 Adapter 4 with val results: {'val_total': 75, 'val_loss': 52.74113082885742, 'val_avg_loss': 0.703215077718099, 'val_seen': 75, 'val_correct': 36, 'val_acc': 0.48}
2025-09-14 02:29:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:29:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:29:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:30:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.831085, avg_loss=0.695777, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:30:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1972MB
2025-09-14 02:30:01 (federatedscope.llm.llm_local.client:501) INFO: Client 28 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.831085205078125, 'test_avg_loss': 0.6957771301269531, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:30:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:30:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:30:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:30:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.886414, avg_loss=0.714432, seen=200, correct=113, accuracy=0.565000
2025-09-14 02:30:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=2023MB
2025-09-14 02:30:10 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 0 with val results: {'val_total': 200, 'val_loss': 142.88641357421875, 'val_avg_loss': 0.7144320678710937, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-14 02:30:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:30:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:30:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.114305, avg_loss=0.702858, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:30:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=2023MB
2025-09-14 02:30:13 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.11430549621582, 'test_avg_loss': 0.7028576374053955, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:30:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:30:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.639847, avg_loss=0.683199, seen=200, correct=117, accuracy=0.585000
2025-09-14 02:30:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1981MB
2025-09-14 02:30:20 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 1 with val results: {'val_total': 200, 'val_loss': 136.6398468017578, 'val_avg_loss': 0.683199234008789, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 02:30:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:30:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.746126, avg_loss=0.668653, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:30:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2038MB allocated=1981MB
2025-09-14 02:30:23 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.746126174926758, 'test_avg_loss': 0.668653154373169, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:30:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:30:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:30:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.846985, avg_loss=0.709235, seen=200, correct=107, accuracy=0.535000
2025-09-14 02:30:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1981MB
2025-09-14 02:30:31 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 2 with val results: {'val_total': 200, 'val_loss': 141.84698486328125, 'val_avg_loss': 0.7092349243164062, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-14 02:30:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:30:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:30:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.556568, avg_loss=0.713914, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:30:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1981MB
2025-09-14 02:30:34 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.556568145751953, 'test_avg_loss': 0.7139142036437989, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:30:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:30:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:30:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=149.262100, avg_loss=0.746311, seen=200, correct=109, accuracy=0.545000
2025-09-14 02:30:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1981MB
2025-09-14 02:30:41 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 3 with val results: {'val_total': 200, 'val_loss': 149.26210021972656, 'val_avg_loss': 0.7463105010986328, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:30:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:30:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.244202, avg_loss=0.781105, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:30:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1981MB
2025-09-14 02:30:44 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 3 with test results: {'test_total': 40, 'test_loss': 31.24420166015625, 'test_avg_loss': 0.7811050415039062, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:30:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:30:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:30:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=144.924713, avg_loss=0.724624, seen=200, correct=101, accuracy=0.505000
2025-09-14 02:30:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1981MB
2025-09-14 02:30:51 (federatedscope.llm.llm_local.client:480) INFO: Client 29 Adapter 4 with val results: {'val_total': 200, 'val_loss': 144.92471313476562, 'val_avg_loss': 0.7246235656738281, 'val_seen': 200, 'val_correct': 101, 'val_acc': 0.505}
2025-09-14 02:30:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:30:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:30:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.284105, avg_loss=0.607103, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:30:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:30:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2036MB allocated=1981MB
2025-09-14 02:30:53 (federatedscope.llm.llm_local.client:501) INFO: Client 29 Adapter 4 with test results: {'test_total': 40, 'test_loss': 24.28410530090332, 'test_avg_loss': 0.607102632522583, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:30:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:30:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 02:30:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:30:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 02:30:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=115.100983, avg_loss=0.677065, seen=170, correct=104, accuracy=0.611765
2025-09-14 02:30:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:30:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2031MB
2025-09-14 02:31:00 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 0 with val results: {'val_total': 170, 'val_loss': 115.10098266601562, 'val_avg_loss': 0.6770646039177389, 'val_seen': 170, 'val_correct': 104, 'val_acc': 0.611764705882353}
2025-09-14 02:31:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:31:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:31:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.432686, avg_loss=0.660817, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:31:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2031MB
2025-09-14 02:31:03 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.43268585205078, 'test_avg_loss': 0.6608171463012695, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:31:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 02:31:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 02:31:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.351288, avg_loss=0.660890, seen=170, correct=99, accuracy=0.582353
2025-09-14 02:31:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=1989MB
2025-09-14 02:31:09 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 1 with val results: {'val_total': 170, 'val_loss': 112.35128784179688, 'val_avg_loss': 0.6608899284811581, 'val_seen': 170, 'val_correct': 99, 'val_acc': 0.5823529411764706}
2025-09-14 02:31:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:31:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:31:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.057709, avg_loss=0.676443, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:31:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=1989MB
2025-09-14 02:31:12 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.057708740234375, 'test_avg_loss': 0.6764427185058594, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:31:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 02:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 02:31:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=115.248085, avg_loss=0.677930, seen=170, correct=92, accuracy=0.541176
2025-09-14 02:31:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=1989MB
2025-09-14 02:31:18 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 2 with val results: {'val_total': 170, 'val_loss': 115.24808502197266, 'val_avg_loss': 0.6779299118939568, 'val_seen': 170, 'val_correct': 92, 'val_acc': 0.5411764705882353}
2025-09-14 02:31:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:31:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:31:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.647623, avg_loss=0.641191, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:31:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=1989MB
2025-09-14 02:31:21 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.64762306213379, 'test_avg_loss': 0.6411905765533448, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:31:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 02:31:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 02:31:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=120.915390, avg_loss=0.711267, seen=170, correct=93, accuracy=0.547059
2025-09-14 02:31:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1989MB
2025-09-14 02:31:28 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 3 with val results: {'val_total': 170, 'val_loss': 120.91539001464844, 'val_avg_loss': 0.7112670000861673, 'val_seen': 170, 'val_correct': 93, 'val_acc': 0.5470588235294118}
2025-09-14 02:31:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:31:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:31:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.130907, avg_loss=0.653273, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:31:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1989MB
2025-09-14 02:31:31 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 3 with test results: {'test_total': 40, 'test_loss': 26.13090705871582, 'test_avg_loss': 0.6532726764678956, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:31:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 02:31:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 02:31:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=119.124519, avg_loss=0.700732, seen=170, correct=90, accuracy=0.529412
2025-09-14 02:31:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1989MB
2025-09-14 02:31:38 (federatedscope.llm.llm_local.client:480) INFO: Client 30 Adapter 4 with val results: {'val_total': 170, 'val_loss': 119.12451934814453, 'val_avg_loss': 0.7007324667537913, 'val_seen': 170, 'val_correct': 90, 'val_acc': 0.5294117647058824}
2025-09-14 02:31:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:31:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:31:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.469311, avg_loss=0.736733, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:31:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1989MB
2025-09-14 02:31:41 (federatedscope.llm.llm_local.client:501) INFO: Client 30 Adapter 4 with test results: {'test_total': 40, 'test_loss': 29.469310760498047, 'test_avg_loss': 0.7367327690124512, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:31:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:31:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 02:31:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 02:31:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=151.961319, avg_loss=0.787364, seen=193, correct=98, accuracy=0.507772
2025-09-14 02:31:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2040MB
2025-09-14 02:31:49 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 0 with val results: {'val_total': 193, 'val_loss': 151.96131896972656, 'val_avg_loss': 0.7873643469934019, 'val_seen': 193, 'val_correct': 98, 'val_acc': 0.5077720207253886}
2025-09-14 02:31:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:31:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:31:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.464411, avg_loss=0.636610, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:31:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:31:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2040MB
2025-09-14 02:31:52 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.46441078186035, 'test_avg_loss': 0.6366102695465088, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:31:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 02:31:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:31:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:31:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 02:31:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=138.833252, avg_loss=0.719343, seen=193, correct=95, accuracy=0.492228
2025-09-14 02:31:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:31:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=1998MB
2025-09-14 02:32:01 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 1 with val results: {'val_total': 193, 'val_loss': 138.833251953125, 'val_avg_loss': 0.7193432743685233, 'val_seen': 193, 'val_correct': 95, 'val_acc': 0.49222797927461137}
2025-09-14 02:32:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:32:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:32:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.033360, avg_loss=0.650834, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:32:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=1998MB
2025-09-14 02:32:03 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.03335952758789, 'test_avg_loss': 0.6508339881896973, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:32:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 02:32:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 02:32:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=139.760071, avg_loss=0.724145, seen=193, correct=83, accuracy=0.430052
2025-09-14 02:32:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1998MB
2025-09-14 02:32:11 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 2 with val results: {'val_total': 193, 'val_loss': 139.76007080078125, 'val_avg_loss': 0.7241454445636334, 'val_seen': 193, 'val_correct': 83, 'val_acc': 0.43005181347150256}
2025-09-14 02:32:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:32:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:32:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.536940, avg_loss=0.663423, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:32:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1998MB
2025-09-14 02:32:13 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.53693962097168, 'test_avg_loss': 0.663423490524292, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:32:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 02:32:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 02:32:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=153.610031, avg_loss=0.795907, seen=193, correct=83, accuracy=0.430052
2025-09-14 02:32:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1998MB
2025-09-14 02:32:22 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 3 with val results: {'val_total': 193, 'val_loss': 153.6100311279297, 'val_avg_loss': 0.7959068970359051, 'val_seen': 193, 'val_correct': 83, 'val_acc': 0.43005181347150256}
2025-09-14 02:32:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:32:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:32:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.862566, avg_loss=0.696564, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:32:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1998MB
2025-09-14 02:32:24 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 3 with test results: {'test_total': 40, 'test_loss': 27.862565994262695, 'test_avg_loss': 0.6965641498565673, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:32:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 02:32:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 02:32:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.456726, avg_loss=0.691486, seen=193, correct=115, accuracy=0.595855
2025-09-14 02:32:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1998MB
2025-09-14 02:32:31 (federatedscope.llm.llm_local.client:480) INFO: Client 31 Adapter 4 with val results: {'val_total': 193, 'val_loss': 133.45672607421875, 'val_avg_loss': 0.6914856273275582, 'val_seen': 193, 'val_correct': 115, 'val_acc': 0.5958549222797928}
2025-09-14 02:32:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:32:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:32:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.524927, avg_loss=0.688123, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:32:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=1998MB
2025-09-14 02:32:34 (federatedscope.llm.llm_local.client:501) INFO: Client 31 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.524927139282227, 'test_avg_loss': 0.6881231784820556, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:32:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:32:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 02:32:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:32:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=68.956009, avg_loss=0.615679, seen=112, correct=77, accuracy=0.687500
2025-09-14 02:32:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2049MB
2025-09-14 02:32:40 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 0 with val results: {'val_total': 112, 'val_loss': 68.95600891113281, 'val_avg_loss': 0.6156786509922573, 'val_seen': 112, 'val_correct': 77, 'val_acc': 0.6875}
2025-09-14 02:32:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:32:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:32:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.520073, avg_loss=0.713002, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:32:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2049MB
2025-09-14 02:32:42 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.52007293701172, 'test_avg_loss': 0.713001823425293, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:32:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 02:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:32:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=70.796448, avg_loss=0.632111, seen=112, correct=74, accuracy=0.660714
2025-09-14 02:32:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2007MB
2025-09-14 02:32:47 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 1 with val results: {'val_total': 112, 'val_loss': 70.79644775390625, 'val_avg_loss': 0.6321111406598773, 'val_seen': 112, 'val_correct': 74, 'val_acc': 0.6607142857142857}
2025-09-14 02:32:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:32:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:32:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.686430, avg_loss=0.742161, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:32:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2007MB
2025-09-14 02:32:49 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.686429977416992, 'test_avg_loss': 0.7421607494354248, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:32:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 02:32:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:32:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=74.736267, avg_loss=0.667288, seen=112, correct=67, accuracy=0.598214
2025-09-14 02:32:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2007MB
2025-09-14 02:32:54 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 2 with val results: {'val_total': 112, 'val_loss': 74.73626708984375, 'val_avg_loss': 0.667288099016462, 'val_seen': 112, 'val_correct': 67, 'val_acc': 0.5982142857142857}
2025-09-14 02:32:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:32:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:32:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:32:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.867176, avg_loss=0.721679, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:32:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:32:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:32:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2007MB
2025-09-14 02:32:57 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.867176055908203, 'test_avg_loss': 0.721679401397705, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:32:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 02:32:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:32:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:33:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=75.119774, avg_loss=0.670712, seen=112, correct=62, accuracy=0.553571
2025-09-14 02:33:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2007MB
2025-09-14 02:33:03 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 3 with val results: {'val_total': 112, 'val_loss': 75.1197738647461, 'val_avg_loss': 0.6707122666495187, 'val_seen': 112, 'val_correct': 62, 'val_acc': 0.5535714285714286}
2025-09-14 02:33:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:33:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:33:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.470161, avg_loss=0.761754, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:33:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2007MB
2025-09-14 02:33:05 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 3 with test results: {'test_total': 40, 'test_loss': 30.47016143798828, 'test_avg_loss': 0.761754035949707, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:33:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 02:33:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:33:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=73.935516, avg_loss=0.660139, seen=112, correct=68, accuracy=0.607143
2025-09-14 02:33:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2007MB
2025-09-14 02:33:10 (federatedscope.llm.llm_local.client:480) INFO: Client 32 Adapter 4 with val results: {'val_total': 112, 'val_loss': 73.93551635742188, 'val_avg_loss': 0.6601385389055524, 'val_seen': 112, 'val_correct': 68, 'val_acc': 0.6071428571428571}
2025-09-14 02:33:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:33:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:33:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.530676, avg_loss=0.688267, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:33:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2007MB
2025-09-14 02:33:12 (federatedscope.llm.llm_local.client:501) INFO: Client 32 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.530675888061523, 'test_avg_loss': 0.6882668972015381, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:33:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:33:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:33:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:33:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=57.886105, avg_loss=0.782245, seen=74, correct=33, accuracy=0.445946
2025-09-14 02:33:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2057MB
2025-09-14 02:33:16 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 0 with val results: {'val_total': 74, 'val_loss': 57.886104583740234, 'val_avg_loss': 0.7822446565370302, 'val_seen': 74, 'val_correct': 33, 'val_acc': 0.44594594594594594}
2025-09-14 02:33:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:33:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:33:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.518728, avg_loss=0.637968, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:33:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2057MB
2025-09-14 02:33:19 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.518728256225586, 'test_avg_loss': 0.6379682064056397, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:33:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:33:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:33:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.780006, avg_loss=0.699730, seen=74, correct=41, accuracy=0.554054
2025-09-14 02:33:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2015MB
2025-09-14 02:33:23 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 1 with val results: {'val_total': 74, 'val_loss': 51.780006408691406, 'val_avg_loss': 0.6997298163336676, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}
2025-09-14 02:33:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:33:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:33:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.651522, avg_loss=0.666288, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:33:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2015MB
2025-09-14 02:33:25 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.651521682739258, 'test_avg_loss': 0.6662880420684815, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:33:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:33:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.531822, avg_loss=0.709889, seen=74, correct=37, accuracy=0.500000
2025-09-14 02:33:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2015MB
2025-09-14 02:33:29 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 2 with val results: {'val_total': 74, 'val_loss': 52.531822204589844, 'val_avg_loss': 0.7098894892512141, 'val_seen': 74, 'val_correct': 37, 'val_acc': 0.5}
2025-09-14 02:33:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:33:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:33:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.671854, avg_loss=0.666796, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:33:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2015MB
2025-09-14 02:33:31 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.67185401916504, 'test_avg_loss': 0.666796350479126, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:33:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:33:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:33:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=57.405205, avg_loss=0.775746, seen=74, correct=34, accuracy=0.459459
2025-09-14 02:33:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2015MB
2025-09-14 02:33:35 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 3 with val results: {'val_total': 74, 'val_loss': 57.40520477294922, 'val_avg_loss': 0.7757460104452597, 'val_seen': 74, 'val_correct': 34, 'val_acc': 0.4594594594594595}
2025-09-14 02:33:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:33:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:33:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.044960, avg_loss=0.676124, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:33:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2015MB
2025-09-14 02:33:37 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 3 with test results: {'test_total': 40, 'test_loss': 27.044960021972656, 'test_avg_loss': 0.6761240005493164, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:33:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:33:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:33:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.959026, avg_loss=0.715663, seen=74, correct=42, accuracy=0.567568
2025-09-14 02:33:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2015MB
2025-09-14 02:33:41 (federatedscope.llm.llm_local.client:480) INFO: Client 33 Adapter 4 with val results: {'val_total': 74, 'val_loss': 52.95902633666992, 'val_avg_loss': 0.7156625180631071, 'val_seen': 74, 'val_correct': 42, 'val_acc': 0.5675675675675675}
2025-09-14 02:33:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:33:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:33:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.457222, avg_loss=0.711431, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:33:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2015MB
2025-09-14 02:33:44 (federatedscope.llm.llm_local.client:501) INFO: Client 33 Adapter 4 with test results: {'test_total': 40, 'test_loss': 28.45722198486328, 'test_avg_loss': 0.711430549621582, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:33:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:33:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:33:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:33:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.725433, avg_loss=0.648627, seen=200, correct=130, accuracy=0.650000
2025-09-14 02:33:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2065MB
2025-09-14 02:33:52 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 0 with val results: {'val_total': 200, 'val_loss': 129.72543334960938, 'val_avg_loss': 0.6486271667480469, 'val_seen': 200, 'val_correct': 130, 'val_acc': 0.65}
2025-09-14 02:33:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:33:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:33:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.827240, avg_loss=0.645681, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:33:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:33:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2065MB
2025-09-14 02:33:54 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.827239990234375, 'test_avg_loss': 0.6456809997558594, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:33:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:33:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:33:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:34:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.123962, avg_loss=0.660620, seen=200, correct=118, accuracy=0.590000
2025-09-14 02:34:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2023MB
2025-09-14 02:34:02 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 1 with val results: {'val_total': 200, 'val_loss': 132.12396240234375, 'val_avg_loss': 0.6606198120117187, 'val_seen': 200, 'val_correct': 118, 'val_acc': 0.59}
2025-09-14 02:34:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:34:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:34:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.083776, avg_loss=0.652094, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:34:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2023MB
2025-09-14 02:34:04 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.083776473999023, 'test_avg_loss': 0.6520944118499756, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:34:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:34:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:34:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.659760, avg_loss=0.683299, seen=200, correct=115, accuracy=0.575000
2025-09-14 02:34:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2023MB
2025-09-14 02:34:12 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 2 with val results: {'val_total': 200, 'val_loss': 136.65975952148438, 'val_avg_loss': 0.6832987976074218, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-14 02:34:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:34:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:34:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.814522, avg_loss=0.670363, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:34:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2023MB
2025-09-14 02:34:14 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.81452178955078, 'test_avg_loss': 0.6703630447387695, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:34:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:34:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:34:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.005142, avg_loss=0.690026, seen=200, correct=112, accuracy=0.560000
2025-09-14 02:34:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2023MB
2025-09-14 02:34:23 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 3 with val results: {'val_total': 200, 'val_loss': 138.00514221191406, 'val_avg_loss': 0.6900257110595703, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-14 02:34:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:34:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:34:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.695435, avg_loss=0.717386, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:34:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2023MB
2025-09-14 02:34:26 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 3 with test results: {'test_total': 40, 'test_loss': 28.6954345703125, 'test_avg_loss': 0.7173858642578125, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:34:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:34:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.532043, avg_loss=0.692660, seen=200, correct=108, accuracy=0.540000
2025-09-14 02:34:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2023MB
2025-09-14 02:34:34 (federatedscope.llm.llm_local.client:480) INFO: Client 34 Adapter 4 with val results: {'val_total': 200, 'val_loss': 138.53204345703125, 'val_avg_loss': 0.6926602172851563, 'val_seen': 200, 'val_correct': 108, 'val_acc': 0.54}
2025-09-14 02:34:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:34:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:34:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.989891, avg_loss=0.624747, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:34:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2023MB
2025-09-14 02:34:36 (federatedscope.llm.llm_local.client:501) INFO: Client 34 Adapter 4 with test results: {'test_total': 40, 'test_loss': 24.989891052246094, 'test_avg_loss': 0.6247472763061523, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:34:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:34:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:34:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:34:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.754715, avg_loss=0.623774, seen=200, correct=124, accuracy=0.620000
2025-09-14 02:34:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2074MB
2025-09-14 02:34:44 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 0 with val results: {'val_total': 200, 'val_loss': 124.75471496582031, 'val_avg_loss': 0.6237735748291016, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-14 02:34:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:34:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.622023, avg_loss=0.640551, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:34:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2074MB
2025-09-14 02:34:46 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.62202262878418, 'test_avg_loss': 0.6405505657196044, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:34:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:34:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:34:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.460999, avg_loss=0.657305, seen=200, correct=122, accuracy=0.610000
2025-09-14 02:34:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2032MB
2025-09-14 02:34:54 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 1 with val results: {'val_total': 200, 'val_loss': 131.46099853515625, 'val_avg_loss': 0.6573049926757812, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-14 02:34:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:34:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:34:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:34:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.810146, avg_loss=0.670254, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:34:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:34:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:34:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2032MB
2025-09-14 02:34:56 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.81014633178711, 'test_avg_loss': 0.6702536582946778, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:34:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:34:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:34:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:35:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.540466, avg_loss=0.662702, seen=200, correct=121, accuracy=0.605000
2025-09-14 02:35:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2032MB
2025-09-14 02:35:03 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 2 with val results: {'val_total': 200, 'val_loss': 132.54046630859375, 'val_avg_loss': 0.6627023315429688, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 02:35:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:35:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:35:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.104061, avg_loss=0.677602, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:35:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2032MB
2025-09-14 02:35:06 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.104061126708984, 'test_avg_loss': 0.6776015281677246, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:35:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:35:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:35:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.872437, avg_loss=0.644362, seen=200, correct=130, accuracy=0.650000
2025-09-14 02:35:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2032MB
2025-09-14 02:35:13 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 3 with val results: {'val_total': 200, 'val_loss': 128.8724365234375, 'val_avg_loss': 0.6443621826171875, 'val_seen': 200, 'val_correct': 130, 'val_acc': 0.65}
2025-09-14 02:35:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:35:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.759356, avg_loss=0.668984, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:35:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2032MB
2025-09-14 02:35:15 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 3 with test results: {'test_total': 40, 'test_loss': 26.759355545043945, 'test_avg_loss': 0.6689838886260986, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:35:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:35:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=151.069992, avg_loss=0.755350, seen=200, correct=92, accuracy=0.460000
2025-09-14 02:35:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2032MB
2025-09-14 02:35:21 (federatedscope.llm.llm_local.client:480) INFO: Client 35 Adapter 4 with val results: {'val_total': 200, 'val_loss': 151.0699920654297, 'val_avg_loss': 0.7553499603271484, 'val_seen': 200, 'val_correct': 92, 'val_acc': 0.46}
2025-09-14 02:35:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:35:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:35:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.431412, avg_loss=0.785785, seen=40, correct=16, accuracy=0.400000
2025-09-14 02:35:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2032MB
2025-09-14 02:35:24 (federatedscope.llm.llm_local.client:501) INFO: Client 35 Adapter 4 with test results: {'test_total': 40, 'test_loss': 31.431411743164062, 'test_avg_loss': 0.7857852935791015, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-14 02:35:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:35:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 02:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 02:35:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=39.367416, avg_loss=0.729026, seen=54, correct=33, accuracy=0.611111
2025-09-14 02:35:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2082MB
2025-09-14 02:35:29 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 0 with val results: {'val_total': 54, 'val_loss': 39.36741638183594, 'val_avg_loss': 0.7290262292932581, 'val_seen': 54, 'val_correct': 33, 'val_acc': 0.6111111111111112}
2025-09-14 02:35:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:35:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:35:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.980698, avg_loss=0.674517, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:35:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2082MB
2025-09-14 02:35:31 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.980697631835938, 'test_avg_loss': 0.6745174407958985, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:35:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 02:35:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 02:35:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=37.132927, avg_loss=0.687647, seen=54, correct=29, accuracy=0.537037
2025-09-14 02:35:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2040MB
2025-09-14 02:35:34 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 1 with val results: {'val_total': 54, 'val_loss': 37.13292694091797, 'val_avg_loss': 0.6876467952021846, 'val_seen': 54, 'val_correct': 29, 'val_acc': 0.5370370370370371}
2025-09-14 02:35:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:35:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:35:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.713327, avg_loss=0.642833, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:35:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2040MB
2025-09-14 02:35:37 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.713327407836914, 'test_avg_loss': 0.6428331851959228, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:35:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 02:35:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 02:35:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.958366, avg_loss=0.665896, seen=54, correct=34, accuracy=0.629630
2025-09-14 02:35:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2040MB
2025-09-14 02:35:40 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 2 with val results: {'val_total': 54, 'val_loss': 35.95836639404297, 'val_avg_loss': 0.6658956739637587, 'val_seen': 54, 'val_correct': 34, 'val_acc': 0.6296296296296297}
2025-09-14 02:35:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:35:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:35:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.752499, avg_loss=0.668812, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:35:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2040MB
2025-09-14 02:35:43 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.752498626708984, 'test_avg_loss': 0.6688124656677246, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:35:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 02:35:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 02:35:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=37.830601, avg_loss=0.700567, seen=54, correct=28, accuracy=0.518519
2025-09-14 02:35:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2040MB
2025-09-14 02:35:46 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 3 with val results: {'val_total': 54, 'val_loss': 37.83060073852539, 'val_avg_loss': 0.7005666803430628, 'val_seen': 54, 'val_correct': 28, 'val_acc': 0.5185185185185185}
2025-09-14 02:35:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:35:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:35:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.303957, avg_loss=0.732599, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:35:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2040MB
2025-09-14 02:35:49 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 3 with test results: {'test_total': 40, 'test_loss': 29.303956985473633, 'test_avg_loss': 0.7325989246368408, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:35:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 02:35:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 02:35:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=38.479290, avg_loss=0.712579, seen=54, correct=27, accuracy=0.500000
2025-09-14 02:35:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2040MB
2025-09-14 02:35:53 (federatedscope.llm.llm_local.client:480) INFO: Client 36 Adapter 4 with val results: {'val_total': 54, 'val_loss': 38.47929000854492, 'val_avg_loss': 0.7125794446026837, 'val_seen': 54, 'val_correct': 27, 'val_acc': 0.5}
2025-09-14 02:35:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:35:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:35:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:35:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.142776, avg_loss=0.653569, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:35:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:35:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:35:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2040MB
2025-09-14 02:35:55 (federatedscope.llm.llm_local.client:501) INFO: Client 36 Adapter 4 with test results: {'test_total': 40, 'test_loss': 26.142776489257812, 'test_avg_loss': 0.6535694122314453, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:35:56 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:35:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:35:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:35:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:36:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.485260, avg_loss=0.707426, seen=200, correct=118, accuracy=0.590000
2025-09-14 02:36:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2091MB
2025-09-14 02:36:04 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 0 with val results: {'val_total': 200, 'val_loss': 141.48526000976562, 'val_avg_loss': 0.7074263000488281, 'val_seen': 200, 'val_correct': 118, 'val_acc': 0.59}
2025-09-14 02:36:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:36:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:36:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.473568, avg_loss=0.711839, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:36:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2091MB
2025-09-14 02:36:07 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.473567962646484, 'test_avg_loss': 0.7118391990661621, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:36:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:36:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:36:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.683929, avg_loss=0.678420, seen=200, correct=117, accuracy=0.585000
2025-09-14 02:36:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2049MB
2025-09-14 02:36:16 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 1 with val results: {'val_total': 200, 'val_loss': 135.68392944335938, 'val_avg_loss': 0.6784196472167969, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 02:36:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:36:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:36:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.018730, avg_loss=0.675468, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:36:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2049MB
2025-09-14 02:36:18 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.01873016357422, 'test_avg_loss': 0.6754682540893555, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:36:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:36:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:36:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.696793, avg_loss=0.693484, seen=200, correct=109, accuracy=0.545000
2025-09-14 02:36:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2049MB
2025-09-14 02:36:27 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 2 with val results: {'val_total': 200, 'val_loss': 138.69679260253906, 'val_avg_loss': 0.6934839630126953, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:36:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:36:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:36:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.690212, avg_loss=0.717255, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:36:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2049MB
2025-09-14 02:36:30 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.69021224975586, 'test_avg_loss': 0.7172553062438964, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:36:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:36:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=146.792603, avg_loss=0.733963, seen=200, correct=109, accuracy=0.545000
2025-09-14 02:36:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2049MB
2025-09-14 02:36:38 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 3 with val results: {'val_total': 200, 'val_loss': 146.7926025390625, 'val_avg_loss': 0.7339630126953125, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:36:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:36:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.719641, avg_loss=0.742991, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:36:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2049MB
2025-09-14 02:36:41 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 3 with test results: {'test_total': 40, 'test_loss': 29.719640731811523, 'test_avg_loss': 0.7429910182952881, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:36:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:36:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:36:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.503311, avg_loss=0.712517, seen=200, correct=109, accuracy=0.545000
2025-09-14 02:36:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2049MB
2025-09-14 02:36:47 (federatedscope.llm.llm_local.client:480) INFO: Client 37 Adapter 4 with val results: {'val_total': 200, 'val_loss': 142.50331115722656, 'val_avg_loss': 0.7125165557861328, 'val_seen': 200, 'val_correct': 109, 'val_acc': 0.545}
2025-09-14 02:36:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:36:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.211193, avg_loss=0.705280, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:36:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2049MB
2025-09-14 02:36:49 (federatedscope.llm.llm_local.client:501) INFO: Client 37 Adapter 4 with test results: {'test_total': 40, 'test_loss': 28.211193084716797, 'test_avg_loss': 0.7052798271179199, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:36:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:36:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:36:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:36:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:36:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.836655, avg_loss=0.644183, seen=200, correct=119, accuracy=0.595000
2025-09-14 02:36:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:36:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:36:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2099MB
2025-09-14 02:36:59 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 0 with val results: {'val_total': 200, 'val_loss': 128.83665466308594, 'val_avg_loss': 0.6441832733154297, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-14 02:36:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:36:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:36:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:37:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.058422, avg_loss=0.626461, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:37:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2099MB
2025-09-14 02:37:02 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 0 with test results: {'test_total': 40, 'test_loss': 25.058422088623047, 'test_avg_loss': 0.6264605522155762, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:37:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:37:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:37:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.347443, avg_loss=0.666737, seen=200, correct=119, accuracy=0.595000
2025-09-14 02:37:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2057MB
2025-09-14 02:37:10 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 1 with val results: {'val_total': 200, 'val_loss': 133.34744262695312, 'val_avg_loss': 0.6667372131347656, 'val_seen': 200, 'val_correct': 119, 'val_acc': 0.595}
2025-09-14 02:37:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:37:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:37:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.539406, avg_loss=0.663485, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:37:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2057MB
2025-09-14 02:37:13 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.539405822753906, 'test_avg_loss': 0.6634851455688476, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:37:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:37:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.170380, avg_loss=0.675852, seen=200, correct=116, accuracy=0.580000
2025-09-14 02:37:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2057MB
2025-09-14 02:37:21 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.17037963867188, 'val_avg_loss': 0.6758518981933593, 'val_seen': 200, 'val_correct': 116, 'val_acc': 0.58}
2025-09-14 02:37:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:37:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:37:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.509224, avg_loss=0.637731, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:37:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2057MB
2025-09-14 02:37:24 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.50922393798828, 'test_avg_loss': 0.637730598449707, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:37:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:37:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:37:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.122665, avg_loss=0.670613, seen=200, correct=121, accuracy=0.605000
2025-09-14 02:37:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2057MB
2025-09-14 02:37:32 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 3 with val results: {'val_total': 200, 'val_loss': 134.12266540527344, 'val_avg_loss': 0.6706133270263672, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 02:37:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:37:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:37:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.998940, avg_loss=0.624973, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:37:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2057MB
2025-09-14 02:37:35 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 3 with test results: {'test_total': 40, 'test_loss': 24.998939514160156, 'test_avg_loss': 0.6249734878540039, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:37:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:37:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:37:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=144.708450, avg_loss=0.723542, seen=200, correct=100, accuracy=0.500000
2025-09-14 02:37:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2057MB
2025-09-14 02:37:42 (federatedscope.llm.llm_local.client:480) INFO: Client 38 Adapter 4 with val results: {'val_total': 200, 'val_loss': 144.7084503173828, 'val_avg_loss': 0.7235422515869141, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-14 02:37:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:37:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:37:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.575287, avg_loss=0.739382, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:37:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2057MB
2025-09-14 02:37:45 (federatedscope.llm.llm_local.client:501) INFO: Client 38 Adapter 4 with test results: {'test_total': 40, 'test_loss': 29.575286865234375, 'test_avg_loss': 0.7393821716308594, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:37:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:37:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:37:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=61.258961, avg_loss=0.738060, seen=83, correct=44, accuracy=0.530120
2025-09-14 02:37:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2108MB
2025-09-14 02:37:50 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 0 with val results: {'val_total': 83, 'val_loss': 61.25896072387695, 'val_avg_loss': 0.7380597677575537, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-14 02:37:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:37:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.126003, avg_loss=0.778150, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:37:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2108MB
2025-09-14 02:37:53 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.12600326538086, 'test_avg_loss': 0.7781500816345215, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:37:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:37:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:37:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.331146, avg_loss=0.714833, seen=83, correct=43, accuracy=0.518072
2025-09-14 02:37:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2066MB
2025-09-14 02:37:57 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 1 with val results: {'val_total': 83, 'val_loss': 59.331146240234375, 'val_avg_loss': 0.7148330872317394, 'val_seen': 83, 'val_correct': 43, 'val_acc': 0.5180722891566265}
2025-09-14 02:37:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:37:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:37:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:37:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.839745, avg_loss=0.745994, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:37:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:37:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:37:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:37:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2066MB
2025-09-14 02:37:59 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.839744567871094, 'test_avg_loss': 0.7459936141967773, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:38:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:38:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:38:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.038574, avg_loss=0.699260, seen=83, correct=42, accuracy=0.506024
2025-09-14 02:38:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2066MB
2025-09-14 02:38:03 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 2 with val results: {'val_total': 83, 'val_loss': 58.03857421875, 'val_avg_loss': 0.6992599303463856, 'val_seen': 83, 'val_correct': 42, 'val_acc': 0.5060240963855421}
2025-09-14 02:38:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:38:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:38:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.949343, avg_loss=0.723734, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:38:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2066MB
2025-09-14 02:38:05 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.949342727661133, 'test_avg_loss': 0.7237335681915283, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:38:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:38:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:38:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=61.842239, avg_loss=0.745087, seen=83, correct=45, accuracy=0.542169
2025-09-14 02:38:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2066MB
2025-09-14 02:38:10 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 3 with val results: {'val_total': 83, 'val_loss': 61.84223937988281, 'val_avg_loss': 0.7450872214443712, 'val_seen': 83, 'val_correct': 45, 'val_acc': 0.5421686746987951}
2025-09-14 02:38:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:38:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.014158, avg_loss=0.775354, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:38:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2066MB
2025-09-14 02:38:12 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 3 with test results: {'test_total': 40, 'test_loss': 31.014158248901367, 'test_avg_loss': 0.7753539562225342, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:38:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:38:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=62.160431, avg_loss=0.748921, seen=83, correct=44, accuracy=0.530120
2025-09-14 02:38:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2066MB
2025-09-14 02:38:16 (federatedscope.llm.llm_local.client:480) INFO: Client 39 Adapter 4 with val results: {'val_total': 83, 'val_loss': 62.160430908203125, 'val_avg_loss': 0.7489208543157003, 'val_seen': 83, 'val_correct': 44, 'val_acc': 0.5301204819277109}
2025-09-14 02:38:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:38:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:38:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.852839, avg_loss=0.771321, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:38:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2066MB
2025-09-14 02:38:18 (federatedscope.llm.llm_local.client:501) INFO: Client 39 Adapter 4 with test results: {'test_total': 40, 'test_loss': 30.85283851623535, 'test_avg_loss': 0.7713209629058838, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:38:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:38:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:38:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:38:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.034286, avg_loss=0.690171, seen=200, correct=115, accuracy=0.575000
2025-09-14 02:38:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2116MB
2025-09-14 02:38:26 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 0 with val results: {'val_total': 200, 'val_loss': 138.03428649902344, 'val_avg_loss': 0.6901714324951171, 'val_seen': 200, 'val_correct': 115, 'val_acc': 0.575}
2025-09-14 02:38:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:38:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.389767, avg_loss=0.709744, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:38:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2116MB
2025-09-14 02:38:29 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.389766693115234, 'test_avg_loss': 0.7097441673278808, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:38:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:38:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:38:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.944061, avg_loss=0.649720, seen=200, correct=121, accuracy=0.605000
2025-09-14 02:38:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2074MB
2025-09-14 02:38:36 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 1 with val results: {'val_total': 200, 'val_loss': 129.94406127929688, 'val_avg_loss': 0.6497203063964844, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 02:38:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:38:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:38:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.591911, avg_loss=0.664798, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:38:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2074MB
2025-09-14 02:38:38 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.59191131591797, 'test_avg_loss': 0.6647977828979492, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:38:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:38:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:38:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.198639, avg_loss=0.675993, seen=200, correct=117, accuracy=0.585000
2025-09-14 02:38:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2074MB
2025-09-14 02:38:46 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 2 with val results: {'val_total': 200, 'val_loss': 135.19863891601562, 'val_avg_loss': 0.6759931945800781, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 02:38:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:38:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:38:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.491247, avg_loss=0.687281, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:38:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2074MB
2025-09-14 02:38:49 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.491247177124023, 'test_avg_loss': 0.6872811794281006, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:38:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:38:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:38:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.680008, avg_loss=0.713400, seen=200, correct=113, accuracy=0.565000
2025-09-14 02:38:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:38:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2074MB
2025-09-14 02:38:57 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 3 with val results: {'val_total': 200, 'val_loss': 142.6800079345703, 'val_avg_loss': 0.7134000396728516, 'val_seen': 200, 'val_correct': 113, 'val_acc': 0.565}
2025-09-14 02:38:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:38:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:38:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:38:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.657297, avg_loss=0.691432, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:38:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:38:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:38:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2074MB
2025-09-14 02:39:00 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 3 with test results: {'test_total': 40, 'test_loss': 27.657297134399414, 'test_avg_loss': 0.6914324283599853, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:39:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:39:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:39:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.029282, avg_loss=0.675146, seen=200, correct=112, accuracy=0.560000
2025-09-14 02:39:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2074MB
2025-09-14 02:39:07 (federatedscope.llm.llm_local.client:480) INFO: Client 40 Adapter 4 with val results: {'val_total': 200, 'val_loss': 135.02928161621094, 'val_avg_loss': 0.6751464080810546, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-14 02:39:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:39:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:39:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.092348, avg_loss=0.677309, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:39:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2074MB
2025-09-14 02:39:10 (federatedscope.llm.llm_local.client:501) INFO: Client 40 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.092348098754883, 'test_avg_loss': 0.6773087024688721, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:39:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:39:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 02:39:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 02:39:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.836250, avg_loss=0.687700, seen=119, correct=77, accuracy=0.647059
2025-09-14 02:39:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2125MB
2025-09-14 02:39:16 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 0 with val results: {'val_total': 119, 'val_loss': 81.83625030517578, 'val_avg_loss': 0.6876995823964351, 'val_seen': 119, 'val_correct': 77, 'val_acc': 0.6470588235294118}
2025-09-14 02:39:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:39:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.494659, avg_loss=0.712366, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:39:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2125MB
2025-09-14 02:39:18 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.494659423828125, 'test_avg_loss': 0.7123664855957031, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:39:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 02:39:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 02:39:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.983650, avg_loss=0.697342, seen=119, correct=65, accuracy=0.546218
2025-09-14 02:39:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2083MB
2025-09-14 02:39:24 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 1 with val results: {'val_total': 119, 'val_loss': 82.98365020751953, 'val_avg_loss': 0.6973415983825171, 'val_seen': 119, 'val_correct': 65, 'val_acc': 0.5462184873949579}
2025-09-14 02:39:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:39:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.420010, avg_loss=0.685500, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:39:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2083MB
2025-09-14 02:39:26 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.42000961303711, 'test_avg_loss': 0.6855002403259277, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:39:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 02:39:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 02:39:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.488968, avg_loss=0.693185, seen=119, correct=68, accuracy=0.571429
2025-09-14 02:39:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2083MB
2025-09-14 02:39:32 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 2 with val results: {'val_total': 119, 'val_loss': 82.48896789550781, 'val_avg_loss': 0.6931846041639312, 'val_seen': 119, 'val_correct': 68, 'val_acc': 0.5714285714285714}
2025-09-14 02:39:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:39:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.349882, avg_loss=0.708747, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:39:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2083MB
2025-09-14 02:39:35 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.349882125854492, 'test_avg_loss': 0.7087470531463623, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:39:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 02:39:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 02:39:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=88.729790, avg_loss=0.745628, seen=119, correct=67, accuracy=0.563025
2025-09-14 02:39:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2083MB
2025-09-14 02:39:39 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 3 with val results: {'val_total': 119, 'val_loss': 88.72978973388672, 'val_avg_loss': 0.7456284851587119, 'val_seen': 119, 'val_correct': 67, 'val_acc': 0.5630252100840336}
2025-09-14 02:39:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:39:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:39:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.971870, avg_loss=0.774297, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:39:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2083MB
2025-09-14 02:39:42 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 3 with test results: {'test_total': 40, 'test_loss': 30.97187042236328, 'test_avg_loss': 0.774296760559082, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:39:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 02:39:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 02:39:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=90.619896, avg_loss=0.761512, seen=119, correct=55, accuracy=0.462185
2025-09-14 02:39:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2083MB
2025-09-14 02:39:47 (federatedscope.llm.llm_local.client:480) INFO: Client 41 Adapter 4 with val results: {'val_total': 119, 'val_loss': 90.6198959350586, 'val_avg_loss': 0.7615117305467108, 'val_seen': 119, 'val_correct': 55, 'val_acc': 0.46218487394957986}
2025-09-14 02:39:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:39:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:39:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.293638, avg_loss=0.682341, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:39:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2083MB
2025-09-14 02:39:50 (federatedscope.llm.llm_local.client:501) INFO: Client 41 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.293638229370117, 'test_avg_loss': 0.6823409557342529, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:39:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:39:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:39:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:39:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:39:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.445953, avg_loss=0.677230, seen=200, correct=118, accuracy=0.590000
2025-09-14 02:39:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:39:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:39:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2133MB
2025-09-14 02:39:58 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 0 with val results: {'val_total': 200, 'val_loss': 135.44595336914062, 'val_avg_loss': 0.6772297668457031, 'val_seen': 200, 'val_correct': 118, 'val_acc': 0.59}
2025-09-14 02:39:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:39:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:39:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:40:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.996674, avg_loss=0.674917, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:40:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2133MB
2025-09-14 02:40:01 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.996673583984375, 'test_avg_loss': 0.6749168395996094, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:40:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:40:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:40:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.220886, avg_loss=0.676104, seen=200, correct=117, accuracy=0.585000
2025-09-14 02:40:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2091MB
2025-09-14 02:40:09 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 1 with val results: {'val_total': 200, 'val_loss': 135.22088623046875, 'val_avg_loss': 0.6761044311523438, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 02:40:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:40:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:40:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.023922, avg_loss=0.650598, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:40:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2091MB
2025-09-14 02:40:11 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.023921966552734, 'test_avg_loss': 0.6505980491638184, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:40:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:40:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:40:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.579605, avg_loss=0.687898, seen=200, correct=117, accuracy=0.585000
2025-09-14 02:40:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2091MB
2025-09-14 02:40:19 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 2 with val results: {'val_total': 200, 'val_loss': 137.57960510253906, 'val_avg_loss': 0.6878980255126953, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 02:40:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:40:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.177776, avg_loss=0.679444, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:40:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2091MB
2025-09-14 02:40:22 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.177776336669922, 'test_avg_loss': 0.679444408416748, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:40:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:40:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:40:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.694733, avg_loss=0.718474, seen=200, correct=112, accuracy=0.560000
2025-09-14 02:40:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2091MB
2025-09-14 02:40:29 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 3 with val results: {'val_total': 200, 'val_loss': 143.69473266601562, 'val_avg_loss': 0.7184736633300781, 'val_seen': 200, 'val_correct': 112, 'val_acc': 0.56}
2025-09-14 02:40:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:40:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:40:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.253351, avg_loss=0.706334, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:40:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2091MB
2025-09-14 02:40:32 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 3 with test results: {'test_total': 40, 'test_loss': 28.25335121154785, 'test_avg_loss': 0.7063337802886963, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:40:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:40:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:40:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.756210, avg_loss=0.708781, seen=200, correct=108, accuracy=0.540000
2025-09-14 02:40:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2091MB
2025-09-14 02:40:40 (federatedscope.llm.llm_local.client:480) INFO: Client 42 Adapter 4 with val results: {'val_total': 200, 'val_loss': 141.75621032714844, 'val_avg_loss': 0.7087810516357422, 'val_seen': 200, 'val_correct': 108, 'val_acc': 0.54}
2025-09-14 02:40:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:40:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.945061, avg_loss=0.748627, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:40:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2091MB
2025-09-14 02:40:42 (federatedscope.llm.llm_local.client:501) INFO: Client 42 Adapter 4 with test results: {'test_total': 40, 'test_loss': 29.94506072998047, 'test_avg_loss': 0.7486265182495118, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:40:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:40:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 02:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 02:40:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.839409, avg_loss=0.694825, seen=89, correct=49, accuracy=0.550562
2025-09-14 02:40:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2142MB
2025-09-14 02:40:47 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 0 with val results: {'val_total': 89, 'val_loss': 61.83940887451172, 'val_avg_loss': 0.6948248188147385, 'val_seen': 89, 'val_correct': 49, 'val_acc': 0.550561797752809}
2025-09-14 02:40:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:40:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.161575, avg_loss=0.654039, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:40:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2142MB
2025-09-14 02:40:50 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.161575317382812, 'test_avg_loss': 0.6540393829345703, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:40:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 02:40:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 02:40:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.293091, avg_loss=0.677450, seen=89, correct=49, accuracy=0.550562
2025-09-14 02:40:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2100MB
2025-09-14 02:40:54 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 1 with val results: {'val_total': 89, 'val_loss': 60.2930908203125, 'val_avg_loss': 0.6774504586551966, 'val_seen': 89, 'val_correct': 49, 'val_acc': 0.550561797752809}
2025-09-14 02:40:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:40:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.213354, avg_loss=0.630334, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:40:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2100MB
2025-09-14 02:40:56 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.213354110717773, 'test_avg_loss': 0.6303338527679443, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:40:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 02:40:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:40:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 02:40:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.576797, avg_loss=0.680638, seen=89, correct=49, accuracy=0.550562
2025-09-14 02:40:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:40:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:40:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:40:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2100MB
2025-09-14 02:40:59 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 2 with val results: {'val_total': 89, 'val_loss': 60.57679748535156, 'val_avg_loss': 0.6806381739927142, 'val_seen': 89, 'val_correct': 49, 'val_acc': 0.550561797752809}
2025-09-14 02:41:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:41:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:41:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.349529, avg_loss=0.683738, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:41:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2100MB
2025-09-14 02:41:01 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.349529266357422, 'test_avg_loss': 0.6837382316589355, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:41:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 02:41:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 02:41:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=62.154373, avg_loss=0.698364, seen=89, correct=51, accuracy=0.573034
2025-09-14 02:41:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2100MB
2025-09-14 02:41:05 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 3 with val results: {'val_total': 89, 'val_loss': 62.15437316894531, 'val_avg_loss': 0.6983637434712956, 'val_seen': 89, 'val_correct': 51, 'val_acc': 0.5730337078651685}
2025-09-14 02:41:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:41:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:41:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.112778, avg_loss=0.702819, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:41:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2100MB
2025-09-14 02:41:07 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 3 with test results: {'test_total': 40, 'test_loss': 28.112777709960938, 'test_avg_loss': 0.7028194427490234, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:41:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 02:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 02:41:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=65.850372, avg_loss=0.739892, seen=89, correct=47, accuracy=0.528090
2025-09-14 02:41:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2100MB
2025-09-14 02:41:12 (federatedscope.llm.llm_local.client:480) INFO: Client 43 Adapter 4 with val results: {'val_total': 89, 'val_loss': 65.85037231445312, 'val_avg_loss': 0.7398918237579003, 'val_seen': 89, 'val_correct': 47, 'val_acc': 0.5280898876404494}
2025-09-14 02:41:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:41:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:41:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.368973, avg_loss=0.684224, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:41:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2100MB
2025-09-14 02:41:14 (federatedscope.llm.llm_local.client:501) INFO: Client 43 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.368972778320312, 'test_avg_loss': 0.6842243194580078, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:41:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:41:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:41:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:41:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.999680, avg_loss=0.704998, seen=200, correct=104, accuracy=0.520000
2025-09-14 02:41:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2150MB
2025-09-14 02:41:22 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 0 with val results: {'val_total': 200, 'val_loss': 140.9996795654297, 'val_avg_loss': 0.7049983978271485, 'val_seen': 200, 'val_correct': 104, 'val_acc': 0.52}
2025-09-14 02:41:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:41:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:41:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.499485, avg_loss=0.587487, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:41:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2150MB
2025-09-14 02:41:25 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.49948501586914, 'test_avg_loss': 0.5874871253967285, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:41:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:41:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.994476, avg_loss=0.704972, seen=200, correct=100, accuracy=0.500000
2025-09-14 02:41:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2108MB
2025-09-14 02:41:32 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 1 with val results: {'val_total': 200, 'val_loss': 140.99447631835938, 'val_avg_loss': 0.7049723815917969, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-14 02:41:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:41:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:41:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.841896, avg_loss=0.671047, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:41:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2108MB
2025-09-14 02:41:35 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.841896057128906, 'test_avg_loss': 0.6710474014282226, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:41:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:41:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:41:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.349792, avg_loss=0.706749, seen=200, correct=107, accuracy=0.535000
2025-09-14 02:41:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2108MB
2025-09-14 02:41:43 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 2 with val results: {'val_total': 200, 'val_loss': 141.34979248046875, 'val_avg_loss': 0.7067489624023438, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-14 02:41:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:41:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.264561, avg_loss=0.656614, seen=40, correct=27, accuracy=0.675000
2025-09-14 02:41:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2108MB
2025-09-14 02:41:46 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.26456069946289, 'test_avg_loss': 0.6566140174865722, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 02:41:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:41:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=149.027786, avg_loss=0.745139, seen=200, correct=102, accuracy=0.510000
2025-09-14 02:41:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2108MB
2025-09-14 02:41:54 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 3 with val results: {'val_total': 200, 'val_loss': 149.0277862548828, 'val_avg_loss': 0.7451389312744141, 'val_seen': 200, 'val_correct': 102, 'val_acc': 0.51}
2025-09-14 02:41:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:41:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:41:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:41:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.763330, avg_loss=0.644083, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:41:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:41:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:41:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2108MB
2025-09-14 02:41:56 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 3 with test results: {'test_total': 40, 'test_loss': 25.763330459594727, 'test_avg_loss': 0.6440832614898682, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:41:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:41:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:41:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:42:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=151.433289, avg_loss=0.757166, seen=200, correct=94, accuracy=0.470000
2025-09-14 02:42:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2108MB
2025-09-14 02:42:03 (federatedscope.llm.llm_local.client:480) INFO: Client 44 Adapter 4 with val results: {'val_total': 200, 'val_loss': 151.43328857421875, 'val_avg_loss': 0.7571664428710938, 'val_seen': 200, 'val_correct': 94, 'val_acc': 0.47}
2025-09-14 02:42:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:42:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.770229, avg_loss=0.794256, seen=40, correct=15, accuracy=0.375000
2025-09-14 02:42:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2108MB
2025-09-14 02:42:06 (federatedscope.llm.llm_local.client:501) INFO: Client 44 Adapter 4 with test results: {'test_total': 40, 'test_loss': 31.77022933959961, 'test_avg_loss': 0.7942557334899902, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-14 02:42:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:42:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:42:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:42:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.472004, avg_loss=0.634720, seen=100, correct=62, accuracy=0.620000
2025-09-14 02:42:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2158MB
2025-09-14 02:42:10 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 0 with val results: {'val_total': 100, 'val_loss': 63.47200393676758, 'val_avg_loss': 0.6347200393676757, 'val_seen': 100, 'val_correct': 62, 'val_acc': 0.62}
2025-09-14 02:42:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:42:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.892923, avg_loss=0.747323, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:42:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2158MB
2025-09-14 02:42:12 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 0 with test results: {'test_total': 40, 'test_loss': 29.89292335510254, 'test_avg_loss': 0.7473230838775635, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:42:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:42:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=67.868713, avg_loss=0.678687, seen=100, correct=57, accuracy=0.570000
2025-09-14 02:42:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2116MB
2025-09-14 02:42:17 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 1 with val results: {'val_total': 100, 'val_loss': 67.86871337890625, 'val_avg_loss': 0.6786871337890625, 'val_seen': 100, 'val_correct': 57, 'val_acc': 0.57}
2025-09-14 02:42:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:42:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:42:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.457455, avg_loss=0.711436, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:42:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2116MB
2025-09-14 02:42:20 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 1 with test results: {'test_total': 40, 'test_loss': 28.457454681396484, 'test_avg_loss': 0.7114363670349121, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:42:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:42:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=66.991310, avg_loss=0.669913, seen=100, correct=60, accuracy=0.600000
2025-09-14 02:42:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2116MB
2025-09-14 02:42:24 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 2 with val results: {'val_total': 100, 'val_loss': 66.9913101196289, 'val_avg_loss': 0.6699131011962891, 'val_seen': 100, 'val_correct': 60, 'val_acc': 0.6}
2025-09-14 02:42:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:42:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:42:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.638592, avg_loss=0.715965, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:42:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2116MB
2025-09-14 02:42:26 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.638591766357422, 'test_avg_loss': 0.7159647941589355, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:42:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:42:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:42:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.800659, avg_loss=0.638007, seen=100, correct=64, accuracy=0.640000
2025-09-14 02:42:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2116MB
2025-09-14 02:42:31 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 3 with val results: {'val_total': 100, 'val_loss': 63.8006591796875, 'val_avg_loss': 0.638006591796875, 'val_seen': 100, 'val_correct': 64, 'val_acc': 0.64}
2025-09-14 02:42:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:42:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:42:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.145493, avg_loss=0.753637, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:42:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2116MB
2025-09-14 02:42:34 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 3 with test results: {'test_total': 40, 'test_loss': 30.145492553710938, 'test_avg_loss': 0.7536373138427734, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:42:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 02:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 02:42:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=79.768265, avg_loss=0.797683, seen=100, correct=37, accuracy=0.370000
2025-09-14 02:42:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2116MB
2025-09-14 02:42:38 (federatedscope.llm.llm_local.client:480) INFO: Client 45 Adapter 4 with val results: {'val_total': 100, 'val_loss': 79.76826477050781, 'val_avg_loss': 0.7976826477050781, 'val_seen': 100, 'val_correct': 37, 'val_acc': 0.37}
2025-09-14 02:42:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:42:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:42:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.362387, avg_loss=0.784060, seen=40, correct=16, accuracy=0.400000
2025-09-14 02:42:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2116MB
2025-09-14 02:42:41 (federatedscope.llm.llm_local.client:501) INFO: Client 45 Adapter 4 with test results: {'test_total': 40, 'test_loss': 31.36238670349121, 'test_avg_loss': 0.7840596675872803, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-14 02:42:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:42:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:42:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:42:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=77.180588, avg_loss=0.701642, seen=110, correct=63, accuracy=0.572727
2025-09-14 02:42:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2167MB
2025-09-14 02:42:46 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 0 with val results: {'val_total': 110, 'val_loss': 77.18058776855469, 'val_avg_loss': 0.7016417069868608, 'val_seen': 110, 'val_correct': 63, 'val_acc': 0.5727272727272728}
2025-09-14 02:42:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:42:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:42:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.891281, avg_loss=0.597282, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:42:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2167MB
2025-09-14 02:42:49 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 0 with test results: {'test_total': 40, 'test_loss': 23.891281127929688, 'test_avg_loss': 0.5972820281982422, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 02:42:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:42:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:42:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.803772, avg_loss=0.680034, seen=110, correct=59, accuracy=0.536364
2025-09-14 02:42:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2125MB
2025-09-14 02:42:53 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 1 with val results: {'val_total': 110, 'val_loss': 74.80377197265625, 'val_avg_loss': 0.6800342906605114, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-14 02:42:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:42:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:42:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.806023, avg_loss=0.645151, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:42:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:42:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2125MB
2025-09-14 02:42:56 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.80602264404297, 'test_avg_loss': 0.6451505661010742, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:42:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:42:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:42:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:42:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.936157, avg_loss=0.699420, seen=110, correct=61, accuracy=0.554545
2025-09-14 02:42:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:42:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:42:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2125MB
2025-09-14 02:43:01 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 2 with val results: {'val_total': 110, 'val_loss': 76.9361572265625, 'val_avg_loss': 0.6994196111505682, 'val_seen': 110, 'val_correct': 61, 'val_acc': 0.5545454545454546}
2025-09-14 02:43:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:43:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:43:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.893188, avg_loss=0.647330, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:43:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2125MB
2025-09-14 02:43:04 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 2 with test results: {'test_total': 40, 'test_loss': 25.8931884765625, 'test_avg_loss': 0.6473297119140625, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:43:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:43:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:43:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=77.918167, avg_loss=0.708347, seen=110, correct=58, accuracy=0.527273
2025-09-14 02:43:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2125MB
2025-09-14 02:43:09 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 3 with val results: {'val_total': 110, 'val_loss': 77.91816711425781, 'val_avg_loss': 0.7083469737659801, 'val_seen': 110, 'val_correct': 58, 'val_acc': 0.5272727272727272}
2025-09-14 02:43:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:43:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:43:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.784702, avg_loss=0.644618, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:43:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2125MB
2025-09-14 02:43:12 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 3 with test results: {'test_total': 40, 'test_loss': 25.78470230102539, 'test_avg_loss': 0.6446175575256348, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:43:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 02:43:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 02:43:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=77.472946, avg_loss=0.704300, seen=110, correct=64, accuracy=0.581818
2025-09-14 02:43:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2125MB
2025-09-14 02:43:18 (federatedscope.llm.llm_local.client:480) INFO: Client 46 Adapter 4 with val results: {'val_total': 110, 'val_loss': 77.47294616699219, 'val_avg_loss': 0.7042995106090199, 'val_seen': 110, 'val_correct': 64, 'val_acc': 0.5818181818181818}
2025-09-14 02:43:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:43:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:43:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.461483, avg_loss=0.761537, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:43:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2125MB
2025-09-14 02:43:20 (federatedscope.llm.llm_local.client:501) INFO: Client 46 Adapter 4 with test results: {'test_total': 40, 'test_loss': 30.461483001708984, 'test_avg_loss': 0.7615370750427246, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:43:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:43:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 02:43:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:43:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=107.919510, avg_loss=0.734146, seen=147, correct=75, accuracy=0.510204
2025-09-14 02:43:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2175MB
2025-09-14 02:43:27 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 0 with val results: {'val_total': 147, 'val_loss': 107.91950988769531, 'val_avg_loss': 0.7341463257666347, 'val_seen': 147, 'val_correct': 75, 'val_acc': 0.5102040816326531}
2025-09-14 02:43:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:43:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:43:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.406921, avg_loss=0.660173, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:43:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2175MB
2025-09-14 02:43:30 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.40692138671875, 'test_avg_loss': 0.6601730346679687, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:43:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 02:43:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:43:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.877045, avg_loss=0.693041, seen=147, correct=76, accuracy=0.517007
2025-09-14 02:43:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2133MB
2025-09-14 02:43:36 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 1 with val results: {'val_total': 147, 'val_loss': 101.87704467773438, 'val_avg_loss': 0.6930411202566964, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-14 02:43:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:43:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:43:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.948286, avg_loss=0.648707, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:43:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2133MB
2025-09-14 02:43:39 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.948286056518555, 'test_avg_loss': 0.6487071514129639, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:43:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 02:43:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:43:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=107.586143, avg_loss=0.731879, seen=147, correct=69, accuracy=0.469388
2025-09-14 02:43:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2133MB
2025-09-14 02:43:45 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 2 with val results: {'val_total': 147, 'val_loss': 107.58614349365234, 'val_avg_loss': 0.731878527167703, 'val_seen': 147, 'val_correct': 69, 'val_acc': 0.46938775510204084}
2025-09-14 02:43:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:43:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:43:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.119680, avg_loss=0.652992, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:43:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2133MB
2025-09-14 02:43:48 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.119680404663086, 'test_avg_loss': 0.6529920101165771, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:43:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 02:43:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:43:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=110.796478, avg_loss=0.753718, seen=147, correct=68, accuracy=0.462585
2025-09-14 02:43:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2133MB
2025-09-14 02:43:54 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 3 with val results: {'val_total': 147, 'val_loss': 110.79647827148438, 'val_avg_loss': 0.7537175392617985, 'val_seen': 147, 'val_correct': 68, 'val_acc': 0.46258503401360546}
2025-09-14 02:43:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:43:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:43:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:43:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.205540, avg_loss=0.680138, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:43:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:43:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:43:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2133MB
2025-09-14 02:43:57 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 3 with test results: {'test_total': 40, 'test_loss': 27.20553970336914, 'test_avg_loss': 0.6801384925842285, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:43:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 02:43:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:43:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 02:44:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=105.843658, avg_loss=0.720025, seen=147, correct=76, accuracy=0.517007
2025-09-14 02:44:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2133MB
2025-09-14 02:44:03 (federatedscope.llm.llm_local.client:480) INFO: Client 47 Adapter 4 with val results: {'val_total': 147, 'val_loss': 105.84365844726562, 'val_avg_loss': 0.7200248873963648, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-14 02:44:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:44:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.548668, avg_loss=0.663717, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:44:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2133MB
2025-09-14 02:44:06 (federatedscope.llm.llm_local.client:501) INFO: Client 47 Adapter 4 with test results: {'test_total': 40, 'test_loss': 26.548667907714844, 'test_avg_loss': 0.6637166976928711, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:44:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:44:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 02:44:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 02:44:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=34.910881, avg_loss=0.758932, seen=46, correct=24, accuracy=0.521739
2025-09-14 02:44:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2184MB
2025-09-14 02:44:10 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 0 with val results: {'val_total': 46, 'val_loss': 34.91088104248047, 'val_avg_loss': 0.7589321965756624, 'val_seen': 46, 'val_correct': 24, 'val_acc': 0.5217391304347826}
2025-09-14 02:44:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:44:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.331867, avg_loss=0.783297, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:44:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2184MB
2025-09-14 02:44:12 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 0 with test results: {'test_total': 40, 'test_loss': 31.331867218017578, 'test_avg_loss': 0.7832966804504394, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:44:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 02:44:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 02:44:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.787172, avg_loss=0.691025, seen=46, correct=24, accuracy=0.521739
2025-09-14 02:44:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2142MB
2025-09-14 02:44:15 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 1 with val results: {'val_total': 46, 'val_loss': 31.787172317504883, 'val_avg_loss': 0.6910254851631497, 'val_seen': 46, 'val_correct': 24, 'val_acc': 0.5217391304347826}
2025-09-14 02:44:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:44:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.414082, avg_loss=0.685352, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:44:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2142MB
2025-09-14 02:44:18 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.414081573486328, 'test_avg_loss': 0.6853520393371582, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:44:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 02:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 02:44:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.206394, avg_loss=0.678400, seen=46, correct=25, accuracy=0.543478
2025-09-14 02:44:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2142MB
2025-09-14 02:44:21 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 2 with val results: {'val_total': 46, 'val_loss': 31.20639419555664, 'val_avg_loss': 0.6783998738164487, 'val_seen': 46, 'val_correct': 25, 'val_acc': 0.5434782608695652}
2025-09-14 02:44:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:44:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.752172, avg_loss=0.718804, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:44:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2142MB
2025-09-14 02:44:24 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.752172470092773, 'test_avg_loss': 0.7188043117523193, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:44:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 02:44:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 02:44:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.991196, avg_loss=0.717200, seen=46, correct=27, accuracy=0.586957
2025-09-14 02:44:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2142MB
2025-09-14 02:44:27 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 3 with val results: {'val_total': 46, 'val_loss': 32.99119567871094, 'val_avg_loss': 0.7171999060589335, 'val_seen': 46, 'val_correct': 27, 'val_acc': 0.5869565217391305}
2025-09-14 02:44:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:44:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.202753, avg_loss=0.780069, seen=40, correct=16, accuracy=0.400000
2025-09-14 02:44:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2142MB
2025-09-14 02:44:29 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 3 with test results: {'test_total': 40, 'test_loss': 31.2027530670166, 'test_avg_loss': 0.780068826675415, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-14 02:44:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 02:44:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 02:44:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=33.972580, avg_loss=0.738534, seen=46, correct=25, accuracy=0.543478
2025-09-14 02:44:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2142MB
2025-09-14 02:44:32 (federatedscope.llm.llm_local.client:480) INFO: Client 48 Adapter 4 with val results: {'val_total': 46, 'val_loss': 33.97257995605469, 'val_avg_loss': 0.738534346870754, 'val_seen': 46, 'val_correct': 25, 'val_acc': 0.5434782608695652}
2025-09-14 02:44:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:44:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.864702, avg_loss=0.646618, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:44:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2142MB
2025-09-14 02:44:34 (federatedscope.llm.llm_local.client:501) INFO: Client 48 Adapter 4 with test results: {'test_total': 40, 'test_loss': 25.864702224731445, 'test_avg_loss': 0.6466175556182862, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:44:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:44:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 02:44:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 02:44:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=87.507080, avg_loss=0.662932, seen=132, correct=85, accuracy=0.643939
2025-09-14 02:44:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2192MB
2025-09-14 02:44:41 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 0 with val results: {'val_total': 132, 'val_loss': 87.507080078125, 'val_avg_loss': 0.6629324248342803, 'val_seen': 132, 'val_correct': 85, 'val_acc': 0.6439393939393939}
2025-09-14 02:44:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:44:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.867348, avg_loss=0.821684, seen=40, correct=17, accuracy=0.425000
2025-09-14 02:44:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2192MB
2025-09-14 02:44:43 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 0 with test results: {'test_total': 40, 'test_loss': 32.867347717285156, 'test_avg_loss': 0.821683692932129, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 02:44:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 02:44:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 02:44:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=87.968674, avg_loss=0.666429, seen=132, correct=81, accuracy=0.613636
2025-09-14 02:44:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2150MB
2025-09-14 02:44:50 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 1 with val results: {'val_total': 132, 'val_loss': 87.96867370605469, 'val_avg_loss': 0.6664293462579901, 'val_seen': 132, 'val_correct': 81, 'val_acc': 0.6136363636363636}
2025-09-14 02:44:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:44:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.386860, avg_loss=0.734671, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:44:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2150MB
2025-09-14 02:44:52 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 1 with test results: {'test_total': 40, 'test_loss': 29.386859893798828, 'test_avg_loss': 0.7346714973449707, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 02:44:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 02:44:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:44:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 02:44:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.199753, avg_loss=0.668180, seen=132, correct=82, accuracy=0.621212
2025-09-14 02:44:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:44:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:44:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2150MB
2025-09-14 02:44:59 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 2 with val results: {'val_total': 132, 'val_loss': 88.19975280761719, 'val_avg_loss': 0.6681799455122515, 'val_seen': 132, 'val_correct': 82, 'val_acc': 0.6212121212121212}
2025-09-14 02:44:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:44:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:45:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.110920, avg_loss=0.727773, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:45:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2150MB
2025-09-14 02:45:01 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 2 with test results: {'test_total': 40, 'test_loss': 29.110919952392578, 'test_avg_loss': 0.7277729988098145, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:45:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 02:45:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 02:45:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=90.020325, avg_loss=0.681972, seen=132, correct=74, accuracy=0.560606
2025-09-14 02:45:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2150MB
2025-09-14 02:45:07 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 3 with val results: {'val_total': 132, 'val_loss': 90.02032470703125, 'val_avg_loss': 0.6819721568714489, 'val_seen': 132, 'val_correct': 74, 'val_acc': 0.5606060606060606}
2025-09-14 02:45:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:45:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:45:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.371326, avg_loss=0.784283, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:45:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2150MB
2025-09-14 02:45:10 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 3 with test results: {'test_total': 40, 'test_loss': 31.371326446533203, 'test_avg_loss': 0.7842831611633301, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:45:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 02:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 02:45:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=89.783966, avg_loss=0.680182, seen=132, correct=73, accuracy=0.553030
2025-09-14 02:45:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2150MB
2025-09-14 02:45:16 (federatedscope.llm.llm_local.client:480) INFO: Client 49 Adapter 4 with val results: {'val_total': 132, 'val_loss': 89.78396606445312, 'val_avg_loss': 0.6801815610943418, 'val_seen': 132, 'val_correct': 73, 'val_acc': 0.553030303030303}
2025-09-14 02:45:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:45:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:45:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.503939, avg_loss=0.787598, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:45:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2150MB
2025-09-14 02:45:18 (federatedscope.llm.llm_local.client:501) INFO: Client 49 Adapter 4 with test results: {'test_total': 40, 'test_loss': 31.503938674926758, 'test_avg_loss': 0.7875984668731689, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:45:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:45:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 02:45:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:45:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=93.920609, avg_loss=0.706170, seen=133, correct=73, accuracy=0.548872
2025-09-14 02:45:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2200MB
2025-09-14 02:45:25 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 0 with val results: {'val_total': 133, 'val_loss': 93.92060852050781, 'val_avg_loss': 0.7061699888759986, 'val_seen': 133, 'val_correct': 73, 'val_acc': 0.5488721804511278}
2025-09-14 02:45:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:45:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:45:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.766052, avg_loss=0.819151, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:45:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2200MB
2025-09-14 02:45:27 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 0 with test results: {'test_total': 40, 'test_loss': 32.76605224609375, 'test_avg_loss': 0.8191513061523438, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 02:45:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 02:45:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:45:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.607086, avg_loss=0.688775, seen=133, correct=78, accuracy=0.586466
2025-09-14 02:45:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2158MB
2025-09-14 02:45:34 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 1 with val results: {'val_total': 133, 'val_loss': 91.60708618164062, 'val_avg_loss': 0.6887750840724859, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}
2025-09-14 02:45:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:45:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:45:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.860191, avg_loss=0.696505, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:45:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2158MB
2025-09-14 02:45:36 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.860191345214844, 'test_avg_loss': 0.696504783630371, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:45:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 02:45:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:45:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.148270, avg_loss=0.692844, seen=133, correct=69, accuracy=0.518797
2025-09-14 02:45:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2158MB
2025-09-14 02:45:42 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 2 with val results: {'val_total': 133, 'val_loss': 92.14826965332031, 'val_avg_loss': 0.6928441327317316, 'val_seen': 133, 'val_correct': 69, 'val_acc': 0.518796992481203}
2025-09-14 02:45:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:45:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:45:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.539509, avg_loss=0.713488, seen=40, correct=16, accuracy=0.400000
2025-09-14 02:45:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2158MB
2025-09-14 02:45:44 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.539508819580078, 'test_avg_loss': 0.7134877204895019, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-14 02:45:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 02:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:45:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=95.898132, avg_loss=0.721039, seen=133, correct=76, accuracy=0.571429
2025-09-14 02:45:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2158MB
2025-09-14 02:45:49 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 3 with val results: {'val_total': 133, 'val_loss': 95.89813232421875, 'val_avg_loss': 0.7210385889039004, 'val_seen': 133, 'val_correct': 76, 'val_acc': 0.5714285714285714}
2025-09-14 02:45:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:45:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.944824, avg_loss=0.773621, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:45:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2158MB
2025-09-14 02:45:52 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 3 with test results: {'test_total': 40, 'test_loss': 30.94482421875, 'test_avg_loss': 0.77362060546875, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:45:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 02:45:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:45:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 02:45:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=94.862038, avg_loss=0.713248, seen=133, correct=69, accuracy=0.518797
2025-09-14 02:45:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:45:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:45:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2158MB
2025-09-14 02:45:58 (federatedscope.llm.llm_local.client:480) INFO: Client 50 Adapter 4 with val results: {'val_total': 133, 'val_loss': 94.8620376586914, 'val_avg_loss': 0.7132484034488076, 'val_seen': 133, 'val_correct': 69, 'val_acc': 0.518796992481203}
2025-09-14 02:45:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:45:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:45:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:46:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.027206, avg_loss=0.725680, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:46:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2158MB
2025-09-14 02:46:01 (federatedscope.llm.llm_local.client:501) INFO: Client 50 Adapter 4 with test results: {'test_total': 40, 'test_loss': 29.027206420898438, 'test_avg_loss': 0.725680160522461, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:46:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:46:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:46:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:46:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.420799, avg_loss=0.703865, seen=83, correct=48, accuracy=0.578313
2025-09-14 02:46:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2209MB
2025-09-14 02:46:06 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 0 with val results: {'val_total': 83, 'val_loss': 58.420799255371094, 'val_avg_loss': 0.7038650512695312, 'val_seen': 83, 'val_correct': 48, 'val_acc': 0.5783132530120482}
2025-09-14 02:46:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:46:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:46:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.976233, avg_loss=0.724406, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:46:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2209MB
2025-09-14 02:46:08 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 0 with test results: {'test_total': 40, 'test_loss': 28.976232528686523, 'test_avg_loss': 0.7244058132171631, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:46:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:46:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:46:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.220360, avg_loss=0.677354, seen=83, correct=52, accuracy=0.626506
2025-09-14 02:46:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2167MB
2025-09-14 02:46:12 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 1 with val results: {'val_total': 83, 'val_loss': 56.220359802246094, 'val_avg_loss': 0.6773537325571819, 'val_seen': 83, 'val_correct': 52, 'val_acc': 0.6265060240963856}
2025-09-14 02:46:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:46:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:46:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.475689, avg_loss=0.661892, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:46:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2167MB
2025-09-14 02:46:14 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 1 with test results: {'test_total': 40, 'test_loss': 26.475688934326172, 'test_avg_loss': 0.6618922233581543, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:46:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:46:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:46:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.950054, avg_loss=0.686145, seen=83, correct=50, accuracy=0.602410
2025-09-14 02:46:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2167MB
2025-09-14 02:46:17 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 2 with val results: {'val_total': 83, 'val_loss': 56.95005416870117, 'val_avg_loss': 0.6861452309482069, 'val_seen': 83, 'val_correct': 50, 'val_acc': 0.6024096385542169}
2025-09-14 02:46:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:46:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:46:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.454382, avg_loss=0.711360, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:46:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2167MB
2025-09-14 02:46:19 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 2 with test results: {'test_total': 40, 'test_loss': 28.454381942749023, 'test_avg_loss': 0.7113595485687256, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 02:46:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:46:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:46:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.199928, avg_loss=0.701204, seen=83, correct=50, accuracy=0.602410
2025-09-14 02:46:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2167MB
2025-09-14 02:46:23 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 3 with val results: {'val_total': 83, 'val_loss': 58.199928283691406, 'val_avg_loss': 0.7012039552251976, 'val_seen': 83, 'val_correct': 50, 'val_acc': 0.6024096385542169}
2025-09-14 02:46:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:46:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:46:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.736034, avg_loss=0.768401, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:46:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2167MB
2025-09-14 02:46:25 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 3 with test results: {'test_total': 40, 'test_loss': 30.736034393310547, 'test_avg_loss': 0.7684008598327636, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:46:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:46:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:46:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=62.956944, avg_loss=0.758517, seen=83, correct=39, accuracy=0.469880
2025-09-14 02:46:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2167MB
2025-09-14 02:46:29 (federatedscope.llm.llm_local.client:480) INFO: Client 51 Adapter 4 with val results: {'val_total': 83, 'val_loss': 62.95694351196289, 'val_avg_loss': 0.7585173917103962, 'val_seen': 83, 'val_correct': 39, 'val_acc': 0.46987951807228917}
2025-09-14 02:46:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:46:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.448389, avg_loss=0.686210, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:46:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2167MB
2025-09-14 02:46:32 (federatedscope.llm.llm_local.client:501) INFO: Client 51 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.448389053344727, 'test_avg_loss': 0.6862097263336182, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:46:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:46:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:46:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:46:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.681992, avg_loss=0.689798, seen=188, correct=106, accuracy=0.563830
2025-09-14 02:46:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2217MB
2025-09-14 02:46:40 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 0 with val results: {'val_total': 188, 'val_loss': 129.68199157714844, 'val_avg_loss': 0.6897978275380237, 'val_seen': 188, 'val_correct': 106, 'val_acc': 0.5638297872340425}
2025-09-14 02:46:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:46:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.100719, avg_loss=0.652518, seen=40, correct=26, accuracy=0.650000
2025-09-14 02:46:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2217MB
2025-09-14 02:46:43 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.100719451904297, 'test_avg_loss': 0.6525179862976074, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 02:46:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:46:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:46:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.244232, avg_loss=0.687469, seen=188, correct=103, accuracy=0.547872
2025-09-14 02:46:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2175MB
2025-09-14 02:46:50 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 1 with val results: {'val_total': 188, 'val_loss': 129.24423217773438, 'val_avg_loss': 0.6874693200943318, 'val_seen': 188, 'val_correct': 103, 'val_acc': 0.5478723404255319}
2025-09-14 02:46:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:46:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:46:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.380028, avg_loss=0.634501, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:46:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:46:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2175MB
2025-09-14 02:46:53 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 1 with test results: {'test_total': 40, 'test_loss': 25.380027770996094, 'test_avg_loss': 0.6345006942749023, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:46:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:46:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:46:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:46:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:46:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=131.360184, avg_loss=0.698724, seen=188, correct=101, accuracy=0.537234
2025-09-14 02:46:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:46:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2175MB
2025-09-14 02:47:01 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 2 with val results: {'val_total': 188, 'val_loss': 131.3601837158203, 'val_avg_loss': 0.6987243814671293, 'val_seen': 188, 'val_correct': 101, 'val_acc': 0.5372340425531915}
2025-09-14 02:47:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:47:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:47:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.885826, avg_loss=0.672146, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:47:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2175MB
2025-09-14 02:47:03 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 2 with test results: {'test_total': 40, 'test_loss': 26.885826110839844, 'test_avg_loss': 0.6721456527709961, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:47:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:47:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:47:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=132.095825, avg_loss=0.702637, seen=188, correct=106, accuracy=0.563830
2025-09-14 02:47:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2175MB
2025-09-14 02:47:11 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 3 with val results: {'val_total': 188, 'val_loss': 132.0958251953125, 'val_avg_loss': 0.7026373680601729, 'val_seen': 188, 'val_correct': 106, 'val_acc': 0.5638297872340425}
2025-09-14 02:47:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:47:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.347912, avg_loss=0.658698, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:47:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2175MB
2025-09-14 02:47:14 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 3 with test results: {'test_total': 40, 'test_loss': 26.347911834716797, 'test_avg_loss': 0.6586977958679199, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 02:47:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 02:47:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 02:47:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=138.600250, avg_loss=0.737235, seen=188, correct=94, accuracy=0.500000
2025-09-14 02:47:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2175MB
2025-09-14 02:47:22 (federatedscope.llm.llm_local.client:480) INFO: Client 52 Adapter 4 with val results: {'val_total': 188, 'val_loss': 138.60025024414062, 'val_avg_loss': 0.7372353736390459, 'val_seen': 188, 'val_correct': 94, 'val_acc': 0.5}
2025-09-14 02:47:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:47:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:47:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.654314, avg_loss=0.691358, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:47:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2175MB
2025-09-14 02:47:25 (federatedscope.llm.llm_local.client:501) INFO: Client 52 Adapter 4 with test results: {'test_total': 40, 'test_loss': 27.654314041137695, 'test_avg_loss': 0.6913578510284424, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:47:25 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:47:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:47:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:47:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.647324, avg_loss=0.678237, seen=200, correct=117, accuracy=0.585000
2025-09-14 02:47:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2226MB
2025-09-14 02:47:32 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 0 with val results: {'val_total': 200, 'val_loss': 135.64732360839844, 'val_avg_loss': 0.6782366180419922, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 02:47:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:47:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:47:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.033195, avg_loss=0.650830, seen=40, correct=29, accuracy=0.725000
2025-09-14 02:47:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2226MB
2025-09-14 02:47:35 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 0 with test results: {'test_total': 40, 'test_loss': 26.03319549560547, 'test_avg_loss': 0.6508298873901367, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 02:47:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:47:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:47:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.962769, avg_loss=0.684814, seen=200, correct=105, accuracy=0.525000
2025-09-14 02:47:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2184MB
2025-09-14 02:47:43 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 1 with val results: {'val_total': 200, 'val_loss': 136.9627685546875, 'val_avg_loss': 0.6848138427734375, 'val_seen': 200, 'val_correct': 105, 'val_acc': 0.525}
2025-09-14 02:47:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:47:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.875118, avg_loss=0.696878, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:47:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2184MB
2025-09-14 02:47:46 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 1 with test results: {'test_total': 40, 'test_loss': 27.875118255615234, 'test_avg_loss': 0.6968779563903809, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 02:47:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:47:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.446701, avg_loss=0.687234, seen=200, correct=107, accuracy=0.535000
2025-09-14 02:47:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2184MB
2025-09-14 02:47:53 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 2 with val results: {'val_total': 200, 'val_loss': 137.4467010498047, 'val_avg_loss': 0.6872335052490235, 'val_seen': 200, 'val_correct': 107, 'val_acc': 0.535}
2025-09-14 02:47:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:47:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:47:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:47:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.563004, avg_loss=0.689075, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:47:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:47:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:47:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2262MB allocated=2184MB
2025-09-14 02:47:56 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 2 with test results: {'test_total': 40, 'test_loss': 27.563003540039062, 'test_avg_loss': 0.6890750885009765, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 02:47:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:47:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:47:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:48:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=146.142975, avg_loss=0.730715, seen=200, correct=106, accuracy=0.530000
2025-09-14 02:48:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:48:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2184MB
2025-09-14 02:48:04 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 3 with val results: {'val_total': 200, 'val_loss': 146.14297485351562, 'val_avg_loss': 0.7307148742675781, 'val_seen': 200, 'val_correct': 106, 'val_acc': 0.53}
2025-09-14 02:48:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:48:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:48:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.772943, avg_loss=0.719324, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:48:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:48:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:48:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2184MB
2025-09-14 02:48:07 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 3 with test results: {'test_total': 40, 'test_loss': 28.7729434967041, 'test_avg_loss': 0.7193235874176025, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 02:48:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:48:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:48:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.904221, avg_loss=0.694521, seen=200, correct=108, accuracy=0.540000
2025-09-14 02:48:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:48:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:48:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2184MB
2025-09-14 02:48:15 (federatedscope.llm.llm_local.client:480) INFO: Client 53 Adapter 4 with val results: {'val_total': 200, 'val_loss': 138.9042205810547, 'val_avg_loss': 0.6945211029052735, 'val_seen': 200, 'val_correct': 108, 'val_acc': 0.54}
2025-09-14 02:48:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:48:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.427122, avg_loss=0.710678, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:48:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:48:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:48:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2184MB
2025-09-14 02:48:18 (federatedscope.llm.llm_local.client:501) INFO: Client 53 Adapter 4 with test results: {'test_total': 40, 'test_loss': 28.427122116088867, 'test_avg_loss': 0.7106780529022216, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 02:48:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:48:19 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 02:48:19 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 02:48:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 02:48:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:48:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:48:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.994476, avg_loss=0.704972, seen=200, correct=100, accuracy=0.500000
2025-09-14 02:48:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:48:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:48:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2192MB
2025-09-14 02:48:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:48:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:48:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.841896, avg_loss=0.671047, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:48:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:48:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:48:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2260MB allocated=2192MB
2025-09-14 02:48:29 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 02:48:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1979, total=7916)
2025-09-14 02:48:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:30 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 02:48:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:30 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=990, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 02:48:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 02:48:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 02:48:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:48:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.535660, avg_loss=0.702678, seen=200, correct=107, accuracy=0.535000
2025-09-14 02:48:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:48:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:48:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:48:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:48:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:48:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:48:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.162502, avg_loss=0.679063, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:48:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:48:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:48:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:49:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 02:49:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 02:49:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:49:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:49:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:49:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.663635, avg_loss=0.703318, seen=200, correct=104, accuracy=0.520000
2025-09-14 02:49:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:49:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 02:49:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:49:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:49:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.841293, avg_loss=0.696032, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:49:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:49:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:49:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:49:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 02:49:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 02:49:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:49:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:49:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:49:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.740173, avg_loss=0.703701, seen=200, correct=106, accuracy=0.530000
2025-09-14 02:49:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:49:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:49:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:49:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:49:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:49:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:49:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.459522, avg_loss=0.711488, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:49:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:49:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:49:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:49:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 02:49:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 02:49:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:49:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:49:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:49:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:49:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.561981, avg_loss=0.702810, seen=200, correct=110, accuracy=0.550000
2025-09-14 02:49:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:49:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:50:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:50:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:50:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:50:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:50:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.501503, avg_loss=0.687538, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:50:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:50:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:50:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 02:50:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 02:50:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:50:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:50:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:50:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.864075, avg_loss=0.704320, seen=200, correct=104, accuracy=0.520000
2025-09-14 02:50:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:50:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:50:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 02:50:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:50:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:50:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:50:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.747799, avg_loss=0.668695, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:50:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:50:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:50:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:50:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 02:50:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 02:50:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:50:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:50:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.391541, avg_loss=0.711958, seen=200, correct=106, accuracy=0.530000
2025-09-14 02:50:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:50:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:50:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:50:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:50:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:50:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.583286, avg_loss=0.664582, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:50:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:50:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:50:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:50:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:51:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 02:51:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 02:51:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:51:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:51:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:51:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.335480, avg_loss=0.711677, seen=200, correct=106, accuracy=0.530000
2025-09-14 02:51:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:51:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:51:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:51:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:51:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:51:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.100504, avg_loss=0.652513, seen=40, correct=25, accuracy=0.625000
2025-09-14 02:51:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:51:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:51:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:51:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 02:51:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 02:51:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:51:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:51:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:51:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.386261, avg_loss=0.716931, seen=200, correct=102, accuracy=0.510000
2025-09-14 02:51:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:51:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:51:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:51:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:51:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:51:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:51:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.782751, avg_loss=0.669569, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:51:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:51:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:51:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:51:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 02:51:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 02:51:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:51:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:51:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.828888, avg_loss=0.714144, seen=200, correct=99, accuracy=0.495000
2025-09-14 02:51:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:51:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:51:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 02:51:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:51:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:51:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.333370, avg_loss=0.683334, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:51:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:51:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:52:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:52:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 02:52:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 02:52:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:52:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:52:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:52:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.513367, avg_loss=0.707567, seen=200, correct=105, accuracy=0.525000
2025-09-14 02:52:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:52:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:52:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:52:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:52:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.079020, avg_loss=0.676975, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:52:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:52:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:52:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:52:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 02:52:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 02:52:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:52:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:52:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 02:52:24 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #44', 'Round': 0, 'Results_raw': {}}
2025-09-14 02:52:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:52:24 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 02:52:25 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 02:52:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 02:52:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=26, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:52:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:52:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.780006, avg_loss=0.699730, seen=74, correct=41, accuracy=0.554054
2025-09-14 02:52:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:52:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:52:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2200MB
2025-09-14 02:52:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:52:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:52:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:52:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.651522, avg_loss=0.666288, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:52:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:52:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2200MB
2025-09-14 02:52:31 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 02:52:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=353, total=1409)
2025-09-14 02:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:31 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 02:52:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:52:31 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=177, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 02:52:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 02:52:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 02:52:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:52:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:52:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=50.961044, avg_loss=0.688663, seen=74, correct=44, accuracy=0.594595
2025-09-14 02:52:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:52:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:52:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:52:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:52:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:52:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.740925, avg_loss=0.693523, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:52:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:52:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:52:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:52:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:53:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 02:53:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 02:53:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:53:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:53:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:53:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.158707, avg_loss=0.691334, seen=74, correct=45, accuracy=0.608108
2025-09-14 02:53:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:53:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:53:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:53:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:53:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:53:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:53:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.975079, avg_loss=0.699377, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:53:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:53:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:53:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:53:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 02:53:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 02:53:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:53:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:53:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:53:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.529686, avg_loss=0.696347, seen=74, correct=42, accuracy=0.567568
2025-09-14 02:53:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:53:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:53:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:53:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:53:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:53:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:53:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.140518, avg_loss=0.678513, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:53:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:53:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:53:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 02:53:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 02:53:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:53:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:53:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:53:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.206860, avg_loss=0.705498, seen=74, correct=36, accuracy=0.486486
2025-09-14 02:53:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:53:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:53:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:53:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:53:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:53:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:53:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.452213, avg_loss=0.686305, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:53:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:53:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:53:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:53:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:54:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 02:54:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 02:54:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:54:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:54:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:54:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.882458, avg_loss=0.701114, seen=74, correct=34, accuracy=0.459459
2025-09-14 02:54:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:54:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:54:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:54:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:54:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:54:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:54:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.642517, avg_loss=0.691063, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:54:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:54:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:54:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:54:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 02:54:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 02:54:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:54:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:54:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:54:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.217651, avg_loss=0.705644, seen=74, correct=37, accuracy=0.500000
2025-09-14 02:54:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:54:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:54:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:54:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:54:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:54:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.805691, avg_loss=0.670142, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:54:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:54:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:54:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:54:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 02:54:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 02:54:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:54:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:54:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:54:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.364353, avg_loss=0.707626, seen=74, correct=38, accuracy=0.513514
2025-09-14 02:54:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:54:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:54:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:54:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:54:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:54:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:54:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.001694, avg_loss=0.675042, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:54:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:54:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:54:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 02:54:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 02:54:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:54:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:54:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=52.523735, avg_loss=0.709780, seen=74, correct=39, accuracy=0.527027
2025-09-14 02:54:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:54:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:54:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:54:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:54:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:54:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:55:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:55:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.689293, avg_loss=0.667232, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:55:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:55:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:55:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:55:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 02:55:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 02:55:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:55:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:55:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:55:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=53.588657, avg_loss=0.724171, seen=74, correct=38, accuracy=0.513514
2025-09-14 02:55:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:55:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:55:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:55:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:55:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:55:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:55:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.023384, avg_loss=0.675585, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:55:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:55:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:55:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 02:55:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 02:55:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 02:55:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:55:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 02:55:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=53.070644, avg_loss=0.717171, seen=74, correct=41, accuracy=0.554054
2025-09-14 02:55:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:55:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:55:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:55:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:55:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:55:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.283985, avg_loss=0.682100, seen=40, correct=22, accuracy=0.550000
2025-09-14 02:55:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:55:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:55:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 02:55:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 02:55:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:55:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:55:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 02:55:38 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #33', 'Round': 0, 'Results_raw': {}}
2025-09-14 02:55:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:55:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-14 02:55:39 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 02:55:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 02:55:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:55:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:55:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.038574, avg_loss=0.699260, seen=83, correct=42, accuracy=0.506024
2025-09-14 02:55:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:55:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:55:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2200MB
2025-09-14 02:55:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:55:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:55:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:55:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.949343, avg_loss=0.723734, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:55:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:55:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:55:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2200MB
2025-09-14 02:55:45 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 02:55:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=399, total=1594)
2025-09-14 02:55:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:46 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 02:55:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:55:46 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=200, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 02:55:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 02:55:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 02:55:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:55:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:56:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:56:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.285458, avg_loss=0.690186, seen=83, correct=44, accuracy=0.530120
2025-09-14 02:56:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:56:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:56:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:56:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:56:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:56:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.158035, avg_loss=0.728951, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:56:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:56:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:56:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 02:56:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 02:56:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:56:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:56:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.124977, avg_loss=0.688253, seen=83, correct=46, accuracy=0.554217
2025-09-14 02:56:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:56:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:56:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:56:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:56:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:56:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.839470, avg_loss=0.745987, seen=40, correct=18, accuracy=0.450000
2025-09-14 02:56:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:56:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:56:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 02:56:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 02:56:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:56:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:56:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.994354, avg_loss=0.686679, seen=83, correct=49, accuracy=0.590361
2025-09-14 02:56:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:56:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:56:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:56:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:56:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:56:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.673916, avg_loss=0.716848, seen=40, correct=20, accuracy=0.500000
2025-09-14 02:56:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:56:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:56:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 02:56:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 02:56:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:56:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:56:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:56:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.234695, avg_loss=0.701623, seen=83, correct=40, accuracy=0.481928
2025-09-14 02:56:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:56:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:56:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:56:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:56:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:56:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.185268, avg_loss=0.704632, seen=40, correct=23, accuracy=0.575000
2025-09-14 02:56:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:56:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:56:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:57:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 02:57:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 02:57:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:57:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:57:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.836754, avg_loss=0.720925, seen=83, correct=41, accuracy=0.493976
2025-09-14 02:57:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:57:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:57:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:57:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:57:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:57:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.182352, avg_loss=0.704559, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:57:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:57:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:57:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 02:57:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 02:57:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:57:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:57:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.460110, avg_loss=0.716387, seen=83, correct=41, accuracy=0.493976
2025-09-14 02:57:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:57:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:57:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:57:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:57:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.724106, avg_loss=0.718103, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:57:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:57:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:57:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:57:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 02:57:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 02:57:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:57:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:57:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.830044, avg_loss=0.708796, seen=83, correct=39, accuracy=0.469880
2025-09-14 02:57:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:57:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:57:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:57:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:57:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:57:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.554470, avg_loss=0.713862, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:57:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:57:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:57:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:57:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:58:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 02:58:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 02:58:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:58:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:58:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:58:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.970490, avg_loss=0.710488, seen=83, correct=39, accuracy=0.469880
2025-09-14 02:58:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:58:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:58:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:58:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:58:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:58:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:58:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.217791, avg_loss=0.705445, seen=40, correct=19, accuracy=0.475000
2025-09-14 02:58:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:58:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:58:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:58:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 02:58:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 02:58:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:58:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:58:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.174217, avg_loss=0.712942, seen=83, correct=46, accuracy=0.554217
2025-09-14 02:58:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:58:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 02:58:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:58:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:58:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.395725, avg_loss=0.709893, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:58:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:58:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:58:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 02:58:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 02:58:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 02:58:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:58:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 02:58:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=59.162693, avg_loss=0.712804, seen=83, correct=35, accuracy=0.421687
2025-09-14 02:58:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:58:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:58:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:58:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:58:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:58:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.096453, avg_loss=0.702411, seen=40, correct=21, accuracy=0.525000
2025-09-14 02:58:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:58:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:58:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 02:58:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 02:58:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:58:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2217MB
2025-09-14 02:58:51 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 02:58:51 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #39', 'Round': 0, 'Results_raw': {}}
2025-09-14 02:58:51 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 02:58:51 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 02:58:51 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 02:58:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 02:58:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:58:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:58:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:58:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.725433, avg_loss=0.648627, seen=200, correct=130, accuracy=0.650000
2025-09-14 02:58:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:58:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:58:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2200MB
2025-09-14 02:58:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:58:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:59:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:59:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.827240, avg_loss=0.645681, seen=40, correct=24, accuracy=0.600000
2025-09-14 02:59:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:59:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:59:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2280MB allocated=2200MB
2025-09-14 02:59:02 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 02:59:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1122, total=4486)
2025-09-14 02:59:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:02 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 02:59:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:59:02 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=561, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 02:59:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 02:59:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 02:59:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:59:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:59:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:59:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.598534, avg_loss=0.627993, seen=200, correct=131, accuracy=0.655000
2025-09-14 02:59:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:59:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:59:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:59:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:59:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:59:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.714027, avg_loss=0.567851, seen=40, correct=30, accuracy=0.750000
2025-09-14 02:59:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:59:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:59:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 02:59:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 02:59:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 02:59:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:59:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 02:59:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.797379, avg_loss=0.643987, seen=200, correct=130, accuracy=0.650000
2025-09-14 02:59:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:59:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:59:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 02:59:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 02:59:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 02:59:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 02:59:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.701935, avg_loss=0.567548, seen=40, correct=28, accuracy=0.700000
2025-09-14 02:59:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 02:59:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 02:59:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 02:59:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:00:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:00:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:00:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:00:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:00:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.274109, avg_loss=0.641371, seen=200, correct=128, accuracy=0.640000
2025-09-14 03:00:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:00:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:00:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:00:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:00:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:00:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:00:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.695164, avg_loss=0.592379, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:00:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:00:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:00:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:00:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:00:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:00:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:00:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:00:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:00:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.724449, avg_loss=0.638622, seen=200, correct=134, accuracy=0.670000
2025-09-14 03:00:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:00:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:00:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:00:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:00:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.995060, avg_loss=0.599876, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:00:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:00:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:00:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:00:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:00:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:00:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:00:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:00:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.849922, avg_loss=0.639250, seen=200, correct=133, accuracy=0.665000
2025-09-14 03:00:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:00:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:00:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:00:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:00:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:00:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:00:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.996624, avg_loss=0.599916, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:00:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:00:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:00:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:01:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:01:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:01:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:01:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:01:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:01:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.482590, avg_loss=0.637413, seen=200, correct=127, accuracy=0.635000
2025-09-14 03:01:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:01:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:01:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:01:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:01:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.264635, avg_loss=0.581616, seen=40, correct=31, accuracy=0.775000
2025-09-14 03:01:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:01:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:01:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:01:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:01:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:01:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:01:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:01:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:01:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.628387, avg_loss=0.643142, seen=200, correct=124, accuracy=0.620000
2025-09-14 03:01:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:01:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:01:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:01:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:01:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:01:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.014618, avg_loss=0.575365, seen=40, correct=29, accuracy=0.725000
2025-09-14 03:01:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:01:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:01:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:01:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:01:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:01:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:01:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:01:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:02:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:02:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.903732, avg_loss=0.644519, seen=200, correct=121, accuracy=0.605000
2025-09-14 03:02:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:02:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:02:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:02:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:02:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:02:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:02:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.795362, avg_loss=0.594884, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:02:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:02:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:02:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:02:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:02:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:02:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:02:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:02:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:02:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.157013, avg_loss=0.640785, seen=200, correct=125, accuracy=0.625000
2025-09-14 03:02:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:02:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:02:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:02:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:02:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.334198, avg_loss=0.608355, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:02:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:02:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:02:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:02:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:02:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:02:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:02:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:02:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:02:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.682251, avg_loss=0.633411, seen=200, correct=131, accuracy=0.655000
2025-09-14 03:02:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:02:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:02:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:02:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:02:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:02:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.756531, avg_loss=0.593913, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:02:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:02:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:02:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:02:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:02:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:02:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 03:02:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:02:54 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #34', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:02:54 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:02:54 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:02:55 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:02:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:02:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:02:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=31, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:02:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:02:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=92.867561, avg_loss=0.677865, seen=137, correct=85, accuracy=0.620438
2025-09-14 03:02:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:02:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:02:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:03:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:03:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:03:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:03:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:03:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.385178, avg_loss=0.659629, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:03:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:03:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:03:02 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:03:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=652, total=2605)
2025-09-14 03:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:02 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:03:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:03:02 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=326, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:03:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:03:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:03:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:03:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:03:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=88.911789, avg_loss=0.648991, seen=137, correct=89, accuracy=0.649635
2025-09-14 03:03:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:03:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:03:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:03:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:03:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:03:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.264324, avg_loss=0.681608, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:03:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:03:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:03:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:03:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:03:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:03:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:03:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:03:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=88.534348, avg_loss=0.646236, seen=137, correct=90, accuracy=0.656934
2025-09-14 03:03:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:03:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:03:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:03:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:03:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:03:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:03:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.622551, avg_loss=0.690564, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:03:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:03:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:03:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:03:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:03:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:03:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:03:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:03:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:04:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:04:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=87.887833, avg_loss=0.641517, seen=137, correct=89, accuracy=0.649635
2025-09-14 03:04:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:04:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:04:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:04:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:04:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:04:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.108030, avg_loss=0.702701, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:04:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:04:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:04:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:04:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:04:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:04:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:04:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:04:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:04:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=89.316330, avg_loss=0.651944, seen=137, correct=86, accuracy=0.627737
2025-09-14 03:04:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:04:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:04:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:04:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:04:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:04:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.598680, avg_loss=0.689967, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:04:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:04:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:04:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:04:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:04:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:04:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:04:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:04:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=90.217468, avg_loss=0.658522, seen=137, correct=86, accuracy=0.627737
2025-09-14 03:04:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:04:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:04:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:04:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:04:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.419538, avg_loss=0.660488, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:04:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:04:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:04:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:04:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:04:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:04:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:04:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:05:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:05:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=91.997902, avg_loss=0.671518, seen=137, correct=87, accuracy=0.635036
2025-09-14 03:05:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:05:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:05:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:05:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:05:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:05:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:05:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.754393, avg_loss=0.643860, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:05:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:05:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:05:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:05:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:05:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:05:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:05:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:05:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=91.072968, avg_loss=0.664766, seen=137, correct=87, accuracy=0.635036
2025-09-14 03:05:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:05:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:05:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:05:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:05:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:05:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.541622, avg_loss=0.638541, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:05:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:05:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:05:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:05:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:05:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:05:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:05:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:05:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:05:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=89.677925, avg_loss=0.654583, seen=137, correct=88, accuracy=0.642336
2025-09-14 03:05:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:05:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:05:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:05:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:05:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:05:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.940636, avg_loss=0.673516, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:05:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:05:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:05:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:05:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:05:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:05:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:05:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:05:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=88.837494, avg_loss=0.648449, seen=137, correct=93, accuracy=0.678832
2025-09-14 03:05:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:05:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:05:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:05:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:05:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:06:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.295431, avg_loss=0.707386, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:06:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:06:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:06:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:06:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:06:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:06:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 03:06:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 03:06:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=87.286606, avg_loss=0.637129, seen=137, correct=86, accuracy=0.627737
2025-09-14 03:06:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:06:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:06:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:06:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:06:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:06:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.286406, avg_loss=0.682160, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:06:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:06:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:06:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:06:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:06:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:06:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:06:24 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #12', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:06:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:06:24 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 4 for training...
2025-09-14 03:06:24 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:06:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:06:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:06:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=10, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:06:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.983858, avg_loss=0.721774, seen=36, correct=21, accuracy=0.583333
2025-09-14 03:06:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:06:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:06:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:06:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:06:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.077810, avg_loss=0.676945, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:06:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:06:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:06:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:06:29 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:06:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=173, total=691)
2025-09-14 03:06:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:29 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:06:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:29 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=87, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:06:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:06:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:06:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:06:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.205120, avg_loss=0.727920, seen=36, correct=13, accuracy=0.361111
2025-09-14 03:06:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:06:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:06:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:06:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:06:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:06:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.917213, avg_loss=0.647930, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:06:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:06:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:06:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:06:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:06:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:06:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:06:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:06:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:06:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:06:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.375151, avg_loss=0.732643, seen=36, correct=21, accuracy=0.583333
2025-09-14 03:06:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:06:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:07:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:07:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:07:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.491695, avg_loss=0.687292, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:07:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:07:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:07:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:07:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:07:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:07:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:07:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:07:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.986034, avg_loss=0.721834, seen=36, correct=17, accuracy=0.472222
2025-09-14 03:07:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:07:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:07:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:07:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:07:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:07:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:07:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.777391, avg_loss=0.694435, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:07:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:07:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:07:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:07:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:07:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:07:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:07:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.666943, avg_loss=0.712971, seen=36, correct=20, accuracy=0.555556
2025-09-14 03:07:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:07:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:07:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:07:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:07:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:07:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.517399, avg_loss=0.712935, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:07:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:07:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:07:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:07:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:07:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:07:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:07:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.831354, avg_loss=0.717538, seen=36, correct=19, accuracy=0.527778
2025-09-14 03:07:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:07:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:07:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:07:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:07:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:07:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.164938, avg_loss=0.704123, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:07:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:07:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:07:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:07:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:08:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:08:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:08:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:08:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:08:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.009428, avg_loss=0.722484, seen=36, correct=18, accuracy=0.500000
2025-09-14 03:08:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:08:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:08:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:08:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:08:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.399986, avg_loss=0.685000, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:08:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:08:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:08:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:08:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:08:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:08:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:08:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:08:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:08:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.212601, avg_loss=0.728128, seen=36, correct=20, accuracy=0.555556
2025-09-14 03:08:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:08:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:08:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:08:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:08:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.964539, avg_loss=0.699113, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:08:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:08:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:08:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:08:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:08:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:08:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:08:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:08:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:08:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.873629, avg_loss=0.718712, seen=36, correct=19, accuracy=0.527778
2025-09-14 03:08:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:08:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:08:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:08:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:08:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:08:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.104202, avg_loss=0.702605, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:08:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:08:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:08:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:08:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:08:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:08:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:08:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=26.157433, avg_loss=0.726595, seen=36, correct=18, accuracy=0.500000
2025-09-14 03:08:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:08:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:08:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:08:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:08:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:08:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:08:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.707539, avg_loss=0.717688, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:08:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:08:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:08:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:08:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:09:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:09:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:09:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 03:09:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:09:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 03:09:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.868975, avg_loss=0.718583, seen=36, correct=18, accuracy=0.500000
2025-09-14 03:09:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:09:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:09:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:09:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:09:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:09:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.774273, avg_loss=0.694357, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:09:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:09:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:09:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:09:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:09:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:09:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:09:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 03:09:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:09:16 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #3', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:09:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:09:17 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:09:17 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:09:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:09:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:09:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=44, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:09:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:09:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=68.956009, avg_loss=0.615679, seen=112, correct=77, accuracy=0.687500
2025-09-14 03:09:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:09:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:09:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:09:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:09:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:09:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:09:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.520073, avg_loss=0.713002, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:09:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:09:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:09:24 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:09:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=536, total=2144)
2025-09-14 03:09:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:24 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:09:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:09:24 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=268, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:09:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:09:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:09:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:09:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:09:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:09:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.469269, avg_loss=0.593476, seen=112, correct=80, accuracy=0.714286
2025-09-14 03:09:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:09:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:09:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:09:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:09:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.662529, avg_loss=0.691563, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:09:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:09:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:09:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:09:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:09:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:09:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:09:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:09:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:09:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=67.273689, avg_loss=0.600658, seen=112, correct=76, accuracy=0.678571
2025-09-14 03:09:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:09:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:09:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:09:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:09:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:09:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:10:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:10:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.630106, avg_loss=0.690753, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:10:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:10:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:10:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:10:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:10:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:10:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:10:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:10:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:10:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=67.426735, avg_loss=0.602024, seen=112, correct=78, accuracy=0.696429
2025-09-14 03:10:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:10:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:10:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:10:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:10:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.112362, avg_loss=0.702809, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:10:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:10:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:10:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:10:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:10:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:10:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:10:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:10:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=67.791397, avg_loss=0.605280, seen=112, correct=80, accuracy=0.714286
2025-09-14 03:10:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:10:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:10:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:10:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:10:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:10:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.803062, avg_loss=0.720077, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:10:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:10:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:10:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:10:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:10:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:10:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:10:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:10:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:10:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.515472, avg_loss=0.584960, seen=112, correct=78, accuracy=0.696429
2025-09-14 03:10:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:10:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:10:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:10:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:10:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:10:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.906086, avg_loss=0.722652, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:10:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:10:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:10:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:11:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:11:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:11:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:11:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:11:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:11:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.919167, avg_loss=0.597493, seen=112, correct=73, accuracy=0.651786
2025-09-14 03:11:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:11:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:11:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:11:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:11:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:11:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:11:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.674841, avg_loss=0.716871, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:11:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:11:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:11:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:11:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:11:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:11:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:11:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:11:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=66.539665, avg_loss=0.594104, seen=112, correct=78, accuracy=0.696429
2025-09-14 03:11:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:11:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:11:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:11:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:11:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:11:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.268120, avg_loss=0.706703, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:11:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:11:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:11:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:11:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:11:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:11:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:11:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.538437, avg_loss=0.585165, seen=112, correct=79, accuracy=0.705357
2025-09-14 03:11:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:11:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:11:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:11:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:11:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:11:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.092705, avg_loss=0.702318, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:11:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:11:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:11:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:11:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:12:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:12:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:12:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:12:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:12:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=65.818550, avg_loss=0.587666, seen=112, correct=78, accuracy=0.696429
2025-09-14 03:12:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:12:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:12:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:12:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:12:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:12:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:12:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.876900, avg_loss=0.721922, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:12:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:12:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:12:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:12:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:12:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:12:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 03:12:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:12:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 03:12:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=68.520554, avg_loss=0.611791, seen=112, correct=79, accuracy=0.705357
2025-09-14 03:12:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:12:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:12:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:12:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:12:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:12:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.831337, avg_loss=0.720783, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:12:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:12:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:12:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:12:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:12:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:12:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:12:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:12:34 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #32', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:12:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:12:34 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 03:12:35 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:12:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:12:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:12:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:12:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:12:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.220886, avg_loss=0.676104, seen=200, correct=117, accuracy=0.585000
2025-09-14 03:12:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:12:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:12:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:12:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:12:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:12:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:12:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.023922, avg_loss=0.650598, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:12:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:12:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:12:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:12:45 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:12:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1443, total=5772)
2025-09-14 03:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:45 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:12:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:12:45 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=722, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:12:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:12:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:12:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:12:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:12:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:13:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:13:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.647263, avg_loss=0.668236, seen=200, correct=122, accuracy=0.610000
2025-09-14 03:13:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:13:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:13:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:13:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:13:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:13:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.213299, avg_loss=0.630332, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:13:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:13:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:13:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:13:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:13:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:13:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:13:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:13:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.891830, avg_loss=0.674459, seen=200, correct=123, accuracy=0.615000
2025-09-14 03:13:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:13:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:13:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:13:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:13:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:13:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:13:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.930134, avg_loss=0.623253, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:13:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:13:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:13:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:13:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:13:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:13:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:13:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:13:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.733902, avg_loss=0.668670, seen=200, correct=121, accuracy=0.605000
2025-09-14 03:13:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:13:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:13:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:13:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:13:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.142426, avg_loss=0.628561, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:13:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:13:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:14:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:14:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:14:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:14:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:14:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.414841, avg_loss=0.667074, seen=200, correct=113, accuracy=0.565000
2025-09-14 03:14:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:14:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:14:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:14:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:14:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:14:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.193624, avg_loss=0.679841, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:14:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:14:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:14:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:14:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:14:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:14:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:14:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:14:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.787262, avg_loss=0.678936, seen=200, correct=105, accuracy=0.525000
2025-09-14 03:14:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:14:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:14:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:14:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:14:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:14:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:14:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.588882, avg_loss=0.714722, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:14:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:14:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:14:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:14:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:14:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:14:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:14:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:14:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:14:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.805298, avg_loss=0.679026, seen=200, correct=104, accuracy=0.520000
2025-09-14 03:14:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:14:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:14:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:14:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:14:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:14:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:14:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.552990, avg_loss=0.713825, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:14:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:14:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:14:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:15:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:15:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:15:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:15:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:15:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:15:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:15:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.677002, avg_loss=0.668385, seen=200, correct=112, accuracy=0.560000
2025-09-14 03:15:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:15:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:15:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:15:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:15:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:15:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.227766, avg_loss=0.680694, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:15:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:15:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:15:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:15:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:15:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:15:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:15:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.150208, avg_loss=0.665751, seen=200, correct=112, accuracy=0.560000
2025-09-14 03:15:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:15:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:15:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:15:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:15:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:15:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:15:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.127974, avg_loss=0.653199, seen=40, correct=29, accuracy=0.725000
2025-09-14 03:15:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:15:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:15:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:15:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:15:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:15:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:15:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:16:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:16:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.058289, avg_loss=0.665291, seen=200, correct=112, accuracy=0.560000
2025-09-14 03:16:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:16:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:16:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:16:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:16:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:16:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.371748, avg_loss=0.659294, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:16:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:16:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:16:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:16:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:16:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:16:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:16:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:16:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:16:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.399902, avg_loss=0.662000, seen=200, correct=112, accuracy=0.560000
2025-09-14 03:16:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:16:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:16:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:16:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:16:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:16:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:16:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.592066, avg_loss=0.664802, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:16:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:16:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:16:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:16:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:16:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:16:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:16:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 03:16:29 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:16:29 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #42', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:16:30 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:16:30 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 03:16:30 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:16:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:16:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:16:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=15, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:16:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:16:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.351288, avg_loss=0.660890, seen=170, correct=99, accuracy=0.582353
2025-09-14 03:16:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:16:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:16:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:16:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:16:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:16:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:16:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.057709, avg_loss=0.676443, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:16:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:16:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:16:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:16:39 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:16:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=812, total=3247)
2025-09-14 03:16:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:40 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:16:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:16:40 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=406, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:16:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:16:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:16:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:16:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:16:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:16:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.230507, avg_loss=0.660179, seen=170, correct=99, accuracy=0.582353
2025-09-14 03:16:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:16:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:16:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:16:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:16:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:16:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:17:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:17:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.784582, avg_loss=0.694615, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:17:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:17:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:17:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:17:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:17:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:17:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:17:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:17:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:17:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.693909, avg_loss=0.662905, seen=170, correct=105, accuracy=0.617647
2025-09-14 03:17:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:17:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:17:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:17:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:17:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:17:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.136652, avg_loss=0.703416, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:17:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:17:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:17:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:17:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:17:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:17:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:17:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:17:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:17:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.898499, avg_loss=0.664109, seen=170, correct=101, accuracy=0.594118
2025-09-14 03:17:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:17:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:17:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:17:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:17:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:17:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:17:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.617010, avg_loss=0.690425, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:17:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:17:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:17:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:17:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:17:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:17:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:17:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:17:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:18:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:18:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=113.908562, avg_loss=0.670050, seen=170, correct=102, accuracy=0.600000
2025-09-14 03:18:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:18:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:18:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:18:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:18:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:18:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.085773, avg_loss=0.677144, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:18:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:18:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:18:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:18:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:18:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:18:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:18:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:18:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:18:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.369560, avg_loss=0.660997, seen=170, correct=98, accuracy=0.576471
2025-09-14 03:18:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:18:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:18:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:18:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:18:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:18:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:18:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.270380, avg_loss=0.681760, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:18:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:18:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:18:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:18:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:18:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:18:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:18:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:18:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=113.268623, avg_loss=0.666286, seen=170, correct=98, accuracy=0.576471
2025-09-14 03:18:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:18:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:18:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:18:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:18:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:18:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:18:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.559551, avg_loss=0.688989, seen=40, correct=18, accuracy=0.450000
2025-09-14 03:18:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:18:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:18:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:18:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:19:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:19:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:19:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:19:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:19:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:19:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.867340, avg_loss=0.663926, seen=170, correct=107, accuracy=0.629412
2025-09-14 03:19:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:19:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:19:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:19:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:19:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:19:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.571609, avg_loss=0.689290, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:19:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:19:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:19:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:19:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:19:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:19:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:19:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:19:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:19:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.752487, avg_loss=0.663250, seen=170, correct=101, accuracy=0.594118
2025-09-14 03:19:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:19:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:19:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:19:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:19:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:19:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:19:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.207039, avg_loss=0.680176, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:19:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:19:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:19:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:19:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:19:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:19:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:19:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:19:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:19:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.154633, avg_loss=0.659733, seen=170, correct=99, accuracy=0.582353
2025-09-14 03:19:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:19:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:19:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:19:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:19:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:19:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.224487, avg_loss=0.680612, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:19:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:19:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:19:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:19:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:20:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:20:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:20:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 03:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:20:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 03:20:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.441246, avg_loss=0.661419, seen=170, correct=97, accuracy=0.570588
2025-09-14 03:20:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:20:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:20:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:20:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:20:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:20:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:20:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.414490, avg_loss=0.685362, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:20:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:20:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:20:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:20:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:20:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:20:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2217MB
2025-09-14 03:20:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:20:16 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #30', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:20:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:20:16 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:20:17 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:20:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:20:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:20:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=38, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:20:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:20:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=82.278519, avg_loss=0.668931, seen=123, correct=75, accuracy=0.609756
2025-09-14 03:20:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:20:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:20:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:20:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:20:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:20:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:20:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.132446, avg_loss=0.653311, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:20:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:20:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:20:24 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:20:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=586, total=2342)
2025-09-14 03:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:24 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:20:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:20:24 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=293, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:20:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:20:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:20:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:20:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:20:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:20:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.018341, avg_loss=0.650556, seen=123, correct=70, accuracy=0.569106
2025-09-14 03:20:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:20:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:20:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:20:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:20:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:20:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.022593, avg_loss=0.725565, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:20:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:20:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:20:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:20:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:20:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:20:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:20:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:20:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:20:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:20:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.300911, avg_loss=0.660983, seen=123, correct=69, accuracy=0.560976
2025-09-14 03:20:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:20:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:21:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:21:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:21:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:21:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.586422, avg_loss=0.764661, seen=40, correct=18, accuracy=0.450000
2025-09-14 03:21:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:21:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:21:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:21:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:21:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:21:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:21:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:21:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:21:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=81.072662, avg_loss=0.659127, seen=123, correct=79, accuracy=0.642276
2025-09-14 03:21:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:21:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:21:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:21:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:21:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:21:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:21:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.176741, avg_loss=0.729419, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:21:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:21:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:21:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:21:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:21:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:21:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:21:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.359352, avg_loss=0.653328, seen=123, correct=81, accuracy=0.658537
2025-09-14 03:21:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:21:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:21:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:21:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:21:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:21:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:21:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.913013, avg_loss=0.672825, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:21:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:21:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:21:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:21:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:21:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:21:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:21:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:21:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=78.228455, avg_loss=0.636004, seen=123, correct=78, accuracy=0.634146
2025-09-14 03:21:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:21:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:21:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:21:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:21:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:21:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.937767, avg_loss=0.673444, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:21:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:21:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:21:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:22:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:22:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:22:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:22:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:22:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:22:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:22:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.103203, avg_loss=0.643115, seen=123, correct=73, accuracy=0.593496
2025-09-14 03:22:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:22:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:22:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:22:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:22:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.368492, avg_loss=0.684212, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:22:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:22:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:22:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:22:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:22:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:22:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:22:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:22:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:22:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.169052, avg_loss=0.643651, seen=123, correct=70, accuracy=0.569106
2025-09-14 03:22:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:22:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:22:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:22:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:22:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:22:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.883688, avg_loss=0.672092, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:22:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:22:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:22:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:22:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:22:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:22:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:22:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:22:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:22:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=78.936943, avg_loss=0.641764, seen=123, correct=75, accuracy=0.609756
2025-09-14 03:22:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:22:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:22:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:22:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:22:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:22:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:22:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.142239, avg_loss=0.653556, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:22:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:22:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:22:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:22:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:23:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:23:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:23:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:23:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:23:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.713860, avg_loss=0.648080, seen=123, correct=83, accuracy=0.674797
2025-09-14 03:23:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:23:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:23:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:23:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:23:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:23:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:23:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.718815, avg_loss=0.692970, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:23:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:23:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:23:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:23:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:23:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:23:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 03:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:23:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 03:23:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=80.431427, avg_loss=0.653914, seen=123, correct=81, accuracy=0.658537
2025-09-14 03:23:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:23:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:23:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:23:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:23:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.454031, avg_loss=0.686351, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:23:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:23:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:23:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:23:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:23:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2217MB
2025-09-14 03:23:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:23:41 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #27', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:23:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:23:41 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 03:23:42 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:23:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:23:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=2, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:23:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:23:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.448443, avg_loss=0.674889, seen=14, correct=9, accuracy=0.642857
2025-09-14 03:23:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:23:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:23:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:23:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:23:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:23:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:23:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.812710, avg_loss=0.670318, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:23:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:23:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:23:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:23:46 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:23:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=72, total=285)
2025-09-14 03:23:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:47 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:23:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:23:47 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=36, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:23:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:23:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:23:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:23:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.503781, avg_loss=0.678842, seen=14, correct=6, accuracy=0.428571
2025-09-14 03:23:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:23:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:23:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:23:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:23:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:24:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:24:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.036526, avg_loss=0.700913, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:24:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:24:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:24:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:24:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:24:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:24:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:24:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:24:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.074535, avg_loss=0.648181, seen=14, correct=9, accuracy=0.642857
2025-09-14 03:24:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:24:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:24:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 03:24:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:24:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:24:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:24:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.530333, avg_loss=0.688258, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:24:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:24:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:24:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:24:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:24:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:24:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:24:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:24:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:24:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.927816, avg_loss=0.637701, seen=14, correct=9, accuracy=0.642857
2025-09-14 03:24:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:24:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:24:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:24:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:24:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:24:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:24:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.655890, avg_loss=0.691397, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:24:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:24:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:24:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:24:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:24:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:24:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:24:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:24:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:24:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.902207, avg_loss=0.635872, seen=14, correct=9, accuracy=0.642857
2025-09-14 03:24:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:24:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:24:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:24:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:24:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:24:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:24:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.290695, avg_loss=0.682267, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:24:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:24:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:24:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:24:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:24:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:24:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:24:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:24:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:25:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.833344, avg_loss=0.630953, seen=14, correct=9, accuracy=0.642857
2025-09-14 03:25:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:25:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:25:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:25:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:25:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.421591, avg_loss=0.685540, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:25:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:25:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:25:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:25:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:25:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:25:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:25:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:25:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:25:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.779160, avg_loss=0.627083, seen=14, correct=9, accuracy=0.642857
2025-09-14 03:25:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:25:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:25:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:25:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:25:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:25:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:25:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.584728, avg_loss=0.689618, seen=40, correct=18, accuracy=0.450000
2025-09-14 03:25:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:25:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:25:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:25:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:25:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:25:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:25:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:25:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.805861, avg_loss=0.628990, seen=14, correct=9, accuracy=0.642857
2025-09-14 03:25:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:25:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:25:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:25:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:25:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:25:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:25:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.448265, avg_loss=0.686207, seen=40, correct=17, accuracy=0.425000
2025-09-14 03:25:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:25:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:25:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:25:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:25:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:25:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:25:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:25:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.750671, avg_loss=0.625048, seen=14, correct=10, accuracy=0.714286
2025-09-14 03:25:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:25:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:25:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:25:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:25:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:25:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.639996, avg_loss=0.691000, seen=40, correct=17, accuracy=0.425000
2025-09-14 03:25:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:25:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:25:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:25:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:26:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:26:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:26:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:26:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:26:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.699616, avg_loss=0.621401, seen=14, correct=10, accuracy=0.714286
2025-09-14 03:26:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:26:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:26:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:26:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.488972, avg_loss=0.687224, seen=40, correct=17, accuracy=0.425000
2025-09-14 03:26:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:26:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:26:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:26:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 03:26:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 03:26:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=8.755901, avg_loss=0.625422, seen=14, correct=11, accuracy=0.785714
2025-09-14 03:26:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:26:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:26:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:26:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.362364, avg_loss=0.684059, seen=40, correct=17, accuracy=0.425000
2025-09-14 03:26:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:26:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:26:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:26:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2332MB allocated=2217MB
2025-09-14 03:26:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:26:26 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #5', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:26:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:26:26 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:26:26 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:26:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:26:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:26:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=4, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:26:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=20.024311, avg_loss=0.625760, seen=32, correct=17, accuracy=0.531250
2025-09-14 03:26:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:26:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:26:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:26:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.305054, avg_loss=0.607626, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:26:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2282MB allocated=2200MB
2025-09-14 03:26:31 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:26:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=156, total=621)
2025-09-14 03:26:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:31 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:26:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:31 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=78, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:26:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:26:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:26:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:26:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:26:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.214466, avg_loss=0.600452, seen=32, correct=24, accuracy=0.750000
2025-09-14 03:26:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:26:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:26:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:26:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.177073, avg_loss=0.654427, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:26:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:26:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:26:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:26:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:26:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:26:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:26:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:26:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:26:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.433491, avg_loss=0.607297, seen=32, correct=22, accuracy=0.687500
2025-09-14 03:26:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:26:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:27:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:27:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:27:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:27:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:27:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.002211, avg_loss=0.675055, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:27:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:27:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:27:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:27:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:27:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:27:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:27:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:27:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:27:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.740128, avg_loss=0.616879, seen=32, correct=21, accuracy=0.656250
2025-09-14 03:27:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:27:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:27:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:27:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:27:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:27:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:27:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.271345, avg_loss=0.706784, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:27:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:27:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:27:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:27:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:27:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:27:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:27:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:27:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:27:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.959330, avg_loss=0.592479, seen=32, correct=22, accuracy=0.687500
2025-09-14 03:27:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:27:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:27:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:27:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:27:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:27:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:27:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.194626, avg_loss=0.679866, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:27:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:27:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:27:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:27:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:27:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:27:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:27:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:27:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:27:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.870438, avg_loss=0.589701, seen=32, correct=24, accuracy=0.750000
2025-09-14 03:27:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:27:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:27:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:27:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:27:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:27:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:27:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.740036, avg_loss=0.643501, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:27:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:27:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:27:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:27:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:28:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:28:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:28:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:28:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:28:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.915329, avg_loss=0.591104, seen=32, correct=22, accuracy=0.687500
2025-09-14 03:28:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:28:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:28:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:28:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:28:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:28:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:28:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.544003, avg_loss=0.638600, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:28:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:28:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:28:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:28:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:28:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:28:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:28:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:28:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:28:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.252144, avg_loss=0.601629, seen=32, correct=21, accuracy=0.656250
2025-09-14 03:28:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:28:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:28:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:28:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:28:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:28:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:28:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.302107, avg_loss=0.707553, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:28:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:28:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:28:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:28:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:28:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:28:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:28:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:28:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:28:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.763466, avg_loss=0.586358, seen=32, correct=23, accuracy=0.718750
2025-09-14 03:28:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:28:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:28:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:28:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:28:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:28:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:28:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.034119, avg_loss=0.675853, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:28:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:28:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:28:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:28:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:28:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:28:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:28:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:28:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:28:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.613323, avg_loss=0.581666, seen=32, correct=25, accuracy=0.781250
2025-09-14 03:28:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:28:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:28:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:28:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:28:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:28:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:29:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:29:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.907822, avg_loss=0.647696, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:29:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:29:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:29:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:29:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:29:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:29:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 03:29:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:29:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:29:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=18.771145, avg_loss=0.586598, seen=32, correct=25, accuracy=0.781250
2025-09-14 03:29:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:29:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:29:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:29:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:29:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:29:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:29:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.196514, avg_loss=0.654913, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:29:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:29:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:29:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:29:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:29:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:29:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:29:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 03:29:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:29:21 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #11', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:29:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:29:21 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 03:29:22 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:29:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:29:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:29:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=24, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:29:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:29:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.962555, avg_loss=0.652834, seen=75, correct=45, accuracy=0.600000
2025-09-14 03:29:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:29:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:29:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:29:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:29:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:29:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:29:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.138943, avg_loss=0.678474, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:29:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:29:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:29:28 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:29:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=359, total=1434)
2025-09-14 03:29:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:28 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:29:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:29:28 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=180, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:29:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:29:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:29:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:29:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:29:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:29:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.964745, avg_loss=0.652863, seen=75, correct=43, accuracy=0.573333
2025-09-14 03:29:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:29:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:29:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:29:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:29:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:29:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:29:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.524626, avg_loss=0.688116, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:29:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:29:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:29:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:29:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:29:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:29:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:29:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:29:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:30:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:30:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.009956, avg_loss=0.653466, seen=75, correct=44, accuracy=0.586667
2025-09-14 03:30:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:30:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:30:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:30:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:30:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:30:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.895733, avg_loss=0.672393, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:30:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:30:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:30:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:30:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:30:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:30:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:30:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:30:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.953484, avg_loss=0.652713, seen=75, correct=44, accuracy=0.586667
2025-09-14 03:30:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:30:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:30:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:30:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:30:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:30:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:30:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.244003, avg_loss=0.681100, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:30:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:30:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:30:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:30:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:30:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:30:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:30:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:30:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.656403, avg_loss=0.662085, seen=75, correct=41, accuracy=0.546667
2025-09-14 03:30:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:30:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:30:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:30:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:30:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:30:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:30:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.126143, avg_loss=0.678154, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:30:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:30:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:30:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:30:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:30:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:30:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:30:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:30:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:30:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.571075, avg_loss=0.660948, seen=75, correct=44, accuracy=0.586667
2025-09-14 03:30:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:30:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:30:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:30:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:30:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:30:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:30:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:30:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.303522, avg_loss=0.682588, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:30:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:31:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:31:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:31:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:31:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:31:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:31:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:31:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.149033, avg_loss=0.655320, seen=75, correct=44, accuracy=0.586667
2025-09-14 03:31:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:31:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:31:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:31:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:31:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:31:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:31:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.031542, avg_loss=0.700789, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:31:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:31:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:31:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:31:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:31:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:31:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:31:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:31:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.000195, avg_loss=0.653336, seen=75, correct=45, accuracy=0.600000
2025-09-14 03:31:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:31:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:31:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:31:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:31:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:31:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:31:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.455444, avg_loss=0.686386, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:31:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:31:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:31:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:31:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:31:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:31:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:31:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:31:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:31:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.113693, avg_loss=0.654849, seen=75, correct=45, accuracy=0.600000
2025-09-14 03:31:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:31:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:31:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:31:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:31:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:31:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:31:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.030485, avg_loss=0.675762, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:31:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:31:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:31:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:32:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:32:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:32:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:32:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:32:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:32:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.762886, avg_loss=0.663505, seen=75, correct=44, accuracy=0.586667
2025-09-14 03:32:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:32:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:32:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:32:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:32:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:32:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:32:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.381229, avg_loss=0.659531, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:32:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:32:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:32:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:32:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:32:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:32:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 03:32:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:32:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 03:32:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=49.316849, avg_loss=0.657558, seen=75, correct=42, accuracy=0.560000
2025-09-14 03:32:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:32:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:32:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:32:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:32:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:32:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:32:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.628628, avg_loss=0.665716, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:32:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:32:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:32:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:32:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:32:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:32:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:32:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:32:32 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #28', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:32:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:32:32 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:32:33 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:32:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:32:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:32:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:32:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:32:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.647324, avg_loss=0.678237, seen=200, correct=117, accuracy=0.585000
2025-09-14 03:32:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:32:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:32:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:32:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:32:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:32:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:32:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.033195, avg_loss=0.650830, seen=40, correct=29, accuracy=0.725000
2025-09-14 03:32:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:32:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:32:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:32:43 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:32:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1698, total=6791)
2025-09-14 03:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:43 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:32:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:32:43 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=849, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:32:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:32:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:32:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:32:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:32:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:33:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:33:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.402313, avg_loss=0.687012, seen=200, correct=125, accuracy=0.625000
2025-09-14 03:33:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:33:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:33:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:33:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:33:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:33:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:33:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.184715, avg_loss=0.654618, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:33:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:33:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:33:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:33:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:33:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:33:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:33:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:33:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.208588, avg_loss=0.676043, seen=200, correct=112, accuracy=0.560000
2025-09-14 03:33:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:33:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:33:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:33:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:33:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:33:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:33:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.682026, avg_loss=0.667051, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:33:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:33:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:33:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:33:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:33:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:33:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:33:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:33:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.422638, avg_loss=0.677113, seen=200, correct=116, accuracy=0.580000
2025-09-14 03:33:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:33:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:33:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:33:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:33:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:33:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.905691, avg_loss=0.672642, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:33:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:33:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:33:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:33:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:34:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:34:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:34:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:34:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:34:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:34:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.017059, avg_loss=0.670085, seen=200, correct=124, accuracy=0.620000
2025-09-14 03:34:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:34:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:34:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:34:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:34:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:34:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.507090, avg_loss=0.662677, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:34:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:34:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:34:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:34:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:34:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:34:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:34:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:34:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:34:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.749786, avg_loss=0.678749, seen=200, correct=119, accuracy=0.595000
2025-09-14 03:34:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:34:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:34:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:34:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:34:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:34:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:34:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.155416, avg_loss=0.678885, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:34:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:34:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:34:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:34:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:34:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:34:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:34:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:34:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:35:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:35:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.018616, avg_loss=0.675093, seen=200, correct=119, accuracy=0.595000
2025-09-14 03:35:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:35:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:35:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:35:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:35:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:35:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:35:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.427158, avg_loss=0.660679, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:35:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:35:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:35:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:35:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:35:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:35:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:35:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:35:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:35:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.924103, avg_loss=0.664621, seen=200, correct=122, accuracy=0.610000
2025-09-14 03:35:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:35:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:35:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:35:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:35:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:35:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:35:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.123449, avg_loss=0.653086, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:35:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:35:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:35:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:35:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:35:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:35:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:35:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:35:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:35:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.320724, avg_loss=0.666604, seen=200, correct=115, accuracy=0.575000
2025-09-14 03:35:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:35:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:35:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:35:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:35:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:35:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:35:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.721748, avg_loss=0.668044, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:35:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:35:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:35:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:35:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:36:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:36:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:36:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:36:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:36:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:36:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.516449, avg_loss=0.672582, seen=200, correct=116, accuracy=0.580000
2025-09-14 03:36:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:36:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:36:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:36:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:36:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:36:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.848019, avg_loss=0.671200, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:36:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:36:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:36:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:36:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:36:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:36:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:36:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:36:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:36:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.364838, avg_loss=0.671824, seen=200, correct=117, accuracy=0.585000
2025-09-14 03:36:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:36:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:36:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:36:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:36:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:36:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:36:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.621666, avg_loss=0.665542, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:36:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:36:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:36:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:36:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:36:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:36:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:36:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2217MB
2025-09-14 03:36:42 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:36:42 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #53', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:36:42 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:36:42 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 4 for training...
2025-09-14 03:36:43 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:36:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:36:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:36:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=3, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:36:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:36:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.456726, avg_loss=0.691486, seen=193, correct=115, accuracy=0.595855
2025-09-14 03:36:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:36:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:36:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:36:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:36:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:36:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:36:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.524927, avg_loss=0.688123, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:36:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:36:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:36:52 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:36:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=920, total=3679)
2025-09-14 03:36:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:36:53 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:36:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:36:53 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=460, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:37:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:37:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:37:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:37:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:37:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:37:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.160309, avg_loss=0.689950, seen=193, correct=108, accuracy=0.559585
2025-09-14 03:37:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:37:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:37:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:37:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:37:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:37:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.895031, avg_loss=0.647376, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:37:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:37:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:37:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:37:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:37:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:37:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:37:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:37:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:37:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.809280, avg_loss=0.693312, seen=193, correct=108, accuracy=0.559585
2025-09-14 03:37:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:37:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:37:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:37:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:37:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:37:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:37:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.600683, avg_loss=0.665017, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:37:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:37:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:37:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:37:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:37:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:37:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:37:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:37:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:37:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=136.608612, avg_loss=0.707817, seen=193, correct=105, accuracy=0.544041
2025-09-14 03:37:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:37:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:37:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:37:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:37:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:37:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:37:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.602814, avg_loss=0.640070, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:37:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:37:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:37:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:38:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:38:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:38:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:38:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:38:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:38:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.252167, avg_loss=0.695607, seen=193, correct=103, accuracy=0.533679
2025-09-14 03:38:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:38:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:38:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:38:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:38:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:38:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:38:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.003618, avg_loss=0.650090, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:38:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:38:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:38:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:38:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:38:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:38:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:38:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:38:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:38:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=131.757263, avg_loss=0.682680, seen=193, correct=110, accuracy=0.569948
2025-09-14 03:38:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:38:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:38:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:38:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:38:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:38:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:38:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.019215, avg_loss=0.675480, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:38:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:38:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:38:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:38:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:38:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:38:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:38:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:38:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:39:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:39:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=131.667694, avg_loss=0.682216, seen=193, correct=112, accuracy=0.580311
2025-09-14 03:39:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:39:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:39:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:39:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:39:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:39:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:39:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.185928, avg_loss=0.654648, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:39:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:39:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:39:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:39:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:39:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:39:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:39:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:39:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:39:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=135.209869, avg_loss=0.700569, seen=193, correct=100, accuracy=0.518135
2025-09-14 03:39:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:39:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:39:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:39:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:39:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:39:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.641535, avg_loss=0.641038, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:39:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:39:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:39:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:39:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:39:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:39:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:39:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:39:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:39:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=138.719330, avg_loss=0.718753, seen=193, correct=87, accuracy=0.450777
2025-09-14 03:39:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:39:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:39:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:39:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:39:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:39:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:39:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.587149, avg_loss=0.639679, seen=40, correct=28, accuracy=0.700000
2025-09-14 03:39:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:39:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:39:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:39:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:39:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:39:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:39:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:39:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:40:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:40:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=134.628342, avg_loss=0.697556, seen=193, correct=103, accuracy=0.533679
2025-09-14 03:40:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:40:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:40:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:40:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:40:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:40:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:40:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.809858, avg_loss=0.670246, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:40:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:40:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:40:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:40:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:40:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:40:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 03:40:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:40:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 03:40:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=132.287399, avg_loss=0.685427, seen=193, correct=110, accuracy=0.569948
2025-09-14 03:40:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:40:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:40:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:40:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:40:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:40:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:40:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.125732, avg_loss=0.653143, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:40:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:40:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:40:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:40:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:40:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:40:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:40:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 03:40:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:40:32 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #31', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:40:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:40:32 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:40:32 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:40:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:40:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:40:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:40:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:40:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.836655, avg_loss=0.644183, seen=200, correct=119, accuracy=0.595000
2025-09-14 03:40:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:40:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:40:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:40:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:40:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:40:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:40:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.058422, avg_loss=0.626461, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:40:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:40:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:40:42 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:40:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1543, total=6171)
2025-09-14 03:40:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:42 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:40:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:40:42 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=772, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:40:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:40:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:40:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:40:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:40:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:41:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:41:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.437851, avg_loss=0.657189, seen=200, correct=130, accuracy=0.650000
2025-09-14 03:41:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:41:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:41:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:41:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:41:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:41:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:41:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.357746, avg_loss=0.633944, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:41:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:41:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:41:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:41:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:41:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:41:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:41:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:41:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:41:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.499664, avg_loss=0.662498, seen=200, correct=126, accuracy=0.630000
2025-09-14 03:41:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:41:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:41:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:41:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:41:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:41:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:41:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.188763, avg_loss=0.654719, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:41:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:41:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:41:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:41:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:41:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:41:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:41:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:41:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:41:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.339905, avg_loss=0.651700, seen=200, correct=128, accuracy=0.640000
2025-09-14 03:41:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:41:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:41:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:41:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:41:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:41:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:41:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.123142, avg_loss=0.653079, seen=40, correct=21, accuracy=0.525000
2025-09-14 03:41:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:41:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:41:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:41:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:42:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:42:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:42:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:42:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:42:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:42:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.969879, avg_loss=0.649849, seen=200, correct=127, accuracy=0.635000
2025-09-14 03:42:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:42:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:42:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:42:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:42:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:42:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:42:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.429523, avg_loss=0.660738, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:42:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:42:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:42:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:42:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:42:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:42:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:42:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:42:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:42:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.766403, avg_loss=0.653832, seen=200, correct=126, accuracy=0.630000
2025-09-14 03:42:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:42:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:42:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:42:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:42:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:42:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:42:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.044783, avg_loss=0.676120, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:42:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:42:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:42:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:42:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:42:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:42:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:42:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:42:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.577332, avg_loss=0.677887, seen=200, correct=120, accuracy=0.600000
2025-09-14 03:42:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:42:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:42:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:42:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:42:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:42:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:42:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.354763, avg_loss=0.683869, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:42:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:42:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:42:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:42:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:43:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:43:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:43:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:43:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:43:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:43:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.330353, avg_loss=0.706652, seen=200, correct=119, accuracy=0.595000
2025-09-14 03:43:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:43:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:43:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:43:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:43:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:43:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:43:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.238255, avg_loss=0.680956, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:43:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:43:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:43:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:43:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:43:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:43:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:43:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:43:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:43:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.579102, avg_loss=0.667896, seen=200, correct=128, accuracy=0.640000
2025-09-14 03:43:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:43:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:43:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:43:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:43:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:43:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:43:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.202810, avg_loss=0.655070, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:43:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:43:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:43:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:43:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:43:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:43:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:43:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:43:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:44:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.169540, avg_loss=0.640848, seen=200, correct=130, accuracy=0.650000
2025-09-14 03:44:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:44:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:44:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:44:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:44:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.433928, avg_loss=0.660848, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:44:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:44:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:44:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:44:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:44:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:44:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.095276, avg_loss=0.650476, seen=200, correct=125, accuracy=0.625000
2025-09-14 03:44:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:44:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:44:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:44:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:44:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.535011, avg_loss=0.688375, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:44:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:44:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:44:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:44:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:44:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 03:44:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:44:27 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #38', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:44:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:44:27 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:44:28 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:44:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:44:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:44:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=10, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:44:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.810236, avg_loss=0.627008, seen=30, correct=18, accuracy=0.600000
2025-09-14 03:44:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:44:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:44:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:44:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:44:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.484348, avg_loss=0.637109, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:44:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:44:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:44:32 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:44:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=146, total=583)
2025-09-14 03:44:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:33 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:44:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:33 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=73, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:44:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:44:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:44:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:44:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:44:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=19.078682, avg_loss=0.635956, seen=30, correct=17, accuracy=0.566667
2025-09-14 03:44:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:44:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:44:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:44:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.942692, avg_loss=0.648567, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:44:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:44:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:44:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:44:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:44:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:44:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:45:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:45:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=19.771313, avg_loss=0.659044, seen=30, correct=18, accuracy=0.600000
2025-09-14 03:45:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:45:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:45:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:45:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:45:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:45:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:45:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.061340, avg_loss=0.626534, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:45:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:45:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:45:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:45:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:45:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:45:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:45:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:45:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:45:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.761734, avg_loss=0.625391, seen=30, correct=20, accuracy=0.666667
2025-09-14 03:45:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:45:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:45:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:45:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:45:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:45:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:45:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.431450, avg_loss=0.610786, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:45:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:45:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:45:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:45:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:45:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:45:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:45:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:45:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.152077, avg_loss=0.605069, seen=30, correct=19, accuracy=0.633333
2025-09-14 03:45:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:45:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:45:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:45:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:45:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:45:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.292950, avg_loss=0.632324, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:45:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:45:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:45:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:45:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:45:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:45:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:45:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:45:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:45:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.049026, avg_loss=0.601634, seen=30, correct=20, accuracy=0.666667
2025-09-14 03:45:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:45:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:45:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:45:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:45:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:45:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:45:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.204607, avg_loss=0.630115, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:45:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:45:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:45:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:46:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:46:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:46:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:46:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:46:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:46:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.599920, avg_loss=0.619997, seen=30, correct=21, accuracy=0.700000
2025-09-14 03:46:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:46:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:46:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:46:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:46:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:46:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:46:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.495224, avg_loss=0.612381, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:46:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:46:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:46:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:46:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:46:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:46:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:46:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:46:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:46:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.485985, avg_loss=0.616199, seen=30, correct=19, accuracy=0.633333
2025-09-14 03:46:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:46:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:46:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:46:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:46:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:46:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:46:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.556950, avg_loss=0.613924, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:46:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:46:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:46:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:46:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:46:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:46:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:46:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.376413, avg_loss=0.612547, seen=30, correct=17, accuracy=0.566667
2025-09-14 03:46:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:46:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:46:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:46:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:46:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:46:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:46:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.097965, avg_loss=0.627449, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:46:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:46:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:46:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:46:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:46:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:46:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:46:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:46:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:46:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.266899, avg_loss=0.608897, seen=30, correct=20, accuracy=0.666667
2025-09-14 03:46:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:46:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:46:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:46:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:46:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:46:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:47:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:47:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.690718, avg_loss=0.617268, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:47:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:47:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:47:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:47:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:47:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:47:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 03:47:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:47:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 03:47:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.662746, avg_loss=0.622092, seen=30, correct=20, accuracy=0.666667
2025-09-14 03:47:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:47:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:47:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:47:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:47:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:47:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:47:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.027447, avg_loss=0.600686, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:47:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:47:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:47:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:47:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:47:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:47:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2217MB
2025-09-14 03:47:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:47:20 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #23', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:47:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:47:20 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:47:21 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:47:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:47:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:47:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=30, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:47:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:47:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.398510, avg_loss=0.672442, seen=69, correct=41, accuracy=0.594203
2025-09-14 03:47:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:47:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:47:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:47:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:47:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:47:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.204552, avg_loss=0.780114, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:47:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:47:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:47:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 03:47:28 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:47:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=329, total=1316)
2025-09-14 03:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:28 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:47:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:47:28 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=165, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:47:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:47:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:47:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:47:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:47:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=51.558189, avg_loss=0.747220, seen=69, correct=34, accuracy=0.492754
2025-09-14 03:47:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:47:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:47:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:47:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:47:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:47:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:47:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.445911, avg_loss=0.761148, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:47:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:47:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:47:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:47:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:47:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:47:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:47:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:48:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:48:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=49.044106, avg_loss=0.710784, seen=69, correct=38, accuracy=0.550725
2025-09-14 03:48:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:48:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:48:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:48:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:48:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:48:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:48:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.871147, avg_loss=0.771779, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:48:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:48:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:48:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:48:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:48:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:48:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:48:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:48:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:48:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.184013, avg_loss=0.683826, seen=69, correct=38, accuracy=0.550725
2025-09-14 03:48:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:48:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:48:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:48:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:48:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:48:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:48:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.886038, avg_loss=0.747151, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:48:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:48:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:48:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:48:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:48:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:48:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:48:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:48:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:48:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=46.178833, avg_loss=0.669258, seen=69, correct=39, accuracy=0.565217
2025-09-14 03:48:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:48:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:48:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:48:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:48:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:48:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:48:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.676113, avg_loss=0.741903, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:48:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:48:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:48:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:48:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:48:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:48:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:48:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=45.816902, avg_loss=0.664013, seen=69, correct=43, accuracy=0.623188
2025-09-14 03:48:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:48:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:48:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:48:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:48:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:48:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.129066, avg_loss=0.753227, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:48:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:48:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:48:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:49:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:49:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:49:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:49:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:49:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:49:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=48.124886, avg_loss=0.697462, seen=69, correct=40, accuracy=0.579710
2025-09-14 03:49:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:49:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:49:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:49:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:49:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:49:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.629995, avg_loss=0.765750, seen=40, correct=18, accuracy=0.450000
2025-09-14 03:49:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:49:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:49:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:49:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:49:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:49:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:49:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:49:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=48.991882, avg_loss=0.710027, seen=69, correct=42, accuracy=0.608696
2025-09-14 03:49:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:49:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:49:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:49:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:49:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.464085, avg_loss=0.761602, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:49:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:49:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:49:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:49:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:49:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:49:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:49:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:49:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:49:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.852745, avg_loss=0.693518, seen=69, correct=41, accuracy=0.594203
2025-09-14 03:49:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:49:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:49:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:49:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:49:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.432621, avg_loss=0.735816, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:49:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:49:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:49:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:49:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:49:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:49:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:49:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:50:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:50:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=47.228149, avg_loss=0.684466, seen=69, correct=41, accuracy=0.594203
2025-09-14 03:50:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:50:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:50:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:50:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:50:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:50:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:50:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.138821, avg_loss=0.728471, seen=40, correct=20, accuracy=0.500000
2025-09-14 03:50:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:50:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:50:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:50:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:50:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 03:50:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:50:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 03:50:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=49.241417, avg_loss=0.713644, seen=69, correct=37, accuracy=0.536232
2025-09-14 03:50:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:50:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:50:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:50:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:50:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:50:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:50:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.061138, avg_loss=0.751528, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:50:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:50:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:50:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:50:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:50:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:50:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 03:50:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:50:28 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #8', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:50:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:50:28 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:50:28 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:50:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:50:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:50:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:50:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:50:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.940552, avg_loss=0.649703, seen=200, correct=118, accuracy=0.590000
2025-09-14 03:50:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:50:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:50:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 03:50:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:50:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:50:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:50:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.334936, avg_loss=0.683373, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:50:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:50:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:50:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 03:50:40 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:50:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3638, total=14550)
2025-09-14 03:50:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:40 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:50:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:50:40 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=1819, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:50:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:50:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:50:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:50:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:50:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.756363, avg_loss=0.648782, seen=200, correct=126, accuracy=0.630000
2025-09-14 03:50:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:50:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:50:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:51:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:51:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:51:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:51:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:51:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.092770, avg_loss=0.702319, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:51:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:51:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:51:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:51:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:51:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:51:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:51:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:51:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:51:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.763199, avg_loss=0.658816, seen=200, correct=117, accuracy=0.585000
2025-09-14 03:51:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:51:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:51:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:51:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:51:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:51:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.027285, avg_loss=0.675682, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:51:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:51:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:51:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:51:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:51:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:51:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:51:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:51:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.388443, avg_loss=0.676942, seen=200, correct=119, accuracy=0.595000
2025-09-14 03:51:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:51:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:51:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:51:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:51:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:51:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:51:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.465654, avg_loss=0.686641, seen=40, correct=24, accuracy=0.600000
2025-09-14 03:51:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:51:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:51:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:51:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:52:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:52:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:52:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:52:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:52:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:52:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.097168, avg_loss=0.690486, seen=200, correct=112, accuracy=0.560000
2025-09-14 03:52:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:52:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:52:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:52:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:52:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:52:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:52:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.812466, avg_loss=0.670312, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:52:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:52:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:52:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:52:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:52:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:52:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:52:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:52:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:52:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.618011, avg_loss=0.668090, seen=200, correct=119, accuracy=0.595000
2025-09-14 03:52:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:52:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:52:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:52:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:52:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:52:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:52:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.979837, avg_loss=0.649496, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:52:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:52:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:52:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:52:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:52:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:52:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:52:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:52:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.201599, avg_loss=0.666008, seen=200, correct=112, accuracy=0.560000
2025-09-14 03:52:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:52:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:52:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:52:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:52:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:52:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:52:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.812744, avg_loss=0.670319, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:52:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:52:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:53:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:53:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:53:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:53:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:53:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:53:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.500687, avg_loss=0.662503, seen=200, correct=121, accuracy=0.605000
2025-09-14 03:53:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:53:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:53:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:53:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:53:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:53:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:53:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.455679, avg_loss=0.711392, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:53:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:53:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:53:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:53:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:53:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:53:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:53:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:53:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:53:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.008835, avg_loss=0.665044, seen=200, correct=120, accuracy=0.600000
2025-09-14 03:53:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:53:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:53:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:53:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:53:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:53:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:53:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.524378, avg_loss=0.713109, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:53:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:53:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:53:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:53:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:53:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:53:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:53:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:53:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:54:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:54:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.487793, avg_loss=0.657439, seen=200, correct=119, accuracy=0.595000
2025-09-14 03:54:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:54:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:54:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:54:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:54:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:54:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:54:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.609888, avg_loss=0.665247, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:54:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:54:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:54:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:54:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:54:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:54:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:54:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:54:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:54:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.392151, avg_loss=0.646961, seen=200, correct=122, accuracy=0.610000
2025-09-14 03:54:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:54:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:54:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:54:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:54:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:54:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:54:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.938194, avg_loss=0.648455, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:54:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:54:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:54:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:54:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:54:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:54:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:54:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2217MB
2025-09-14 03:54:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:54:34 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #15', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:54:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:54:34 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:54:34 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:54:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:54:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:54:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:54:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:54:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=124.754715, avg_loss=0.623774, seen=200, correct=124, accuracy=0.620000
2025-09-14 03:54:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:54:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:54:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 03:54:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:54:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:54:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.622023, avg_loss=0.640551, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:54:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:54:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 03:54:44 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:54:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1184, total=4736)
2025-09-14 03:54:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:44 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:54:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:54:44 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=592, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:54:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:54:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:54:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:54:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:55:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:55:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.315865, avg_loss=0.636579, seen=200, correct=130, accuracy=0.650000
2025-09-14 03:55:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:55:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:55:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:55:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:55:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:55:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.302176, avg_loss=0.632554, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:55:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:55:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:55:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:55:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:55:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:55:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:55:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.828949, avg_loss=0.644145, seen=200, correct=127, accuracy=0.635000
2025-09-14 03:55:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:55:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:55:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:55:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:55:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:55:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.426283, avg_loss=0.660657, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:55:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:55:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:55:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:55:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:55:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:55:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:55:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:55:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:55:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.633224, avg_loss=0.633166, seen=200, correct=126, accuracy=0.630000
2025-09-14 03:55:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:55:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:55:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:55:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:55:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:55:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.780659, avg_loss=0.644516, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:55:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:55:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:55:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:56:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:56:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:56:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:56:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:56:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:56:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.306366, avg_loss=0.631532, seen=200, correct=127, accuracy=0.635000
2025-09-14 03:56:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:56:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:56:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:56:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:56:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:56:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:56:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.374571, avg_loss=0.659364, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:56:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:56:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:56:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 03:56:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 03:56:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:56:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:56:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:56:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.109192, avg_loss=0.630546, seen=200, correct=127, accuracy=0.635000
2025-09-14 03:56:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:56:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:56:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:56:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:56:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:56:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:56:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.534594, avg_loss=0.638365, seen=40, correct=25, accuracy=0.625000
2025-09-14 03:56:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:56:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:56:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 03:56:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 03:56:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:56:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:56:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:56:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.917824, avg_loss=0.639589, seen=200, correct=121, accuracy=0.605000
2025-09-14 03:56:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:56:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:56:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:56:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:56:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:56:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.345552, avg_loss=0.633639, seen=40, correct=27, accuracy=0.675000
2025-09-14 03:56:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:56:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:56:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:56:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:57:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 03:57:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 03:57:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:57:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:57:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.209824, avg_loss=0.646049, seen=200, correct=120, accuracy=0.600000
2025-09-14 03:57:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:57:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:57:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:57:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:57:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:57:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.373878, avg_loss=0.659347, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:57:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:57:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:57:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 03:57:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 03:57:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:57:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:57:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.430298, avg_loss=0.642151, seen=200, correct=123, accuracy=0.615000
2025-09-14 03:57:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:57:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:57:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:57:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:57:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:57:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.909895, avg_loss=0.672747, seen=40, correct=23, accuracy=0.575000
2025-09-14 03:57:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:57:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:57:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 03:57:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 03:57:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:57:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:57:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:57:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:57:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.972076, avg_loss=0.644860, seen=200, correct=127, accuracy=0.635000
2025-09-14 03:57:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:57:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:58:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:58:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:58:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:58:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.723797, avg_loss=0.668095, seen=40, correct=26, accuracy=0.650000
2025-09-14 03:58:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:58:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 03:58:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 03:58:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 03:58:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:58:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 03:58:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.465485, avg_loss=0.647327, seen=200, correct=125, accuracy=0.625000
2025-09-14 03:58:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:58:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:58:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:58:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:58:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.917337, avg_loss=0.672933, seen=40, correct=22, accuracy=0.550000
2025-09-14 03:58:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:58:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:58:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 03:58:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 03:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2346MB allocated=2217MB
2025-09-14 03:58:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 03:58:28 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #35', 'Round': 0, 'Results_raw': {}}
2025-09-14 03:58:28 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 03:58:28 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 03:58:29 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 03:58:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 03:58:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 03:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=34, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:58:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 03:58:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=87.507080, avg_loss=0.662932, seen=132, correct=85, accuracy=0.643939
2025-09-14 03:58:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 03:58:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:58:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:58:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:58:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.867348, avg_loss=0.821684, seen=40, correct=17, accuracy=0.425000
2025-09-14 03:58:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:58:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 03:58:37 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 03:58:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=631, total=2521)
2025-09-14 03:58:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:37 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 03:58:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:58:37 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 03:58:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 03:58:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 03:58:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 03:58:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:58:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 03:58:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=84.402885, avg_loss=0.639416, seen=132, correct=86, accuracy=0.651515
2025-09-14 03:58:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:58:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 03:58:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:58:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:58:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:58:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=32.433857, avg_loss=0.810846, seen=40, correct=18, accuracy=0.450000
2025-09-14 03:58:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:58:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:58:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:58:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 03:59:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 03:59:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 03:59:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 03:59:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:59:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 03:59:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=86.171814, avg_loss=0.652817, seen=132, correct=85, accuracy=0.643939
2025-09-14 03:59:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:59:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 03:59:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:59:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:59:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.918789, avg_loss=0.797970, seen=40, correct=16, accuracy=0.400000
2025-09-14 03:59:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:59:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:59:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 03:59:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 03:59:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 03:59:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 03:59:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:59:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 03:59:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=92.505653, avg_loss=0.700800, seen=132, correct=75, accuracy=0.568182
2025-09-14 03:59:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:59:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:59:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 03:59:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:59:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:59:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:59:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.886030, avg_loss=0.797151, seen=40, correct=18, accuracy=0.450000
2025-09-14 03:59:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:59:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:59:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 03:59:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 03:59:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 03:59:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 03:59:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:59:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 03:59:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=89.307220, avg_loss=0.676570, seen=132, correct=86, accuracy=0.651515
2025-09-14 03:59:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:59:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 03:59:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 03:59:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 03:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 03:59:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 03:59:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 03:59:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.640499, avg_loss=0.791012, seen=40, correct=19, accuracy=0.475000
2025-09-14 03:59:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 03:59:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:00:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:00:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:00:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:00:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 04:00:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:00:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 04:00:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=87.535263, avg_loss=0.663146, seen=132, correct=81, accuracy=0.613636
2025-09-14 04:00:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:00:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:00:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:00:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:00:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.989761, avg_loss=0.774744, seen=40, correct=16, accuracy=0.400000
2025-09-14 04:00:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:00:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:00:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:00:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:00:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:00:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 04:00:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:00:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 04:00:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=86.319962, avg_loss=0.653939, seen=132, correct=84, accuracy=0.636364
2025-09-14 04:00:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:00:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:00:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:00:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:00:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:00:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:00:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.841629, avg_loss=0.771041, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:00:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:00:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:00:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:00:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:00:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:00:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 04:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:00:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 04:00:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.329330, avg_loss=0.669162, seen=132, correct=75, accuracy=0.568182
2025-09-14 04:00:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:00:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:00:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:00:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:00:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:00:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.794605, avg_loss=0.769865, seen=40, correct=18, accuracy=0.450000
2025-09-14 04:00:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:00:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:01:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:01:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:01:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:01:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 04:01:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:01:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 04:01:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=90.057167, avg_loss=0.682251, seen=132, correct=76, accuracy=0.575758
2025-09-14 04:01:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:01:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:01:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:01:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:01:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:01:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:01:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.847500, avg_loss=0.771187, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:01:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:01:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:01:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:01:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:01:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:01:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 04:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:01:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 04:01:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=88.571602, avg_loss=0.670997, seen=132, correct=80, accuracy=0.606061
2025-09-14 04:01:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:01:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:01:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:01:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:01:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:01:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:01:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.518723, avg_loss=0.762968, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:01:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:01:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:01:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:01:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:01:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 04:01:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:01:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 04:01:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=87.903107, avg_loss=0.665933, seen=132, correct=79, accuracy=0.598485
2025-09-14 04:01:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:01:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:01:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:01:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:01:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:01:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:01:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.221903, avg_loss=0.755548, seen=40, correct=18, accuracy=0.450000
2025-09-14 04:01:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:01:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:02:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:02:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:02:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:02:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:02:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:02:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:02:01 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #49', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:02:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:02:01 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:02:01 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:02:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:02:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:02:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:02:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.292328, avg_loss=0.675385, seen=110, correct=60, accuracy=0.545455
2025-09-14 04:02:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:02:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:02:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 04:02:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:02:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:02:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:02:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.987919, avg_loss=0.749698, seen=40, correct=16, accuracy=0.400000
2025-09-14 04:02:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:02:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:02:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 04:02:08 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:02:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=526, total=2102)
2025-09-14 04:02:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:09 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:02:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:02:09 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:02:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:02:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:02:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:02:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:02:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:02:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.572777, avg_loss=0.677934, seen=110, correct=62, accuracy=0.563636
2025-09-14 04:02:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:02:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:02:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:02:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:02:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:02:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.808472, avg_loss=0.770212, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:02:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:02:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:02:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:02:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:02:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:02:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:02:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:02:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:02:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.065880, avg_loss=0.673326, seen=110, correct=59, accuracy=0.536364
2025-09-14 04:02:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:02:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:02:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:02:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:02:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:02:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:02:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.826134, avg_loss=0.770653, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:02:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:02:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:02:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:02:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:02:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:02:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:02:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:02:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:03:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:03:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.796669, avg_loss=0.679970, seen=110, correct=64, accuracy=0.581818
2025-09-14 04:03:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:03:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:03:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:03:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:03:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:03:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.130856, avg_loss=0.753271, seen=40, correct=17, accuracy=0.425000
2025-09-14 04:03:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:03:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:03:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:03:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:03:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:03:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:03:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:03:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:03:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.273048, avg_loss=0.675210, seen=110, correct=65, accuracy=0.590909
2025-09-14 04:03:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:03:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:03:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:03:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:03:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:03:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.744741, avg_loss=0.743619, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:03:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:03:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:03:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:03:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:03:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:03:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:03:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:03:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:03:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.789864, avg_loss=0.670817, seen=110, correct=68, accuracy=0.618182
2025-09-14 04:03:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:03:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:03:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:03:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:03:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:03:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:03:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.098700, avg_loss=0.727467, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:03:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:03:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:03:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:03:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:03:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:03:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:03:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:03:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:03:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.173584, avg_loss=0.665214, seen=110, correct=70, accuracy=0.636364
2025-09-14 04:03:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:03:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:03:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:03:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:04:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:04:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:04:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.316854, avg_loss=0.732921, seen=40, correct=18, accuracy=0.450000
2025-09-14 04:04:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:04:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:04:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:04:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:04:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:04:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:04:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:04:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:04:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.495956, avg_loss=0.677236, seen=110, correct=64, accuracy=0.581818
2025-09-14 04:04:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:04:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:04:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:04:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:04:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:04:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:04:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.800278, avg_loss=0.745007, seen=40, correct=17, accuracy=0.425000
2025-09-14 04:04:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:04:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:04:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:04:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:04:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:04:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:04:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.129059, avg_loss=0.673901, seen=110, correct=66, accuracy=0.600000
2025-09-14 04:04:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:04:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:04:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:04:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:04:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.064377, avg_loss=0.751609, seen=40, correct=17, accuracy=0.425000
2025-09-14 04:04:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:04:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:04:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:04:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:04:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:04:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:04:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:04:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:04:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=73.976852, avg_loss=0.672517, seen=110, correct=63, accuracy=0.572727
2025-09-14 04:04:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:04:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:04:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:04:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:04:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:04:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:04:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.448843, avg_loss=0.736221, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:04:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:04:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:04:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:04:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:05:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:05:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:05:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:05:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:05:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=72.553909, avg_loss=0.659581, seen=110, correct=63, accuracy=0.572727
2025-09-14 04:05:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:05:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:05:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:05:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:05:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:05:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.203335, avg_loss=0.755083, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:05:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:05:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:05:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:05:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:05:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:05:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2352MB allocated=2217MB
2025-09-14 04:05:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:05:18 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #19', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:05:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:05:18 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:05:18 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:05:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:05:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=16, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:05:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.220360, avg_loss=0.677354, seen=83, correct=52, accuracy=0.626506
2025-09-14 04:05:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:05:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:05:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:05:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:05:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:05:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.475689, avg_loss=0.661892, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:05:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:05:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:05:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:05:24 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:05:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=395, total=1580)
2025-09-14 04:05:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:25 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:05:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:25 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=198, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:05:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:05:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:05:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:05:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:05:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.266926, avg_loss=0.689963, seen=83, correct=48, accuracy=0.578313
2025-09-14 04:05:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:05:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:05:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:05:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:05:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:05:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.063707, avg_loss=0.676593, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:05:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:05:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:05:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:05:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:05:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:05:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:05:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:05:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.365307, avg_loss=0.679100, seen=83, correct=49, accuracy=0.590361
2025-09-14 04:05:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:05:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:05:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:05:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:05:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:05:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:05:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:05:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.390558, avg_loss=0.659764, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:05:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:06:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:06:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:06:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:06:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:06:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:06:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:06:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.180138, avg_loss=0.676869, seen=83, correct=48, accuracy=0.578313
2025-09-14 04:06:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:06:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:06:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:06:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:06:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:06:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:06:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.081451, avg_loss=0.627036, seen=40, correct=30, accuracy=0.750000
2025-09-14 04:06:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:06:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:06:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:06:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:06:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:06:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:06:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:06:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:06:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.423012, avg_loss=0.679795, seen=83, correct=49, accuracy=0.590361
2025-09-14 04:06:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:06:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:06:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:06:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:06:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:06:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:06:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.997726, avg_loss=0.624943, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:06:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:06:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:06:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:06:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:06:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:06:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:06:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:06:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:06:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.375832, avg_loss=0.679227, seen=83, correct=49, accuracy=0.590361
2025-09-14 04:06:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:06:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:06:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:06:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:06:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:06:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:06:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.365248, avg_loss=0.609131, seen=40, correct=29, accuracy=0.725000
2025-09-14 04:06:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:06:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:06:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:06:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:07:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:07:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:07:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:07:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:07:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:07:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.342873, avg_loss=0.678830, seen=83, correct=49, accuracy=0.590361
2025-09-14 04:07:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:07:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:07:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:07:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:07:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:07:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:07:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.603970, avg_loss=0.615099, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:07:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:07:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:07:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:07:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:07:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:07:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:07:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:07:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:07:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.532913, avg_loss=0.681119, seen=83, correct=51, accuracy=0.614458
2025-09-14 04:07:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:07:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:07:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:07:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:07:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:07:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:07:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.602400, avg_loss=0.640060, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:07:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:07:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:07:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:07:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:07:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:07:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:07:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:07:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:07:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.466179, avg_loss=0.680315, seen=83, correct=51, accuracy=0.614458
2025-09-14 04:07:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:07:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:07:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:07:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:07:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:07:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:07:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.377285, avg_loss=0.634432, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:07:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:07:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:07:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:07:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:07:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:08:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:08:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:08:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:08:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.816254, avg_loss=0.684533, seen=83, correct=49, accuracy=0.590361
2025-09-14 04:08:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:08:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:08:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:08:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:08:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.030602, avg_loss=0.650765, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:08:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:08:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:08:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:08:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:08:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 04:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 04:08:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=57.238823, avg_loss=0.689624, seen=83, correct=46, accuracy=0.554217
2025-09-14 04:08:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:08:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:08:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:08:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:08:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.615589, avg_loss=0.640390, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:08:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:08:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:08:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:08:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:08:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2334MB allocated=2217MB
2025-09-14 04:08:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:08:27 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #51', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:08:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:08:27 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-14 04:08:27 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:08:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:08:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:08:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=19, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:08:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.958366, avg_loss=0.665896, seen=54, correct=34, accuracy=0.629630
2025-09-14 04:08:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:08:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:08:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:08:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:08:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.752499, avg_loss=0.668812, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:08:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:08:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:08:33 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:08:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=258, total=1030)
2025-09-14 04:08:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:34 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:08:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:34 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=129, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:08:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:08:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:08:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:08:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:08:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.466751, avg_loss=0.675310, seen=54, correct=31, accuracy=0.574074
2025-09-14 04:08:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:08:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:08:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:08:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:08:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:08:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.463478, avg_loss=0.661587, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:08:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:08:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:08:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:09:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:09:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:09:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:09:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:09:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.779045, avg_loss=0.681093, seen=54, correct=30, accuracy=0.555556
2025-09-14 04:09:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:09:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:09:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:09:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:09:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:09:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:09:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.887875, avg_loss=0.672197, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:09:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:09:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:09:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:09:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:09:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:09:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:09:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:09:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:09:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.620522, avg_loss=0.659639, seen=54, correct=32, accuracy=0.592593
2025-09-14 04:09:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:09:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:09:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:09:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:09:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:09:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.764688, avg_loss=0.669117, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:09:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:09:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:09:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:09:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:09:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:09:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:09:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:09:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:09:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.688072, avg_loss=0.660890, seen=54, correct=35, accuracy=0.648148
2025-09-14 04:09:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:09:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:09:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:09:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:09:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:09:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:09:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.514141, avg_loss=0.662854, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:09:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:09:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:09:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:09:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:09:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:09:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:09:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:09:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:09:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:09:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.806404, avg_loss=0.663082, seen=54, correct=33, accuracy=0.611111
2025-09-14 04:09:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:09:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:10:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:10:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:10:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:10:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:10:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.666626, avg_loss=0.666666, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:10:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:10:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:10:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:10:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:10:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:10:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:10:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:10:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:10:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.948387, avg_loss=0.665711, seen=54, correct=32, accuracy=0.592593
2025-09-14 04:10:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:10:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:10:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:10:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:10:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:10:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.876282, avg_loss=0.646907, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:10:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:10:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:10:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:10:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:10:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:10:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:10:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:10:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:10:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.195240, avg_loss=0.670282, seen=54, correct=28, accuracy=0.518519
2025-09-14 04:10:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:10:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:10:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:10:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:10:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:10:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.322550, avg_loss=0.658064, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:10:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:10:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:10:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:10:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:10:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:10:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:10:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:10:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:10:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.158108, avg_loss=0.669595, seen=54, correct=30, accuracy=0.555556
2025-09-14 04:10:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:10:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:10:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:10:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:10:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:10:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.095650, avg_loss=0.677391, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:10:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:10:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:10:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:10:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:11:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:11:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:11:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:11:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:11:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:11:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=36.120396, avg_loss=0.668896, seen=54, correct=32, accuracy=0.592593
2025-09-14 04:11:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:11:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:11:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:11:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:11:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:11:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:11:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.883217, avg_loss=0.672080, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:11:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:11:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:11:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:11:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:11:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:11:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 04:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:11:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 04:11:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.442799, avg_loss=0.656348, seen=54, correct=32, accuracy=0.592593
2025-09-14 04:11:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:11:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:11:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:11:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:11:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:11:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:11:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.381115, avg_loss=0.659528, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:11:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:11:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:11:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:11:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:11:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:11:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:11:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2308MB allocated=2217MB
2025-09-14 04:11:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:11:37 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #36', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:11:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:11:37 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:11:37 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:11:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:11:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:11:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:11:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:11:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=88.860985, avg_loss=0.653390, seen=136, correct=82, accuracy=0.602941
2025-09-14 04:11:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:11:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:11:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:11:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:11:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:11:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:11:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.079582, avg_loss=0.676990, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:11:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:11:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:11:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:11:46 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:11:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=648, total=2589)
2025-09-14 04:11:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:46 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:11:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:11:46 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=324, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:11:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:11:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:11:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:11:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:11:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:12:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:12:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=90.416115, avg_loss=0.664824, seen=136, correct=81, accuracy=0.595588
2025-09-14 04:12:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:12:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:12:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:12:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:12:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:12:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:12:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.977856, avg_loss=0.699446, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:12:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:12:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:12:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:12:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:12:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:12:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:12:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:12:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:12:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=90.359329, avg_loss=0.664407, seen=136, correct=85, accuracy=0.625000
2025-09-14 04:12:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:12:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:12:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:12:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:12:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:12:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:12:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.040752, avg_loss=0.701019, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:12:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:12:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:12:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:12:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:12:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:12:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:12:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:12:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:12:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.651970, avg_loss=0.659206, seen=136, correct=84, accuracy=0.617647
2025-09-14 04:12:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:12:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:12:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:12:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:12:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:12:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:12:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.200993, avg_loss=0.705025, seen=40, correct=18, accuracy=0.450000
2025-09-14 04:12:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:12:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:12:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:12:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:13:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:13:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:13:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:13:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:13:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:13:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.929108, avg_loss=0.661243, seen=136, correct=79, accuracy=0.580882
2025-09-14 04:13:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:13:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:13:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:13:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:13:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:13:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:13:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.357447, avg_loss=0.708936, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:13:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:13:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:13:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:13:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:13:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:13:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:13:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:13:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=90.079643, avg_loss=0.662350, seen=136, correct=78, accuracy=0.573529
2025-09-14 04:13:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:13:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:13:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:13:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:13:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:13:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:13:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.979948, avg_loss=0.699499, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:13:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:13:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:13:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:13:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:13:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:13:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:13:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:13:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.323288, avg_loss=0.656789, seen=136, correct=83, accuracy=0.610294
2025-09-14 04:13:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:13:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:13:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:13:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:13:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:13:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.309378, avg_loss=0.657734, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:13:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:13:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:13:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:13:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:14:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:14:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:14:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:14:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:14:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:14:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=88.757721, avg_loss=0.652630, seen=136, correct=83, accuracy=0.610294
2025-09-14 04:14:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:14:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:14:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:14:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:14:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:14:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:14:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.019470, avg_loss=0.650487, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:14:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:14:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:14:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:14:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:14:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:14:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:14:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:14:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=88.891571, avg_loss=0.653614, seen=136, correct=85, accuracy=0.625000
2025-09-14 04:14:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:14:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:14:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:14:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:14:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:14:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:14:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.577587, avg_loss=0.639440, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:14:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:14:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:14:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:14:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:14:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:14:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:14:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:14:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:14:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.476173, avg_loss=0.657913, seen=136, correct=86, accuracy=0.632353
2025-09-14 04:14:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:14:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:14:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:14:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:14:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:14:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:14:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.225128, avg_loss=0.630628, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:14:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:14:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:14:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:14:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:15:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:15:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:15:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 04:15:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:15:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:15:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=89.211105, avg_loss=0.655964, seen=136, correct=85, accuracy=0.625000
2025-09-14 04:15:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:15:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:15:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:15:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:15:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:15:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.950230, avg_loss=0.648756, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:15:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:15:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:15:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:15:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:15:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:15:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:15:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:15:17 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #16', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:15:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:15:17 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:15:17 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:15:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:15:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:15:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:15:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.282585, avg_loss=0.643900, seen=134, correct=88, accuracy=0.656716
2025-09-14 04:15:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:15:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:15:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:15:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:15:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:15:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:15:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.395231, avg_loss=0.734881, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:15:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:15:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:15:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:15:25 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:15:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=637, total=2547)
2025-09-14 04:15:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:25 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:15:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:15:25 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=319, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:15:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:15:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:15:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:15:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:15:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=88.306793, avg_loss=0.659006, seen=134, correct=81, accuracy=0.604478
2025-09-14 04:15:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:15:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:15:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:15:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:15:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:15:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:15:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.141926, avg_loss=0.728548, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:15:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:15:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:15:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:15:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:15:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:15:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:15:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:15:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:16:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:16:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=88.710640, avg_loss=0.662020, seen=134, correct=83, accuracy=0.619403
2025-09-14 04:16:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:16:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:16:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:16:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:16:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:16:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:16:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.243177, avg_loss=0.731079, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:16:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:16:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:16:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:16:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:16:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:16:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:16:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:16:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:16:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=88.414276, avg_loss=0.659808, seen=134, correct=85, accuracy=0.634328
2025-09-14 04:16:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:16:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:16:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:16:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:16:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:16:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.260286, avg_loss=0.731507, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:16:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:16:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:16:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:16:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:16:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:16:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:16:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:16:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:16:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=87.324783, avg_loss=0.651677, seen=134, correct=87, accuracy=0.649254
2025-09-14 04:16:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:16:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:16:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:16:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:16:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:16:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:16:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.540165, avg_loss=0.738504, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:16:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:16:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:16:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:16:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:16:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:16:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:16:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:16:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=88.254776, avg_loss=0.658618, seen=134, correct=81, accuracy=0.604478
2025-09-14 04:16:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:16:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:16:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:17:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:17:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:17:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:17:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:17:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.596996, avg_loss=0.739925, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:17:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:17:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:17:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:17:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:17:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:17:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:17:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:17:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=87.270744, avg_loss=0.651274, seen=134, correct=83, accuracy=0.619403
2025-09-14 04:17:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:17:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:17:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:17:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:17:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:17:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:17:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.832489, avg_loss=0.720812, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:17:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:17:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:17:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:17:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:17:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:17:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:17:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:17:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.574165, avg_loss=0.646076, seen=134, correct=85, accuracy=0.634328
2025-09-14 04:17:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:17:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:17:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:17:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:17:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.288393, avg_loss=0.732210, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:17:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:17:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:17:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:17:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:17:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:17:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:17:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:17:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:17:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=85.958000, avg_loss=0.641478, seen=134, correct=85, accuracy=0.634328
2025-09-14 04:17:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:17:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:17:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:17:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:17:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:17:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:17:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:17:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.557018, avg_loss=0.738925, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:17:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:17:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:18:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:18:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:18:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:18:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:18:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:18:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:18:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=85.171585, avg_loss=0.635609, seen=134, correct=83, accuracy=0.619403
2025-09-14 04:18:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:18:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:18:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:18:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:18:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:18:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:18:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.487535, avg_loss=0.737188, seen=40, correct=15, accuracy=0.375000
2025-09-14 04:18:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:18:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:18:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:18:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:18:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 04:18:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:18:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:18:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.941681, avg_loss=0.648819, seen=134, correct=84, accuracy=0.626866
2025-09-14 04:18:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:18:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:18:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:18:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:18:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:18:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:18:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.220968, avg_loss=0.730524, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:18:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:18:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:18:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:18:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:18:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:18:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:18:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2217MB
2025-09-14 04:18:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:18:45 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #6', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:18:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:18:45 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:18:45 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:18:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:18:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:18:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:18:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:18:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.639847, avg_loss=0.683199, seen=200, correct=117, accuracy=0.585000
2025-09-14 04:18:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:18:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:18:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:18:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:18:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:18:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:18:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.746126, avg_loss=0.668653, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:18:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:18:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:18:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 04:18:55 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:18:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1548, total=6191)
2025-09-14 04:18:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:18:56 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:18:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:18:56 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=774, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:19:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:19:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:19:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:19:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:19:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.943848, avg_loss=0.694719, seen=200, correct=113, accuracy=0.565000
2025-09-14 04:19:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:19:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:19:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:19:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:19:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:19:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:19:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.746857, avg_loss=0.718671, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:19:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:19:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:19:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:19:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:19:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:19:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:19:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:19:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:19:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.087189, avg_loss=0.680436, seen=200, correct=111, accuracy=0.555000
2025-09-14 04:19:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:19:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:19:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:19:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:19:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:19:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.360737, avg_loss=0.684018, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:19:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:19:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:19:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:19:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:19:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:19:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:19:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:20:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:20:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.090820, avg_loss=0.675454, seen=200, correct=115, accuracy=0.575000
2025-09-14 04:20:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:20:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:20:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:20:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:20:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:20:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.515106, avg_loss=0.637878, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:20:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:20:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:20:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:20:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:20:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:20:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:20:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:20:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:20:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.054749, avg_loss=0.680274, seen=200, correct=115, accuracy=0.575000
2025-09-14 04:20:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:20:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:20:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:20:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:20:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:20:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:20:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.321173, avg_loss=0.608029, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:20:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:20:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:20:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:20:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:20:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:20:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:20:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:20:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.703598, avg_loss=0.678518, seen=200, correct=115, accuracy=0.575000
2025-09-14 04:20:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:20:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:20:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:20:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:20:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:20:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:20:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.142796, avg_loss=0.603570, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:20:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:20:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:20:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:20:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:20:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:20:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:20:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:21:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:21:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.607758, avg_loss=0.678039, seen=200, correct=113, accuracy=0.565000
2025-09-14 04:21:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:21:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:21:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:21:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:21:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:21:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.348169, avg_loss=0.633704, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:21:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:21:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:21:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:21:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:21:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:21:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:21:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:21:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.982193, avg_loss=0.684911, seen=200, correct=113, accuracy=0.565000
2025-09-14 04:21:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:21:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:21:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:21:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:21:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:21:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:21:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.398006, avg_loss=0.659950, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:21:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:21:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:21:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:21:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:21:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:21:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:21:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:21:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:21:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.057175, avg_loss=0.675286, seen=200, correct=120, accuracy=0.600000
2025-09-14 04:21:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:21:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:21:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:21:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:21:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:21:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:21:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.189291, avg_loss=0.629732, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:21:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:21:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:21:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:21:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:22:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:22:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:22:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:22:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:22:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:22:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.275848, avg_loss=0.676379, seen=200, correct=119, accuracy=0.595000
2025-09-14 04:22:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:22:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:22:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:22:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:22:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:22:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:22:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.874155, avg_loss=0.621854, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:22:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:22:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:22:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:22:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:22:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:22:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:22:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:22:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:22:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.328110, avg_loss=0.676641, seen=200, correct=120, accuracy=0.600000
2025-09-14 04:22:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:22:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:22:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:22:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:22:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:22:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:22:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.531918, avg_loss=0.638298, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:22:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:22:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:22:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:22:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:22:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:22:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:22:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2316MB allocated=2217MB
2025-09-14 04:22:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:22:39 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #29', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:22:40 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:22:40 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 04:22:40 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:22:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:22:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:22:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:22:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:22:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.997833, avg_loss=0.639989, seen=200, correct=132, accuracy=0.660000
2025-09-14 04:22:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:22:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:22:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:22:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:22:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:22:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:22:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.385239, avg_loss=0.659631, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:22:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:22:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:22:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:22:50 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:22:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1471, total=5883)
2025-09-14 04:22:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:22:50 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:22:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:22:50 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=736, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:23:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:23:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:23:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:23:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:23:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:23:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.644775, avg_loss=0.653224, seen=200, correct=128, accuracy=0.640000
2025-09-14 04:23:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:23:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:23:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:23:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:23:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:23:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.611498, avg_loss=0.665287, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:23:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:23:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:23:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:23:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:23:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:23:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:23:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:23:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:23:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.772079, avg_loss=0.648860, seen=200, correct=126, accuracy=0.630000
2025-09-14 04:23:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:23:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:23:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:23:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:23:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:23:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:23:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.104807, avg_loss=0.677620, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:23:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:23:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:23:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:23:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:23:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:23:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:23:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:23:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.242439, avg_loss=0.631212, seen=200, correct=127, accuracy=0.635000
2025-09-14 04:23:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:23:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:23:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:23:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:23:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:23:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.658134, avg_loss=0.666453, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:23:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:23:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:23:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:23:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:24:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:24:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:24:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:24:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:24:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=125.763618, avg_loss=0.628818, seen=200, correct=129, accuracy=0.645000
2025-09-14 04:24:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:24:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:24:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:24:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:24:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:24:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.055489, avg_loss=0.651387, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:24:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:24:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:24:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:24:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:24:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:24:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:24:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:24:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:24:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.730232, avg_loss=0.638651, seen=200, correct=126, accuracy=0.630000
2025-09-14 04:24:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:24:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:24:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:24:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:24:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:24:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.091400, avg_loss=0.652285, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:24:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:24:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:24:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:24:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:24:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:24:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:24:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:24:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:25:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.885178, avg_loss=0.634426, seen=200, correct=123, accuracy=0.615000
2025-09-14 04:25:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:25:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:25:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:25:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:25:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:25:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.930714, avg_loss=0.673268, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:25:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:25:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:25:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:25:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:25:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:25:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:25:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:25:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:25:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.259926, avg_loss=0.636300, seen=200, correct=126, accuracy=0.630000
2025-09-14 04:25:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:25:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:25:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:25:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:25:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:25:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:25:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.348415, avg_loss=0.658710, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:25:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:25:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:25:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:25:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:25:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:25:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:25:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:25:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:25:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=126.163040, avg_loss=0.630815, seen=200, correct=128, accuracy=0.640000
2025-09-14 04:25:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:25:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:25:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:25:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:25:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:25:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:25:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.352951, avg_loss=0.658824, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:25:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:25:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:25:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:25:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:26:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:26:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:26:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:26:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:26:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.649490, avg_loss=0.638247, seen=200, correct=128, accuracy=0.640000
2025-09-14 04:26:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:26:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:26:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:26:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:26:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:26:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:26:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.745506, avg_loss=0.643638, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:26:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:26:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:26:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:26:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:26:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:26:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 04:26:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:26:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 04:26:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.580933, avg_loss=0.637905, seen=200, correct=126, accuracy=0.630000
2025-09-14 04:26:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:26:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:26:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:26:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:26:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:26:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:26:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.873989, avg_loss=0.646850, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:26:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:26:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:26:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:26:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:26:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:26:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:26:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 04:26:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:26:37 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #17', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:26:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:26:37 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:26:38 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:26:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:26:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:26:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=45, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:26:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:26:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.803772, avg_loss=0.680034, seen=110, correct=59, accuracy=0.536364
2025-09-14 04:26:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:26:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:26:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:26:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:26:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:26:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:26:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.806023, avg_loss=0.645151, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:26:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:26:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:26:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:26:44 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:26:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=525, total=2100)
2025-09-14 04:26:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:45 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:26:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:26:45 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=263, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:26:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:26:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:26:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:26:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:26:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:27:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:27:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.003571, avg_loss=0.690942, seen=110, correct=63, accuracy=0.572727
2025-09-14 04:27:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:27:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:27:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:27:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:27:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:27:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:27:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.990807, avg_loss=0.674770, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:27:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:27:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:27:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:27:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:27:16 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:27:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:27:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:27:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:27:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.111382, avg_loss=0.682831, seen=110, correct=63, accuracy=0.572727
2025-09-14 04:27:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:27:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:27:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:27:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:27:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:27:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.043350, avg_loss=0.676084, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:27:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:27:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:27:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:27:35 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:27:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:27:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:27:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:27:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.680359, avg_loss=0.688003, seen=110, correct=61, accuracy=0.554545
2025-09-14 04:27:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:27:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:27:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:27:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:27:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:27:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:27:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.039497, avg_loss=0.650987, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:27:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:27:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:27:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:27:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:27:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:27:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:27:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:27:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:27:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:27:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.687538, avg_loss=0.678978, seen=110, correct=61, accuracy=0.554545
2025-09-14 04:27:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:27:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:28:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:28:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:28:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:28:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.608568, avg_loss=0.665214, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:28:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:28:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:28:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:28:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:28:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:28:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:28:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:28:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:28:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.783546, avg_loss=0.679850, seen=110, correct=63, accuracy=0.572727
2025-09-14 04:28:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:28:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:28:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:28:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:28:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:28:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:28:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.508366, avg_loss=0.662709, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:28:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:28:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:28:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:28:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:28:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:28:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:28:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:28:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:28:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.443779, avg_loss=0.685853, seen=110, correct=60, accuracy=0.545455
2025-09-14 04:28:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:28:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:28:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:28:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:28:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:28:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.306318, avg_loss=0.657658, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:28:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:28:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:28:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:28:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:28:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:28:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:28:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:28:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:28:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=76.153168, avg_loss=0.692302, seen=110, correct=59, accuracy=0.536364
2025-09-14 04:28:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:28:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:28:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:28:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:28:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:28:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:29:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:29:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.011822, avg_loss=0.650296, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:29:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:29:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:29:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:29:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:29:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:29:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:29:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:29:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:29:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.618233, avg_loss=0.687438, seen=110, correct=57, accuracy=0.518182
2025-09-14 04:29:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:29:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:29:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:29:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:29:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:29:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:29:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.689486, avg_loss=0.642237, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:29:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:29:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:29:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:29:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:29:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:29:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:29:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:29:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:29:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=75.306641, avg_loss=0.684606, seen=110, correct=61, accuracy=0.554545
2025-09-14 04:29:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:29:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:29:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:29:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:29:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:29:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:29:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.605711, avg_loss=0.640143, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:29:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:29:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:29:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:29:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:29:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:29:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 04:29:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:29:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 04:29:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.664131, avg_loss=0.678765, seen=110, correct=67, accuracy=0.609091
2025-09-14 04:29:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:29:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:29:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:29:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:29:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:30:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:30:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.536953, avg_loss=0.638424, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:30:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:30:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:30:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:30:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:30:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:30:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:30:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:30:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:30:02 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #46', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:30:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:30:02 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:30:03 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:30:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:30:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:30:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=23, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:30:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:30:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.045158, avg_loss=0.673498, seen=153, correct=90, accuracy=0.588235
2025-09-14 04:30:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:30:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:30:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:30:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:30:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:30:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:30:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.278633, avg_loss=0.706966, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:30:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:30:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:30:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:30:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:30:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=729, total=2915)
2025-09-14 04:30:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:30:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:30:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=365, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:30:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:30:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:30:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:30:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:30:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:30:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.280426, avg_loss=0.681571, seen=153, correct=87, accuracy=0.568627
2025-09-14 04:30:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:30:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:30:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:30:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:30:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:30:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:30:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.733833, avg_loss=0.718346, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:30:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:30:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:30:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:30:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:30:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:30:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:30:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:30:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.377373, avg_loss=0.669133, seen=153, correct=94, accuracy=0.614379
2025-09-14 04:30:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:30:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:30:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:30:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:30:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:30:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:30:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.701458, avg_loss=0.717536, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:30:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:30:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:30:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:30:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:31:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:31:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:31:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:31:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:31:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:31:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.540222, avg_loss=0.670198, seen=153, correct=94, accuracy=0.614379
2025-09-14 04:31:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:31:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:31:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:31:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:31:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:31:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:31:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.895973, avg_loss=0.722399, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:31:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:31:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:31:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:31:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:31:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:31:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:31:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:31:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:31:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.325195, avg_loss=0.681864, seen=153, correct=85, accuracy=0.555556
2025-09-14 04:31:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:31:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:31:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:31:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:31:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:31:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:31:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.713646, avg_loss=0.717841, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:31:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:31:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:31:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:31:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:31:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:31:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:31:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:31:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:31:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.496521, avg_loss=0.682984, seen=153, correct=86, accuracy=0.562092
2025-09-14 04:31:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:31:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:31:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:31:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:31:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:31:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:31:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.484272, avg_loss=0.712107, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:31:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:31:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:31:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:31:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:32:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:32:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:32:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:32:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:32:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.971741, avg_loss=0.679554, seen=153, correct=87, accuracy=0.568627
2025-09-14 04:32:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:32:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:32:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:32:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:32:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:32:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:32:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.902458, avg_loss=0.722561, seen=40, correct=18, accuracy=0.450000
2025-09-14 04:32:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:32:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:32:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:32:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:32:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:32:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:32:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:32:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:32:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=102.987091, avg_loss=0.673118, seen=153, correct=90, accuracy=0.588235
2025-09-14 04:32:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:32:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:32:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:32:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:32:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:32:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:32:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.907167, avg_loss=0.697679, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:32:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:32:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:32:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:32:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:32:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:32:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:32:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:32:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:32:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=104.684822, avg_loss=0.684215, seen=153, correct=85, accuracy=0.555556
2025-09-14 04:32:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:32:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:32:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:32:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:32:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:32:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:33:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:33:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.381107, avg_loss=0.684528, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:33:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:33:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:33:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:33:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:33:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:33:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:33:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:33:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:33:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.973579, avg_loss=0.679566, seen=153, correct=89, accuracy=0.581699
2025-09-14 04:33:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:33:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:33:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:33:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:33:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:33:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:33:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.059927, avg_loss=0.701498, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:33:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:33:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:33:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:33:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:33:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:33:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 04:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:33:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 04:33:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.279907, avg_loss=0.675032, seen=153, correct=87, accuracy=0.568627
2025-09-14 04:33:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:33:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:33:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:33:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:33:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:33:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.514313, avg_loss=0.712858, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:33:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:33:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:33:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:33:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:33:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:33:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2330MB allocated=2217MB
2025-09-14 04:33:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:33:43 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #21', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:33:44 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:33:44 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:33:44 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:33:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:33:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:33:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=26, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:33:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:33:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.877045, avg_loss=0.693041, seen=147, correct=76, accuracy=0.517007
2025-09-14 04:33:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:33:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:33:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:33:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:33:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:33:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:33:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.948286, avg_loss=0.648707, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:33:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:33:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:33:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:33:53 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:33:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=703, total=2812)
2025-09-14 04:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:33:53 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:33:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:33:53 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=352, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:34:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:34:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:34:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:34:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:34:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=104.255539, avg_loss=0.709221, seen=147, correct=77, accuracy=0.523810
2025-09-14 04:34:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:34:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:34:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:34:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:34:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:34:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:34:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.000116, avg_loss=0.700003, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:34:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:34:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:34:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:34:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:34:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:34:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:34:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:34:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:34:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.668198, avg_loss=0.705226, seen=147, correct=67, accuracy=0.455782
2025-09-14 04:34:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:34:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:34:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:34:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:34:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:34:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:34:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.529831, avg_loss=0.663246, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:34:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:34:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:34:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:34:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:34:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:34:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:34:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:34:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.787659, avg_loss=0.706038, seen=147, correct=72, accuracy=0.489796
2025-09-14 04:34:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:34:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:34:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:34:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:34:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:34:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:34:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.533672, avg_loss=0.638342, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:34:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:34:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:34:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:34:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:35:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:35:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:35:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:35:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:35:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:35:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.125862, avg_loss=0.701536, seen=147, correct=76, accuracy=0.517007
2025-09-14 04:35:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:35:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:35:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:35:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:35:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:35:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.672714, avg_loss=0.641818, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:35:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:35:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:35:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:35:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:35:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:35:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:35:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:35:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=103.329079, avg_loss=0.702919, seen=147, correct=81, accuracy=0.551020
2025-09-14 04:35:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:35:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:35:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:35:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:35:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:35:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:35:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.597668, avg_loss=0.639942, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:35:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:35:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:35:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:35:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:35:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:35:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:35:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:35:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:35:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.702194, avg_loss=0.698654, seen=147, correct=85, accuracy=0.578231
2025-09-14 04:35:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:35:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:35:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:35:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:35:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:35:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:35:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.058065, avg_loss=0.651452, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:35:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:35:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:35:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:35:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:36:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:36:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:36:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:36:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:36:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:36:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=102.236900, avg_loss=0.695489, seen=147, correct=82, accuracy=0.557823
2025-09-14 04:36:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:36:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:36:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:36:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:36:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:36:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:36:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.746161, avg_loss=0.668654, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:36:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:36:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:36:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:36:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:36:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:36:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:36:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:36:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:36:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.321510, avg_loss=0.689262, seen=147, correct=81, accuracy=0.551020
2025-09-14 04:36:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:36:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:36:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:36:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:36:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:36:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:36:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.244520, avg_loss=0.681113, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:36:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:36:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:36:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:36:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:36:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:36:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:36:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:36:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:36:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.510956, avg_loss=0.690551, seen=147, correct=82, accuracy=0.557823
2025-09-14 04:36:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:36:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:36:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:36:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:36:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:36:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.320780, avg_loss=0.658019, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:36:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:36:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:36:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:36:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:37:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:37:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:37:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 04:37:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:37:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 04:37:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=100.332527, avg_loss=0.682534, seen=147, correct=84, accuracy=0.571429
2025-09-14 04:37:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:37:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:37:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:37:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:37:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:37:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:37:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.371063, avg_loss=0.659277, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:37:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:37:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:37:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:37:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:37:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:37:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:37:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 04:37:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:37:19 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #47', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:37:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:37:19 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 04:37:19 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:37:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:37:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:37:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:37:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:37:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=127.371384, avg_loss=0.677507, seen=188, correct=116, accuracy=0.617021
2025-09-14 04:37:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:37:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:37:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:37:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:37:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:37:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.734896, avg_loss=0.643372, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:37:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:37:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 04:37:28 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:37:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=893, total=3572)
2025-09-14 04:37:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:28 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:37:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:37:28 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=447, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:37:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:37:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:37:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:37:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:37:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:37:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=121.351479, avg_loss=0.645487, seen=188, correct=112, accuracy=0.595745
2025-09-14 04:37:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:37:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:37:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:37:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:37:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:37:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:37:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.185137, avg_loss=0.679628, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:37:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:37:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:37:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:38:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:38:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:38:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:38:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:38:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:38:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=123.704216, avg_loss=0.658001, seen=188, correct=112, accuracy=0.595745
2025-09-14 04:38:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:38:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:38:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:38:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:38:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:38:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.572216, avg_loss=0.664305, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:38:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:38:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:38:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:38:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:38:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:38:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:38:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:38:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:38:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=123.074181, avg_loss=0.654650, seen=188, correct=117, accuracy=0.622340
2025-09-14 04:38:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:38:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:38:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:38:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:38:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:38:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:38:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.490925, avg_loss=0.662273, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:38:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:38:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:38:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:38:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:38:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:38:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:38:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:38:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:38:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=122.463379, avg_loss=0.651401, seen=188, correct=113, accuracy=0.601064
2025-09-14 04:38:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:38:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:38:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:38:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:38:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:38:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:38:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.260994, avg_loss=0.656525, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:38:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:38:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:38:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:38:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:39:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:39:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:39:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:39:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:39:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:39:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=122.075760, avg_loss=0.649339, seen=188, correct=117, accuracy=0.622340
2025-09-14 04:39:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:39:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:39:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:39:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:39:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:39:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:39:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.592384, avg_loss=0.664810, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:39:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:39:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:39:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:39:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:39:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:39:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:39:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:39:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:39:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=125.267853, avg_loss=0.666318, seen=188, correct=119, accuracy=0.632979
2025-09-14 04:39:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:39:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:39:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:39:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:39:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:39:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:39:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.255215, avg_loss=0.631380, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:39:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:39:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:39:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:39:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:39:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:39:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:39:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:39:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:40:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:40:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=127.246620, avg_loss=0.676844, seen=188, correct=112, accuracy=0.595745
2025-09-14 04:40:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:40:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:40:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:40:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:40:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:40:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:40:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.187696, avg_loss=0.654692, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:40:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:40:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:40:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:40:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:40:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:40:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:40:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:40:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:40:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=124.441589, avg_loss=0.661923, seen=188, correct=109, accuracy=0.579787
2025-09-14 04:40:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:40:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:40:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:40:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:40:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:40:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:40:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.298223, avg_loss=0.657456, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:40:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:40:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:40:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:40:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:40:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:40:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:40:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:40:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:40:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=121.244843, avg_loss=0.644919, seen=188, correct=115, accuracy=0.611702
2025-09-14 04:40:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:40:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:40:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:40:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:40:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:40:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.050709, avg_loss=0.676268, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:40:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:40:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:40:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:40:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:41:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:41:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:41:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:41:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:41:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:41:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=123.533371, avg_loss=0.657092, seen=188, correct=115, accuracy=0.611702
2025-09-14 04:41:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:41:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:41:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:41:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:41:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:41:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.321247, avg_loss=0.658031, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:41:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:41:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:41:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:41:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:41:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:41:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:41:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 04:41:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:41:14 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #9', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:41:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:41:14 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 04:41:15 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:41:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:41:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:41:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=20, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:41:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:41:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.715950, avg_loss=0.666975, seen=160, correct=99, accuracy=0.618750
2025-09-14 04:41:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:41:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:41:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 04:41:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:41:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:41:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:41:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.343418, avg_loss=0.608585, seen=40, correct=29, accuracy=0.725000
2025-09-14 04:41:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:41:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:41:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 04:41:24 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:41:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=764, total=3055)
2025-09-14 04:41:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:24 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:41:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:41:24 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=382, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:41:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:41:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:41:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:41:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:41:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:41:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=108.445953, avg_loss=0.677787, seen=160, correct=91, accuracy=0.568750
2025-09-14 04:41:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:41:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:41:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:41:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:41:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:41:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.095879, avg_loss=0.652397, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:41:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:41:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:41:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:41:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:41:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:41:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:41:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:42:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:42:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=105.077805, avg_loss=0.656736, seen=160, correct=98, accuracy=0.612500
2025-09-14 04:42:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:42:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:42:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:42:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:42:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:42:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:42:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.424952, avg_loss=0.610624, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:42:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:42:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:42:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:42:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:42:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:42:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:42:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:42:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:42:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=111.338326, avg_loss=0.695865, seen=160, correct=96, accuracy=0.600000
2025-09-14 04:42:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:42:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:42:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:42:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:42:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:42:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:42:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.496780, avg_loss=0.637420, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:42:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:42:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:42:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:42:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:42:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:42:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:42:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:42:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=105.556755, avg_loss=0.659730, seen=160, correct=101, accuracy=0.631250
2025-09-14 04:42:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:42:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:42:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:42:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:42:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:42:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:42:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.995829, avg_loss=0.649896, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:42:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:42:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:42:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:42:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:42:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:43:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:43:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:43:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:43:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:43:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=102.440636, avg_loss=0.640254, seen=160, correct=100, accuracy=0.625000
2025-09-14 04:43:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:43:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:43:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:43:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:43:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:43:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:43:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.336802, avg_loss=0.633420, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:43:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:43:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:43:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:43:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:43:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:43:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:43:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:43:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:43:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.549347, avg_loss=0.665933, seen=160, correct=95, accuracy=0.593750
2025-09-14 04:43:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:43:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:43:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:43:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:43:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:43:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:43:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.039783, avg_loss=0.625995, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:43:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:43:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:43:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:43:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:43:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:43:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:43:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:43:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:43:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=107.203354, avg_loss=0.670021, seen=160, correct=97, accuracy=0.606250
2025-09-14 04:43:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:43:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:43:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:43:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:43:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:43:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:43:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.682190, avg_loss=0.617055, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:43:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:43:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:43:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:43:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:44:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:44:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:44:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:44:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:44:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:44:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=103.626434, avg_loss=0.647665, seen=160, correct=100, accuracy=0.625000
2025-09-14 04:44:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:44:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:44:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:44:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:44:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:44:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:44:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.589708, avg_loss=0.614743, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:44:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:44:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:44:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:44:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:44:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:44:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:44:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:44:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:44:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=101.213432, avg_loss=0.632584, seen=160, correct=96, accuracy=0.600000
2025-09-14 04:44:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:44:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:44:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:44:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:44:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:44:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:44:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.808420, avg_loss=0.645211, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:44:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:44:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:44:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:44:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:44:49 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:44:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 04:44:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:44:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 04:44:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=102.131554, avg_loss=0.638322, seen=160, correct=97, accuracy=0.606250
2025-09-14 04:44:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:44:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:44:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:44:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:44:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:44:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:44:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:44:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.858158, avg_loss=0.646454, seen=40, correct=29, accuracy=0.725000
2025-09-14 04:44:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:44:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:45:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:45:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:45:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:45:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:45:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:45:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:45:01 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #14', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:45:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:45:01 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 04:45:02 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:45:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:45:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:45:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=19, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:45:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:45:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=103.734650, avg_loss=0.644315, seen=161, correct=107, accuracy=0.664596
2025-09-14 04:45:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:45:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:45:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 04:45:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:45:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:45:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:45:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.471905, avg_loss=0.611798, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:45:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:45:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 04:45:12 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:45:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=766, total=3063)
2025-09-14 04:45:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:12 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:45:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:45:12 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=383, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:45:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:45:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:45:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:45:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:45:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=103.169266, avg_loss=0.640803, seen=161, correct=108, accuracy=0.670807
2025-09-14 04:45:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:45:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:45:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:45:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:45:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:45:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.951077, avg_loss=0.623777, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:45:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:45:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:45:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:45:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:45:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:45:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:45:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:45:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:45:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=103.041885, avg_loss=0.640012, seen=161, correct=109, accuracy=0.677019
2025-09-14 04:45:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:45:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:45:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:45:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:45:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:45:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:45:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.595387, avg_loss=0.639885, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:45:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:45:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:45:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:46:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:46:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:46:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:46:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:46:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:46:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=99.823257, avg_loss=0.620020, seen=161, correct=110, accuracy=0.683230
2025-09-14 04:46:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:46:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:46:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:46:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:46:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:46:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:46:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.357460, avg_loss=0.633937, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:46:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:46:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:46:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:46:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:46:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:46:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:46:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:46:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:46:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.235626, avg_loss=0.628793, seen=161, correct=106, accuracy=0.658385
2025-09-14 04:46:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:46:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:46:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:46:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:46:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:46:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.383259, avg_loss=0.634581, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:46:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:46:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:46:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:46:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:46:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:46:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:46:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:46:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:46:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=103.399460, avg_loss=0.642233, seen=161, correct=100, accuracy=0.621118
2025-09-14 04:46:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:46:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:46:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:46:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:46:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:46:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:46:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.914507, avg_loss=0.647863, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:46:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:46:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:46:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:47:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:47:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:47:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:47:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:47:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:47:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=100.453316, avg_loss=0.623934, seen=161, correct=107, accuracy=0.664596
2025-09-14 04:47:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:47:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:47:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:47:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:47:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:47:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:47:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.534771, avg_loss=0.613369, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:47:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:47:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:47:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:47:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:47:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:47:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:47:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:47:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.095078, avg_loss=0.627920, seen=161, correct=110, accuracy=0.683230
2025-09-14 04:47:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:47:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:47:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:47:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:47:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:47:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:47:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.222139, avg_loss=0.605553, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:47:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:47:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:47:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:47:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:47:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:47:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:47:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:47:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=102.290100, avg_loss=0.635342, seen=161, correct=108, accuracy=0.670807
2025-09-14 04:47:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:47:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:47:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:48:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:48:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:48:01 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.764107, avg_loss=0.619103, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:48:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:48:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:48:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:48:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:48:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:48:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:48:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:48:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=106.773224, avg_loss=0.663188, seen=161, correct=99, accuracy=0.614907
2025-09-14 04:48:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:48:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:48:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:48:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:48:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.177944, avg_loss=0.654449, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:48:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:48:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:48:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:48:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:48:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 04:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:48:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 04:48:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=109.277031, avg_loss=0.678739, seen=161, correct=93, accuracy=0.577640
2025-09-14 04:48:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:48:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:48:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:48:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:48:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:48:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.592896, avg_loss=0.664822, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:48:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:48:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:48:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:48:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:48:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2217MB
2025-09-14 04:48:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:48:45 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #26', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:48:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:48:45 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:48:45 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:48:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:48:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:48:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=32, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:48:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:48:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.245239, avg_loss=0.698113, seen=135, correct=74, accuracy=0.548148
2025-09-14 04:48:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:48:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 04:48:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:48:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:48:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:48:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.385050, avg_loss=0.609626, seen=40, correct=28, accuracy=0.700000
2025-09-14 04:48:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:48:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 04:48:54 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:48:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=644, total=2576)
2025-09-14 04:48:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:48:54 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:48:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:48:54 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=322, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:49:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:49:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:49:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:49:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:49:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:49:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.888199, avg_loss=0.702876, seen=135, correct=76, accuracy=0.562963
2025-09-14 04:49:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:49:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:49:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:49:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:49:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:49:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:49:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.494156, avg_loss=0.612354, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:49:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:49:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:49:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:49:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:49:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:49:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:49:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.970428, avg_loss=0.703485, seen=135, correct=66, accuracy=0.488889
2025-09-14 04:49:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:49:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:49:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:49:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:49:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:49:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:49:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.209644, avg_loss=0.655241, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:49:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:49:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:49:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:49:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:49:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:49:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:49:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:49:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:49:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.855392, avg_loss=0.710040, seen=135, correct=66, accuracy=0.488889
2025-09-14 04:49:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:49:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:49:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:49:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:49:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:49:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:49:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.128376, avg_loss=0.678209, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:49:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:49:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:49:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:49:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:50:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:50:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:50:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:50:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:50:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:50:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.756775, avg_loss=0.709309, seen=135, correct=66, accuracy=0.488889
2025-09-14 04:50:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:50:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:50:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:50:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:50:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:50:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:50:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.242954, avg_loss=0.681074, seen=40, correct=21, accuracy=0.525000
2025-09-14 04:50:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:50:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:50:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:50:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:50:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:50:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:50:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:50:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:50:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.947678, avg_loss=0.710724, seen=135, correct=66, accuracy=0.488889
2025-09-14 04:50:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:50:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:50:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:50:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:50:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:50:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:50:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.921501, avg_loss=0.698038, seen=40, correct=19, accuracy=0.475000
2025-09-14 04:50:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:50:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:50:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:50:49 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:50:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:50:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:50:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:50:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:50:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.895264, avg_loss=0.702928, seen=135, correct=72, accuracy=0.533333
2025-09-14 04:50:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:50:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:50:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:50:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:50:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:50:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:50:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:50:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.857693, avg_loss=0.671442, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:50:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:50:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:51:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:51:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:51:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:51:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:51:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:51:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:51:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.346039, avg_loss=0.698860, seen=135, correct=72, accuracy=0.533333
2025-09-14 04:51:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:51:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:51:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:51:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:51:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:51:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.266958, avg_loss=0.631674, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:51:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:51:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:51:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:51:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:51:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:51:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:51:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:51:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:51:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=93.653564, avg_loss=0.693730, seen=135, correct=73, accuracy=0.540741
2025-09-14 04:51:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:51:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:51:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:51:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:51:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:51:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:51:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.395748, avg_loss=0.634894, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:51:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:51:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:51:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:51:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:51:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:51:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:51:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:51:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:51:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.326553, avg_loss=0.698715, seen=135, correct=70, accuracy=0.518519
2025-09-14 04:51:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:51:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:51:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:51:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:51:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:51:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:52:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:52:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.385626, avg_loss=0.634641, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:52:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:52:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:52:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:52:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:52:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:52:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 04:52:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:52:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 04:52:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=95.322380, avg_loss=0.706092, seen=135, correct=66, accuracy=0.488889
2025-09-14 04:52:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:52:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:52:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:52:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:52:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:52:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:52:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.400154, avg_loss=0.635004, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:52:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:52:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:52:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:52:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:52:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:52:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:52:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2328MB allocated=2217MB
2025-09-14 04:52:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:52:22 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #18', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:52:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:52:22 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:52:23 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:52:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:52:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:52:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=6, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:52:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:52:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.244232, avg_loss=0.687469, seen=188, correct=103, accuracy=0.547872
2025-09-14 04:52:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:52:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:52:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2200MB
2025-09-14 04:52:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:52:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:52:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.380028, avg_loss=0.634501, seen=40, correct=22, accuracy=0.550000
2025-09-14 04:52:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:52:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:52:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2200MB
2025-09-14 04:52:33 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:52:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=898, total=3589)
2025-09-14 04:52:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:33 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:52:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:52:33 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=449, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:52:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:52:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:52:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:52:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:52:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:52:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.087646, avg_loss=0.686636, seen=188, correct=103, accuracy=0.547872
2025-09-14 04:52:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:52:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:52:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:52:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:52:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:52:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.121338, avg_loss=0.628033, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:52:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:52:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:52:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:52:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:53:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:53:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:53:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:53:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:53:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:53:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.352966, avg_loss=0.688048, seen=188, correct=97, accuracy=0.515957
2025-09-14 04:53:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:53:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:53:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:53:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:53:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:53:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.889341, avg_loss=0.622234, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:53:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:53:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:53:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:53:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:53:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:53:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:53:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:53:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:53:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.661499, avg_loss=0.689689, seen=188, correct=103, accuracy=0.547872
2025-09-14 04:53:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:53:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:53:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:53:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:53:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:53:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.388035, avg_loss=0.634701, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:53:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:53:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:53:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:53:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:53:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:53:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:53:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:53:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:53:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.820374, avg_loss=0.690534, seen=188, correct=102, accuracy=0.542553
2025-09-14 04:53:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:53:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:53:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:53:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:53:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:53:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:53:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:54:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.254656, avg_loss=0.631366, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:54:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:54:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:54:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:54:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:54:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:54:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:54:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:54:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:54:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.529892, avg_loss=0.683670, seen=188, correct=104, accuracy=0.553191
2025-09-14 04:54:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:54:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:54:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:54:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:54:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:54:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:54:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.544786, avg_loss=0.638620, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:54:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:54:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:54:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:54:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:54:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:54:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:54:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:54:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:54:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.990479, avg_loss=0.686120, seen=188, correct=107, accuracy=0.569149
2025-09-14 04:54:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:54:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:54:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:54:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:54:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:54:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:54:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.606993, avg_loss=0.640175, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:54:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:54:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:54:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:54:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:54:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:54:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:54:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:54:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:55:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:55:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.312408, avg_loss=0.687832, seen=188, correct=105, accuracy=0.558511
2025-09-14 04:55:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:55:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:55:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:55:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:55:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:55:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.545870, avg_loss=0.638647, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:55:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:55:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:55:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:55:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:55:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:55:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:55:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:55:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:55:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.889175, avg_loss=0.685581, seen=188, correct=109, accuracy=0.579787
2025-09-14 04:55:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:55:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:55:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:55:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:55:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:55:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:55:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.761005, avg_loss=0.644025, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:55:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:55:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:55:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:55:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:55:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:55:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:55:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:55:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:55:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=128.990677, avg_loss=0.686121, seen=188, correct=104, accuracy=0.553191
2025-09-14 04:55:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:55:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:55:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:55:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:55:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:55:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.927830, avg_loss=0.623196, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:55:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:55:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:55:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:55:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:56:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:56:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:56:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 04:56:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 04:56:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=126.614899, avg_loss=0.673484, seen=188, correct=109, accuracy=0.579787
2025-09-14 04:56:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:56:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:56:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:56:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:56:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.467850, avg_loss=0.636696, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:56:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:56:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:56:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:56:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2217MB
2025-09-14 04:56:14 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:56:14 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #52', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:56:14 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:56:14 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 04:56:15 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:56:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:56:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:56:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=10, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:56:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.293091, avg_loss=0.677450, seen=89, correct=49, accuracy=0.550562
2025-09-14 04:56:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:56:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2200MB
2025-09-14 04:56:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:56:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:56:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.213354, avg_loss=0.630334, seen=40, correct=26, accuracy=0.650000
2025-09-14 04:56:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:56:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2200MB
2025-09-14 04:56:21 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:56:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=424, total=1694)
2025-09-14 04:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:21 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:56:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:21 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=212, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:56:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:56:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:56:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:56:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:56:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.359192, avg_loss=0.689429, seen=89, correct=50, accuracy=0.561798
2025-09-14 04:56:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:56:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:56:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:56:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:56:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.748066, avg_loss=0.643702, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:56:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:56:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:56:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:56:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:56:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:56:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.598755, avg_loss=0.680885, seen=89, correct=50, accuracy=0.561798
2025-09-14 04:56:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:56:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:56:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:56:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:56:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.616005, avg_loss=0.640400, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:56:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:56:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:56:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:56:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:57:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 04:57:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 04:57:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:57:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:57:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:57:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.144680, avg_loss=0.675783, seen=89, correct=53, accuracy=0.595506
2025-09-14 04:57:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:57:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:57:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:57:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:57:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:57:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.564392, avg_loss=0.639110, seen=40, correct=25, accuracy=0.625000
2025-09-14 04:57:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:57:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:57:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:57:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 04:57:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 04:57:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:57:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:57:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:57:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.903297, avg_loss=0.684307, seen=89, correct=45, accuracy=0.505618
2025-09-14 04:57:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:57:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:57:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:57:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:57:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:57:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:57:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.824793, avg_loss=0.620620, seen=40, correct=27, accuracy=0.675000
2025-09-14 04:57:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:57:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:57:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 04:57:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 04:57:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:57:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:57:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:57:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.259151, avg_loss=0.688305, seen=89, correct=46, accuracy=0.516854
2025-09-14 04:57:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:57:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:57:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:57:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:57:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:57:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:57:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.817715, avg_loss=0.620443, seen=40, correct=29, accuracy=0.725000
2025-09-14 04:57:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:57:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:57:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:57:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:58:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 04:58:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 04:58:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:58:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:58:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:58:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.515495, avg_loss=0.691185, seen=89, correct=47, accuracy=0.528090
2025-09-14 04:58:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:58:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:58:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:58:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:58:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:58:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:58:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.664524, avg_loss=0.616613, seen=40, correct=29, accuracy=0.725000
2025-09-14 04:58:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:58:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:58:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:58:23 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 04:58:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 04:58:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:58:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:58:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.732880, avg_loss=0.693628, seen=89, correct=45, accuracy=0.505618
2025-09-14 04:58:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:58:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:58:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:58:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:58:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:58:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:58:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.934261, avg_loss=0.623357, seen=40, correct=30, accuracy=0.750000
2025-09-14 04:58:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:58:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:58:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:58:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 04:58:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 04:58:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:58:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:58:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:58:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.224987, avg_loss=0.687921, seen=89, correct=47, accuracy=0.528090
2025-09-14 04:58:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:58:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:58:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:58:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:58:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:58:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:58:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.118900, avg_loss=0.627973, seen=40, correct=30, accuracy=0.750000
2025-09-14 04:58:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:58:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:58:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:58:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 04:58:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 04:58:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:58:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:59:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=61.344131, avg_loss=0.689260, seen=89, correct=51, accuracy=0.573034
2025-09-14 04:59:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:59:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:59:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:59:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.876575, avg_loss=0.646914, seen=40, correct=23, accuracy=0.575000
2025-09-14 04:59:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:59:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 04:59:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 04:59:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 04:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 04:59:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=62.265976, avg_loss=0.699618, seen=89, correct=47, accuracy=0.528090
2025-09-14 04:59:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:59:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:59:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:59:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.858280, avg_loss=0.646457, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:59:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:59:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 04:59:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 04:59:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2358MB allocated=2217MB
2025-09-14 04:59:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 04:59:26 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #43', 'Round': 0, 'Results_raw': {}}
2025-09-14 04:59:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 04:59:27 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-14 04:59:27 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 04:59:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 04:59:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 04:59:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 04:59:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.420978, avg_loss=0.674634, seen=11, correct=7, accuracy=0.636364
2025-09-14 04:59:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2200MB
2025-09-14 04:59:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:59:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:59:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.547169, avg_loss=0.638679, seen=40, correct=24, accuracy=0.600000
2025-09-14 04:59:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2200MB
2025-09-14 04:59:32 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 04:59:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=214)
2025-09-14 04:59:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:32 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 04:59:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:32 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 04:59:42 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 04:59:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 04:59:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 04:59:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 04:59:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.759007, avg_loss=0.796273, seen=11, correct=4, accuracy=0.363636
2025-09-14 04:59:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 04:59:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 04:59:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 04:59:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.843607, avg_loss=0.696090, seen=40, correct=20, accuracy=0.500000
2025-09-14 04:59:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 04:59:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 04:59:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 04:59:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 04:59:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 04:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 04:59:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 04:59:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 04:59:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.805750, avg_loss=0.891432, seen=11, correct=3, accuracy=0.272727
2025-09-14 04:59:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 04:59:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:00:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:00:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:00:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:00:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.387732, avg_loss=0.734693, seen=40, correct=18, accuracy=0.450000
2025-09-14 05:00:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:00:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:00:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:00:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:00:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:00:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:00:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:00:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.201345, avg_loss=0.927395, seen=11, correct=3, accuracy=0.272727
2025-09-14 05:00:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:00:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:00:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:00:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:00:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:00:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:00:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.805405, avg_loss=0.745135, seen=40, correct=18, accuracy=0.450000
2025-09-14 05:00:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:00:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:00:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:00:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:00:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:00:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:00:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=9.919521, avg_loss=0.901775, seen=11, correct=3, accuracy=0.272727
2025-09-14 05:00:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:00:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:00:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:00:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:00:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:00:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.645161, avg_loss=0.741129, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:00:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:00:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:00:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:00:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:00:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:00:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:00:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:00:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:00:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.204278, avg_loss=0.927662, seen=11, correct=3, accuracy=0.272727
2025-09-14 05:00:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:00:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:00:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:00:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:00:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:00:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:00:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.382578, avg_loss=0.734564, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:00:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:00:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:00:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:00:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:00:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:00:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:00:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:00:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:00:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.398970, avg_loss=0.945361, seen=11, correct=3, accuracy=0.272727
2025-09-14 05:00:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:00:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:01:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:01:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:01:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:01:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:01:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.153933, avg_loss=0.728848, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:01:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:01:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:01:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:01:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:01:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:01:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:01:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:01:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.648218, avg_loss=0.968020, seen=11, correct=3, accuracy=0.272727
2025-09-14 05:01:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:01:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:01:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:01:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:01:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:01:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.523632, avg_loss=0.738091, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:01:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:01:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:01:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:01:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:01:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:01:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:01:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:01:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=10.709589, avg_loss=0.973599, seen=11, correct=3, accuracy=0.272727
2025-09-14 05:01:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:01:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:01:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:01:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:01:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:01:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:01:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.509190, avg_loss=0.737730, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:01:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:01:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:01:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:01:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:01:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:01:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:01:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:01:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:01:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=11.009192, avg_loss=1.000836, seen=11, correct=4, accuracy=0.363636
2025-09-14 05:01:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:01:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:01:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:01:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:01:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:01:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:01:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.739761, avg_loss=0.743494, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:01:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:01:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:01:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:01:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:02:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:02:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:02:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:02:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=11.113279, avg_loss=1.010298, seen=11, correct=4, accuracy=0.363636
2025-09-14 05:02:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:02:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:02:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:02:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:02:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.160305, avg_loss=0.729008, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:02:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:02:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:02:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:02:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:02:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2344MB allocated=2217MB
2025-09-14 05:02:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:02:10 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #2', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:02:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:02:10 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:02:11 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:02:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:02:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=28, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:02:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.656120, avg_loss=0.675779, seen=72, correct=38, accuracy=0.527778
2025-09-14 05:02:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:02:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2200MB
2025-09-14 05:02:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:02:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:02:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.046883, avg_loss=0.651172, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:02:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:02:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2200MB
2025-09-14 05:02:16 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:02:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=343, total=1372)
2025-09-14 05:02:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:16 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:02:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:16 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=172, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:02:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:02:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:02:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:02:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:02:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.993195, avg_loss=0.680461, seen=72, correct=41, accuracy=0.569444
2025-09-14 05:02:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:02:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:02:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:02:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:02:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.819805, avg_loss=0.695495, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:02:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:02:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:02:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:02:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:02:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:02:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=49.779079, avg_loss=0.691376, seen=72, correct=41, accuracy=0.569444
2025-09-14 05:02:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:02:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:02:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:02:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:02:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.676613, avg_loss=0.716915, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:02:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:02:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:02:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:02:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:03:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:03:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:03:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:03:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:03:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=50.261383, avg_loss=0.698075, seen=72, correct=42, accuracy=0.583333
2025-09-14 05:03:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:03:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:03:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:03:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:03:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:03:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:03:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.744682, avg_loss=0.693617, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:03:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:03:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:03:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:03:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:03:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:03:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:03:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:03:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:03:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.849049, avg_loss=0.678459, seen=72, correct=39, accuracy=0.541667
2025-09-14 05:03:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:03:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:03:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:03:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:03:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:03:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:03:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.482786, avg_loss=0.662070, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:03:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:03:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:03:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:03:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:03:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:03:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:03:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:03:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:03:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.976204, avg_loss=0.680225, seen=72, correct=41, accuracy=0.569444
2025-09-14 05:03:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:03:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:03:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:03:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:03:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:03:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:03:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.793877, avg_loss=0.644847, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:03:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:03:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:03:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:03:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:04:00 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:04:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:04:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:04:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:04:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:04:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.225189, avg_loss=0.669794, seen=72, correct=47, accuracy=0.652778
2025-09-14 05:04:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:04:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:04:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:04:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:04:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:04:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:04:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.990490, avg_loss=0.649762, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:04:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:04:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:04:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:04:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:04:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:04:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:04:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:04:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:04:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=47.942673, avg_loss=0.665870, seen=72, correct=46, accuracy=0.638889
2025-09-14 05:04:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:04:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:04:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:04:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:04:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:04:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:04:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.953655, avg_loss=0.648841, seen=40, correct=27, accuracy=0.675000
2025-09-14 05:04:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:04:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:04:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:04:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:04:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:04:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:04:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:04:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:04:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.186703, avg_loss=0.669260, seen=72, correct=42, accuracy=0.583333
2025-09-14 05:04:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:04:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:04:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:04:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:04:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:04:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:04:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.978024, avg_loss=0.649451, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:04:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:04:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:04:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:04:50 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:04:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:04:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:04:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:04:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:04:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.050739, avg_loss=0.667371, seen=72, correct=41, accuracy=0.569444
2025-09-14 05:04:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:04:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:04:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:04:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:04:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:04:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:04:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:04:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.233288, avg_loss=0.655832, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:04:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:04:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:05:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:05:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:05:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:05:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:05:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:05:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:05:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.354729, avg_loss=0.671593, seen=72, correct=40, accuracy=0.555556
2025-09-14 05:05:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:05:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:05:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:05:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:05:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:05:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:05:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.295530, avg_loss=0.682388, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:05:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:05:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:05:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:05:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:05:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:05:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:05:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2304MB allocated=2217MB
2025-09-14 05:05:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:05:20 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #13', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:05:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:05:20 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 05:05:20 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:05:20 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:05:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:05:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=40, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:05:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:05:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.836250, avg_loss=0.687700, seen=119, correct=77, accuracy=0.647059
2025-09-14 05:05:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:05:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:05:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:05:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:05:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:05:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:05:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.494659, avg_loss=0.712366, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:05:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:05:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:05:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:05:28 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:05:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=569, total=2275)
2025-09-14 05:05:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:28 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:05:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:05:28 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=285, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:05:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:05:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:05:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:05:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:05:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:05:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.835365, avg_loss=0.696096, seen=119, correct=72, accuracy=0.605042
2025-09-14 05:05:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:05:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:05:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:05:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:05:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:05:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:05:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.128012, avg_loss=0.703200, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:05:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:05:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:05:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:05:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:05:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:05:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:05:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:05:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:06:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:06:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=80.663521, avg_loss=0.677845, seen=119, correct=71, accuracy=0.596639
2025-09-14 05:06:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:06:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:06:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:06:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:06:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:06:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:06:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.930687, avg_loss=0.673267, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:06:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:06:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:06:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:06:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:06:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:06:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:06:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:06:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:06:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.297188, avg_loss=0.691573, seen=119, correct=62, accuracy=0.521008
2025-09-14 05:06:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:06:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:06:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:06:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:06:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:06:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:06:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.545929, avg_loss=0.663648, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:06:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:06:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:06:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:06:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:06:37 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:06:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:06:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:06:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:06:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.355293, avg_loss=0.692061, seen=119, correct=66, accuracy=0.554622
2025-09-14 05:06:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:06:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:06:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:06:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:06:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:06:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:06:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.400354, avg_loss=0.685009, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:06:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:06:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:06:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:06:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:06:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:06:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:06:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:06:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:06:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:06:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.550781, avg_loss=0.685301, seen=119, correct=69, accuracy=0.579832
2025-09-14 05:06:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:06:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:07:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:07:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:07:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:07:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:07:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.222797, avg_loss=0.680570, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:07:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:07:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:07:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:07:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:07:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:07:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:07:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:07:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:07:18 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.959000, avg_loss=0.688731, seen=119, correct=64, accuracy=0.537815
2025-09-14 05:07:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:07:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:07:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:07:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:07:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:07:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:07:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.689989, avg_loss=0.667250, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:07:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:07:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:07:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:07:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:07:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:07:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:07:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:07:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:07:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=82.528450, avg_loss=0.693516, seen=119, correct=62, accuracy=0.521008
2025-09-14 05:07:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:07:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:07:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:07:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:07:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:07:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:07:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.256016, avg_loss=0.656400, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:07:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:07:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:07:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:07:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:07:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:07:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:07:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:07:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:07:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.679169, avg_loss=0.686380, seen=119, correct=70, accuracy=0.588235
2025-09-14 05:07:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:07:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:07:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:07:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:07:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:07:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:07:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.290998, avg_loss=0.682275, seen=40, correct=27, accuracy=0.675000
2025-09-14 05:07:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:07:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:07:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:08:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:08:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:08:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:08:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:08:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:08:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:08:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=81.384476, avg_loss=0.683903, seen=119, correct=73, accuracy=0.613445
2025-09-14 05:08:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:08:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:08:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:08:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:08:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:08:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:08:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.976454, avg_loss=0.699411, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:08:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:08:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:08:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:08:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:08:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:08:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 05:08:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:08:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 05:08:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=80.304596, avg_loss=0.674829, seen=119, correct=76, accuracy=0.638655
2025-09-14 05:08:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:08:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:08:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:08:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:08:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:08:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:08:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.236391, avg_loss=0.680910, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:08:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:08:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:08:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:08:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:08:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:08:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:08:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:08:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:08:41 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #41', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:08:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:08:41 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:08:42 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:08:42 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:08:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:08:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:08:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:08:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.120956, avg_loss=0.705605, seen=200, correct=111, accuracy=0.555000
2025-09-14 05:08:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:08:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:08:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:08:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:08:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:08:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:08:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.888559, avg_loss=0.722214, seen=40, correct=15, accuracy=0.375000
2025-09-14 05:08:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:08:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:08:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:08:52 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:08:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1162, total=4647)
2025-09-14 05:08:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:08:52 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:08:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:08:52 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=581, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:09:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:09:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:09:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:09:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:09:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:09:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=139.706116, avg_loss=0.698531, seen=200, correct=102, accuracy=0.510000
2025-09-14 05:09:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:09:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:09:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:09:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:09:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:09:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.306362, avg_loss=0.757659, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:09:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:09:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:09:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:09:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:09:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:09:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:09:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:09:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:09:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.663300, avg_loss=0.708316, seen=200, correct=104, accuracy=0.520000
2025-09-14 05:09:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:09:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:09:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:09:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:09:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:09:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:09:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.969591, avg_loss=0.724240, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:09:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:09:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:09:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:09:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:09:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:09:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:09:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:09:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:09:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.681061, avg_loss=0.708405, seen=200, correct=106, accuracy=0.530000
2025-09-14 05:09:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:09:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:09:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:09:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:09:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:09:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:09:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.727180, avg_loss=0.718180, seen=40, correct=17, accuracy=0.425000
2025-09-14 05:09:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:09:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:09:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:09:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:10:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:10:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:10:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:10:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:10:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:10:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.856018, avg_loss=0.704280, seen=200, correct=108, accuracy=0.540000
2025-09-14 05:10:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:10:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:10:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:10:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:10:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:10:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:10:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.881931, avg_loss=0.722048, seen=40, correct=16, accuracy=0.400000
2025-09-14 05:10:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:10:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:10:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:10:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:10:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:10:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:10:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:10:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:10:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.102448, avg_loss=0.715512, seen=200, correct=103, accuracy=0.515000
2025-09-14 05:10:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:10:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:10:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:10:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:10:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:10:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:10:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.220987, avg_loss=0.730525, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:10:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:10:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:10:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:10:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:10:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:10:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:10:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:10:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:11:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:11:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=143.791260, avg_loss=0.718956, seen=200, correct=104, accuracy=0.520000
2025-09-14 05:11:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:11:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:11:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:11:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:11:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:11:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:11:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.131550, avg_loss=0.728289, seen=40, correct=18, accuracy=0.450000
2025-09-14 05:11:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:11:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:11:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:11:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:11:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:11:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:11:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:11:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:11:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=142.620270, avg_loss=0.713101, seen=200, correct=105, accuracy=0.525000
2025-09-14 05:11:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:11:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:11:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:11:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:11:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:11:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:11:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.786522, avg_loss=0.719663, seen=40, correct=17, accuracy=0.425000
2025-09-14 05:11:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:11:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:11:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:11:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:11:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:11:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:11:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:11:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:11:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.787491, avg_loss=0.703937, seen=200, correct=110, accuracy=0.550000
2025-09-14 05:11:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:11:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:11:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:11:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:11:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:11:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:11:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.003525, avg_loss=0.725088, seen=40, correct=18, accuracy=0.450000
2025-09-14 05:11:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:11:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:11:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:11:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:12:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:12:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:12:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:12:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:12:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.139862, avg_loss=0.705699, seen=200, correct=106, accuracy=0.530000
2025-09-14 05:12:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:12:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:12:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:12:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:12:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:12:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.843304, avg_loss=0.746083, seen=40, correct=18, accuracy=0.450000
2025-09-14 05:12:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:12:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:12:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:12:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:12:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:12:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:12:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:12:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.348846, avg_loss=0.706744, seen=200, correct=105, accuracy=0.525000
2025-09-14 05:12:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:12:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:12:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:12:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:12:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:12:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.677893, avg_loss=0.741947, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:12:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:12:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:12:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:12:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:12:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:12:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:12:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:12:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:12:35 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #25', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:12:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:12:35 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:12:36 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:12:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:12:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:12:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=13, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:12:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.800781, avg_loss=0.645628, seen=57, correct=36, accuracy=0.631579
2025-09-14 05:12:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:12:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:12:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:12:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:12:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:12:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.477499, avg_loss=0.611937, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:12:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:12:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:12:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:12:42 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:12:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=272, total=1088)
2025-09-14 05:12:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:42 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:12:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:42 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=136, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:12:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:12:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:12:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:12:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:12:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.053844, avg_loss=0.685155, seen=57, correct=33, accuracy=0.578947
2025-09-14 05:12:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:12:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:12:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:12:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:12:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:12:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:12:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:12:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.724018, avg_loss=0.618100, seen=40, correct=27, accuracy=0.675000
2025-09-14 05:12:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:13:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:13:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:13:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:13:12 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:13:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:13:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:13:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:13:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.988365, avg_loss=0.666463, seen=57, correct=38, accuracy=0.666667
2025-09-14 05:13:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:13:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:13:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:13:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:13:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:13:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:13:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.091969, avg_loss=0.602299, seen=40, correct=31, accuracy=0.775000
2025-09-14 05:13:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:13:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:13:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:13:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:13:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:13:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:13:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:13:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:13:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=37.865852, avg_loss=0.664313, seen=57, correct=36, accuracy=0.631579
2025-09-14 05:13:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:13:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:13:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:13:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:13:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:13:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:13:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.024027, avg_loss=0.600601, seen=40, correct=31, accuracy=0.775000
2025-09-14 05:13:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:13:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:13:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:13:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:13:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:13:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:13:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:13:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:13:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.908806, avg_loss=0.647523, seen=57, correct=37, accuracy=0.649123
2025-09-14 05:13:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:13:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:13:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:13:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:13:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:13:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:13:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.532063, avg_loss=0.588302, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:13:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:13:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:13:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:13:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:14:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:14:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:14:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:14:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:14:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:14:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.665386, avg_loss=0.643252, seen=57, correct=37, accuracy=0.649123
2025-09-14 05:14:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:14:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:14:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:14:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:14:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:14:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:14:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.727337, avg_loss=0.593183, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:14:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:14:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:14:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:14:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:14:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:14:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:14:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:14:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:14:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.556774, avg_loss=0.676435, seen=57, correct=35, accuracy=0.614035
2025-09-14 05:14:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:14:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:14:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:14:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:14:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:14:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:14:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.549709, avg_loss=0.588743, seen=40, correct=30, accuracy=0.750000
2025-09-14 05:14:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:14:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:14:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:14:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:14:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:14:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:14:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:14:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:14:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=40.050407, avg_loss=0.702639, seen=57, correct=29, accuracy=0.508772
2025-09-14 05:14:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:14:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:14:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:14:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:14:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:14:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:14:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.887516, avg_loss=0.597188, seen=40, correct=30, accuracy=0.750000
2025-09-14 05:14:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:14:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:14:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:14:55 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:14:55 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:14:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:14:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:14:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:14:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.981384, avg_loss=0.701428, seen=57, correct=31, accuracy=0.543860
2025-09-14 05:14:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:14:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:14:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:15:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:15:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:15:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:15:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.653534, avg_loss=0.591338, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:15:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:15:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:15:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:15:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:15:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:15:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:15:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:15:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=39.150665, avg_loss=0.686854, seen=57, correct=28, accuracy=0.491228
2025-09-14 05:15:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:15:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:15:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:15:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:15:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:15:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.445126, avg_loss=0.586128, seen=40, correct=30, accuracy=0.750000
2025-09-14 05:15:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:15:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:15:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:15:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:15:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:15:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:15:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:15:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=38.250900, avg_loss=0.671068, seen=57, correct=33, accuracy=0.578947
2025-09-14 05:15:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:15:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:15:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:15:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:15:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.169716, avg_loss=0.579243, seen=40, correct=29, accuracy=0.725000
2025-09-14 05:15:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:15:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:15:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:15:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:15:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2310MB allocated=2217MB
2025-09-14 05:15:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:15:38 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #7', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:15:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:15:38 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:15:38 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:15:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:15:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:15:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:15:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:15:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.850082, avg_loss=0.679250, seen=200, correct=110, accuracy=0.550000
2025-09-14 05:15:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:15:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:15:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:15:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:15:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:15:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.554890, avg_loss=0.713872, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:15:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:15:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:15:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:15:47 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:15:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1236, total=4944)
2025-09-14 05:15:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:47 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:15:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:15:47 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=618, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:15:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:15:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:15:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:15:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:15:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:16:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:16:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.843658, avg_loss=0.679218, seen=200, correct=112, accuracy=0.560000
2025-09-14 05:16:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:16:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:16:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:16:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:16:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:16:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:16:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.611366, avg_loss=0.715284, seen=40, correct=18, accuracy=0.450000
2025-09-14 05:16:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:16:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:16:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:16:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:16:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:16:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:16:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:16:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:16:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.757645, avg_loss=0.678788, seen=200, correct=110, accuracy=0.550000
2025-09-14 05:16:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:16:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:16:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:16:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:16:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:16:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:16:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.070778, avg_loss=0.726769, seen=40, correct=15, accuracy=0.375000
2025-09-14 05:16:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:16:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:16:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:16:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:16:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:16:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:16:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:16:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:16:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.390228, avg_loss=0.676951, seen=200, correct=110, accuracy=0.550000
2025-09-14 05:16:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:16:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:16:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:16:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:16:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:16:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:16:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.694090, avg_loss=0.717352, seen=40, correct=16, accuracy=0.400000
2025-09-14 05:16:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:16:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:16:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:16:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:17:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:17:06 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:17:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:17:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:17:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:17:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.881683, avg_loss=0.674408, seen=200, correct=111, accuracy=0.555000
2025-09-14 05:17:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:17:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:17:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:17:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:17:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:17:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:17:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.759666, avg_loss=0.718992, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:17:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:17:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:17:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:17:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:17:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:17:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:17:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:17:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:17:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.126770, avg_loss=0.675634, seen=200, correct=105, accuracy=0.525000
2025-09-14 05:17:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:17:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:17:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:17:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:17:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:17:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:17:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.293545, avg_loss=0.707339, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:17:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:17:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:17:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:17:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:17:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:17:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:17:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:17:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:17:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:17:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.584518, avg_loss=0.677923, seen=200, correct=106, accuracy=0.530000
2025-09-14 05:17:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:17:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:18:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:18:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:18:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:18:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:18:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.499838, avg_loss=0.712496, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:18:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:18:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:18:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:18:12 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:18:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:18:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:18:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:18:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:18:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.609253, avg_loss=0.678046, seen=200, correct=108, accuracy=0.540000
2025-09-14 05:18:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:18:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:18:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:18:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:18:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:18:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:18:24 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.360031, avg_loss=0.709001, seen=40, correct=16, accuracy=0.400000
2025-09-14 05:18:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:18:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:18:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:18:34 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:18:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:18:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:18:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:18:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:18:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.975006, avg_loss=0.679875, seen=200, correct=111, accuracy=0.555000
2025-09-14 05:18:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:18:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:18:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:18:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:18:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:18:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:18:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.242146, avg_loss=0.706054, seen=40, correct=17, accuracy=0.425000
2025-09-14 05:18:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:18:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:18:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:18:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:18:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:18:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:18:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:18:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:19:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:19:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.082443, avg_loss=0.680412, seen=200, correct=115, accuracy=0.575000
2025-09-14 05:19:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:19:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:19:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2300MB allocated=2217MB
2025-09-14 05:19:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:19:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:19:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:19:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.027987, avg_loss=0.700700, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:19:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:19:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:19:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:19:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:19:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:19:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:19:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:19:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:19:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.267944, avg_loss=0.681340, seen=200, correct=113, accuracy=0.565000
2025-09-14 05:19:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:19:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:19:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:19:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:19:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:19:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:19:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.919222, avg_loss=0.697981, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:19:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:19:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:19:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:19:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:19:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:19:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:19:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:19:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:19:32 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #24', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:19:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:19:32 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:19:32 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:19:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:19:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:19:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:19:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:19:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.683929, avg_loss=0.678420, seen=200, correct=117, accuracy=0.585000
2025-09-14 05:19:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:19:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:19:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:19:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:19:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:19:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:19:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.018730, avg_loss=0.675468, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:19:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:19:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:19:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:19:42 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:19:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1069, total=4273)
2025-09-14 05:19:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:42 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:19:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:19:42 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=535, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:19:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:19:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:19:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:19:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:19:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:19:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:19:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.518463, avg_loss=0.677592, seen=200, correct=115, accuracy=0.575000
2025-09-14 05:19:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:19:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:20:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:20:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:20:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:20:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:20:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.507290, avg_loss=0.662682, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:20:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:20:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:20:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:20:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:20:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:20:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:20:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:20:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:20:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.314667, avg_loss=0.686573, seen=200, correct=117, accuracy=0.585000
2025-09-14 05:20:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:20:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:20:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:20:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:20:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:20:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:20:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.114300, avg_loss=0.702857, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:20:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:20:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:20:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:20:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:20:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:20:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:20:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:20:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:20:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=138.474274, avg_loss=0.692371, seen=200, correct=115, accuracy=0.575000
2025-09-14 05:20:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:20:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:20:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:20:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:20:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:20:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:20:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.077253, avg_loss=0.701931, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:20:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:20:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:20:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:20:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:21:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:21:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:21:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:21:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:21:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:21:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.528503, avg_loss=0.687643, seen=200, correct=114, accuracy=0.570000
2025-09-14 05:21:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:21:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:21:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:21:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:21:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:21:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:21:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.743328, avg_loss=0.693583, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:21:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:21:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:21:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:21:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:21:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:21:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:21:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:21:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:21:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.824738, avg_loss=0.684124, seen=200, correct=115, accuracy=0.575000
2025-09-14 05:21:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:21:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:21:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:21:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:21:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:21:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:21:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.481535, avg_loss=0.687038, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:21:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:21:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:21:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:21:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:21:45 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:21:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:21:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:21:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:21:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.072174, avg_loss=0.680361, seen=200, correct=113, accuracy=0.565000
2025-09-14 05:21:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:21:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:21:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:21:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:21:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:21:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:21:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.600948, avg_loss=0.665024, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:21:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:21:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:21:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:21:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:22:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:22:09 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:22:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:22:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:22:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:22:17 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.768265, avg_loss=0.683841, seen=200, correct=104, accuracy=0.520000
2025-09-14 05:22:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:22:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:22:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:22:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:22:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:22:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:22:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.218908, avg_loss=0.655473, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:22:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:22:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:22:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:22:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:22:34 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:22:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:22:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:22:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:22:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=137.275620, avg_loss=0.686378, seen=200, correct=107, accuracy=0.535000
2025-09-14 05:22:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:22:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:22:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:22:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:22:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:22:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:22:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.908207, avg_loss=0.647705, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:22:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:22:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:22:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:22:59 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:22:59 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:22:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:22:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:22:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:23:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.296173, avg_loss=0.671481, seen=200, correct=114, accuracy=0.570000
2025-09-14 05:23:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:23:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:23:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:23:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:23:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.545221, avg_loss=0.663631, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:23:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:23:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:23:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:23:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:23:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:23:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:23:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.736557, avg_loss=0.668683, seen=200, correct=124, accuracy=0.620000
2025-09-14 05:23:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:23:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:23:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:23:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:23:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.126623, avg_loss=0.678166, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:23:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:23:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:23:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:23:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:23:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:23:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:23:37 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #37', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:23:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:23:37 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 4 for training...
2025-09-14 05:23:38 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:23:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:23:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:23:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:23:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.796032, avg_loss=0.617821, seen=11, correct=8, accuracy=0.727273
2025-09-14 05:23:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:23:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:23:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:23:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:23:41 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.753204, avg_loss=0.768830, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:23:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:23:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:23:42 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:23:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=56, total=224)
2025-09-14 05:23:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:43 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:23:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:43 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=28, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:23:52 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:23:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:23:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:23:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:23:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.559563, avg_loss=0.596324, seen=11, correct=9, accuracy=0.818182
2025-09-14 05:23:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:23:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:23:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:23:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:23:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:23:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.061108, avg_loss=0.751528, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:23:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:23:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:23:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:23:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:24:08 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:24:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:24:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:24:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:24:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:24:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.283319, avg_loss=0.662120, seen=11, correct=8, accuracy=0.727273
2025-09-14 05:24:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:24:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:24:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:24:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:24:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:24:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:24:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.212000, avg_loss=0.730300, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:24:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:24:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:24:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:24:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:24:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:24:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:24:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:24:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:24:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.722692, avg_loss=0.702063, seen=11, correct=6, accuracy=0.545455
2025-09-14 05:24:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:24:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:24:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:24:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:24:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:24:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:24:28 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.793644, avg_loss=0.719841, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:24:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:24:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:24:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:24:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:24:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:24:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:24:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:24:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:24:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.443001, avg_loss=0.676636, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:24:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:24:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:24:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:24:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:24:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:24:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:24:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.972132, avg_loss=0.724303, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:24:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:24:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:24:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:24:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:24:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:24:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:24:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:24:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:24:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.263885, avg_loss=0.660353, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:24:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:24:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:24:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:24:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:24:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:24:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:25:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:25:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.096691, avg_loss=0.727417, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:25:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:25:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:25:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:25:11 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:25:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:25:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:25:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:25:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.240244, avg_loss=0.658204, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:25:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:25:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:25:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:25:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:25:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:25:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:25:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.050556, avg_loss=0.726264, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:25:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:25:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:25:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:25:28 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:25:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:25:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:25:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:25:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:25:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.410248, avg_loss=0.673659, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:25:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:25:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:25:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:25:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:25:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:25:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:25:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.518270, avg_loss=0.737957, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:25:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:25:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:25:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:25:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:25:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:25:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:25:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:25:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:25:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.221048, avg_loss=0.656459, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:25:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:25:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:25:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:25:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:25:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:25:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:25:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.406366, avg_loss=0.735159, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:25:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:25:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:25:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:25:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:25:58 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:25:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:25:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:25:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:25:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:25:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.080948, avg_loss=0.643723, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:25:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:25:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:26:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:26:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:26:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:26:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.297699, avg_loss=0.732442, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:26:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:26:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:26:15 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:26:15 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:26:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:26:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:26:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:26:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.207488, avg_loss=0.655226, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:26:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:26:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:26:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:26:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:26:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:26:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.603302, avg_loss=0.740083, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:26:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:26:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:26:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:26:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:26:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:26:22 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:26:22 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #22', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:26:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:26:22 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:26:23 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:26:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:26:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:26:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=37, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:26:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:26:27 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.256042, avg_loss=0.676635, seen=126, correct=70, accuracy=0.555556
2025-09-14 05:26:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:26:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:26:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:26:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:26:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:26:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.611897, avg_loss=0.640297, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:26:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:26:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:26:32 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:26:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=600, total=2399)
2025-09-14 05:26:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:32 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:26:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:26:32 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=300, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:26:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:26:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:26:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:26:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:26:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:26:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=84.391350, avg_loss=0.669773, seen=126, correct=76, accuracy=0.603175
2025-09-14 05:26:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:26:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:26:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:26:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:26:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:26:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.976498, avg_loss=0.624412, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:26:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:26:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:26:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:26:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:27:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:27:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:27:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:27:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:27:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:27:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.969543, avg_loss=0.682298, seen=126, correct=72, accuracy=0.571429
2025-09-14 05:27:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:27:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:27:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:27:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:27:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:27:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:27:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.788782, avg_loss=0.619720, seen=40, correct=29, accuracy=0.725000
2025-09-14 05:27:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:27:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:27:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:27:20 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:27:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:27:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:27:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:27:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:27:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.561722, avg_loss=0.679061, seen=126, correct=72, accuracy=0.571429
2025-09-14 05:27:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:27:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:27:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:27:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:27:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:27:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:27:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.708530, avg_loss=0.617713, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:27:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:27:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:27:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:27:40 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:27:40 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:27:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:27:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:27:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:27:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=84.108238, avg_loss=0.667526, seen=126, correct=77, accuracy=0.611111
2025-09-14 05:27:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:27:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:27:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:27:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:27:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:27:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:27:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.828604, avg_loss=0.620715, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:27:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:27:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:27:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:27:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:28:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:28:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:28:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:28:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:28:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:28:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.705566, avg_loss=0.680203, seen=126, correct=76, accuracy=0.603175
2025-09-14 05:28:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:28:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:28:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:28:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:28:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:28:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:28:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.356070, avg_loss=0.633902, seen=40, correct=27, accuracy=0.675000
2025-09-14 05:28:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:28:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:28:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:28:18 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:28:18 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:28:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:28:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:28:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:28:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=86.525459, avg_loss=0.686710, seen=126, correct=61, accuracy=0.484127
2025-09-14 05:28:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:28:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:28:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:28:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:28:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:28:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:28:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.591694, avg_loss=0.639792, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:28:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:28:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:28:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:28:37 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:28:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:28:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:28:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:28:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:28:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.111755, avg_loss=0.691363, seen=126, correct=63, accuracy=0.500000
2025-09-14 05:28:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:28:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:28:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:28:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:28:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:28:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:28:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.555656, avg_loss=0.638891, seen=40, correct=27, accuracy=0.675000
2025-09-14 05:28:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:28:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:28:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:28:56 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:28:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:28:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:28:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:28:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:29:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.919228, avg_loss=0.697772, seen=126, correct=65, accuracy=0.515873
2025-09-14 05:29:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:29:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:29:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:29:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:29:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.466379, avg_loss=0.636659, seen=40, correct=27, accuracy=0.675000
2025-09-14 05:29:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:29:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:29:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:29:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:29:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:29:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:29:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=87.103378, avg_loss=0.691297, seen=126, correct=70, accuracy=0.555556
2025-09-14 05:29:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:29:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:29:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:29:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:29:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.051840, avg_loss=0.626296, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:29:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:29:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:29:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:29:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:29:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:29:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:29:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.523010, avg_loss=0.678754, seen=126, correct=75, accuracy=0.595238
2025-09-14 05:29:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:29:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:29:45 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:29:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:29:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.608999, avg_loss=0.615225, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:29:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:29:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:29:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:29:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:29:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:29:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:29:49 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #20', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:29:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:29:49 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 05:29:50 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:29:50 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:29:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:29:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=4, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:29:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=39.924397, avg_loss=0.633721, seen=63, correct=43, accuracy=0.682540
2025-09-14 05:29:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:29:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:29:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:29:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:29:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.562874, avg_loss=0.564072, seen=40, correct=29, accuracy=0.725000
2025-09-14 05:29:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:29:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:29:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:29:56 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:29:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=303, total=1209)
2025-09-14 05:29:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:29:56 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:29:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:29:56 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=152, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:30:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:30:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:30:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:30:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:30:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:30:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.681423, avg_loss=0.693356, seen=63, correct=41, accuracy=0.650794
2025-09-14 05:30:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:30:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:30:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:30:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:30:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:30:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:30:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.149994, avg_loss=0.553750, seen=40, correct=31, accuracy=0.775000
2025-09-14 05:30:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:30:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:30:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:30:24 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:30:24 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:30:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:30:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:30:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:30:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.091927, avg_loss=0.683999, seen=63, correct=38, accuracy=0.603175
2025-09-14 05:30:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:30:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:30:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:30:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:30:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:30:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:30:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.419628, avg_loss=0.560491, seen=40, correct=30, accuracy=0.750000
2025-09-14 05:30:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:30:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:30:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:30:39 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:30:39 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:30:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:30:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:30:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:30:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.823868, avg_loss=0.679744, seen=63, correct=40, accuracy=0.634921
2025-09-14 05:30:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:30:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:30:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:30:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:30:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:30:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:30:46 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.859589, avg_loss=0.571490, seen=40, correct=29, accuracy=0.725000
2025-09-14 05:30:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:30:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:30:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:30:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:30:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:30:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:30:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:30:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:30:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:30:59 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=44.725960, avg_loss=0.709936, seen=63, correct=37, accuracy=0.587302
2025-09-14 05:30:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:30:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:31:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:31:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:31:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:31:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:31:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.932678, avg_loss=0.573317, seen=40, correct=29, accuracy=0.725000
2025-09-14 05:31:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:31:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:31:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:31:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:31:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:31:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:31:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:31:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:31:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=44.563496, avg_loss=0.707357, seen=63, correct=39, accuracy=0.619048
2025-09-14 05:31:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:31:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:31:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:31:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:31:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:31:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:31:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=22.379465, avg_loss=0.559487, seen=40, correct=30, accuracy=0.750000
2025-09-14 05:31:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:31:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:31:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:31:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:31:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:31:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:31:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:31:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:31:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=42.245323, avg_loss=0.670561, seen=63, correct=41, accuracy=0.650794
2025-09-14 05:31:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:31:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:31:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:31:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:31:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:31:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:31:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=21.520775, avg_loss=0.538019, seen=40, correct=31, accuracy=0.775000
2025-09-14 05:31:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:31:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:31:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:31:46 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:31:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:31:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:31:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:31:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:31:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=40.872498, avg_loss=0.648770, seen=63, correct=41, accuracy=0.650794
2025-09-14 05:31:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:31:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:31:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:31:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:31:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:31:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:31:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=21.058073, avg_loss=0.526452, seen=40, correct=30, accuracy=0.750000
2025-09-14 05:31:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:31:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:31:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:31:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:32:05 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:32:05 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:32:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:32:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:32:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=40.017689, avg_loss=0.635201, seen=63, correct=41, accuracy=0.650794
2025-09-14 05:32:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:32:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:32:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:32:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:32:12 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=20.769754, avg_loss=0.519244, seen=40, correct=34, accuracy=0.850000
2025-09-14 05:32:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:32:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:32:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:32:22 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:32:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:32:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:32:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=41.345554, avg_loss=0.656279, seen=63, correct=41, accuracy=0.650794
2025-09-14 05:32:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:32:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:32:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:32:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:27 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:32:27 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=20.654869, avg_loss=0.516372, seen=40, correct=32, accuracy=0.800000
2025-09-14 05:32:27 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:32:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:32:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:32:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:32:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:32:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:32:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=43.248238, avg_loss=0.686480, seen=63, correct=40, accuracy=0.634921
2025-09-14 05:32:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:32:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:42 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:32:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:32:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:32:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=21.830479, avg_loss=0.545762, seen=40, correct=29, accuracy=0.725000
2025-09-14 05:32:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:32:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:32:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:32:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:32:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2320MB allocated=2217MB
2025-09-14 05:32:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:32:45 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #10', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:32:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:32:45 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:32:46 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:32:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:32:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:32:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:32:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.944061, avg_loss=0.649720, seen=200, correct=121, accuracy=0.605000
2025-09-14 05:32:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:32:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:32:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:32:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:32:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.591911, avg_loss=0.664798, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:32:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:32:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:32:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:32:56 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:32:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=1002, total=4005)
2025-09-14 05:32:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:32:56 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:32:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:32:56 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=501, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:33:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:33:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:33:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:33:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:33:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:33:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.446777, avg_loss=0.657234, seen=200, correct=125, accuracy=0.625000
2025-09-14 05:33:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:33:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:33:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:33:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:33:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:33:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:33:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.579962, avg_loss=0.664499, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:33:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:33:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:33:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:33:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:33:31 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:33:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:33:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:33:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:33:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=130.277313, avg_loss=0.651387, seen=200, correct=118, accuracy=0.590000
2025-09-14 05:33:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:33:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:33:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:33:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:33:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:33:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:33:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.812675, avg_loss=0.645317, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:33:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:33:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:33:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:33:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:33:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:33:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:33:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:33:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:34:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:34:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.588531, avg_loss=0.647943, seen=200, correct=123, accuracy=0.615000
2025-09-14 05:34:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:34:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:34:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:34:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:34:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:34:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:34:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.503315, avg_loss=0.637583, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:34:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:34:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:34:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:34:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:34:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:34:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:34:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:34:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:34:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.964050, avg_loss=0.649820, seen=200, correct=124, accuracy=0.620000
2025-09-14 05:34:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:34:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:34:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:34:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:34:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:34:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:34:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.324869, avg_loss=0.658122, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:34:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:34:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:34:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:34:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:34:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:34:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:34:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:34:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:34:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.932922, avg_loss=0.649665, seen=200, correct=130, accuracy=0.650000
2025-09-14 05:34:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:34:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:34:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:34:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:34:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:34:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:34:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.602692, avg_loss=0.665067, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:34:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:34:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:34:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:34:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:35:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:35:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:35:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:35:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:35:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:35:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.883362, avg_loss=0.644417, seen=200, correct=127, accuracy=0.635000
2025-09-14 05:35:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:35:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:35:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:35:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:35:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:35:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:35:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.327415, avg_loss=0.658185, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:35:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:35:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:35:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:35:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:35:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:35:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:35:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:35:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:35:37 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.651001, avg_loss=0.643255, seen=200, correct=125, accuracy=0.625000
2025-09-14 05:35:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:35:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:35:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:35:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:35:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:35:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:35:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.715719, avg_loss=0.667893, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:35:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:35:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:35:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:35:53 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:35:53 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:35:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:35:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:35:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:36:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:36:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.672028, avg_loss=0.643360, seen=200, correct=120, accuracy=0.600000
2025-09-14 05:36:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:36:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:36:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:36:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:36:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:36:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:36:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.822758, avg_loss=0.670569, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:36:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:36:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:36:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:36:16 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:36:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:36:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:36:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:36:24 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:36:24 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=128.780121, avg_loss=0.643901, seen=200, correct=121, accuracy=0.605000
2025-09-14 05:36:24 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:36:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:36:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:36:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:36:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:36:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:36:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.604368, avg_loss=0.665109, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:36:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:36:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:36:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:36:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:36:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:36:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:36:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:36:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:36:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=131.205002, avg_loss=0.656025, seen=200, correct=117, accuracy=0.585000
2025-09-14 05:36:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:36:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:36:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:36:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:36:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:36:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:36:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.352015, avg_loss=0.683800, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:36:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:36:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:36:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:36:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:36:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:36:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:36:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:36:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:37:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:37:00 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #40', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:37:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:37:00 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:37:01 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:37:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:37:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:37:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=33, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:37:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:37:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.607086, avg_loss=0.688775, seen=133, correct=78, accuracy=0.586466
2025-09-14 05:37:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:37:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:37:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:37:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:37:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:37:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:37:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.860191, avg_loss=0.696505, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:37:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:37:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:37:11 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:37:11 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:37:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=632, total=2527)
2025-09-14 05:37:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:11 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:37:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:37:11 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=316, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:37:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:37:21 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:37:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:37:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:37:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:37:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.808540, avg_loss=0.697809, seen=133, correct=66, accuracy=0.496241
2025-09-14 05:37:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:37:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:28 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:37:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:37:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:37:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:37:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:37:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.999712, avg_loss=0.699993, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:37:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:37:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:37:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:37:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:37:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:37:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:37:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:37:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:37:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.120903, avg_loss=0.692638, seen=133, correct=68, accuracy=0.511278
2025-09-14 05:37:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:37:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:37:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:37:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:37:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:37:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:37:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.207554, avg_loss=0.705189, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:37:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:37:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:37:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:37:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:38:04 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:38:04 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:38:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:38:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:38:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:38:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.962067, avg_loss=0.691444, seen=133, correct=72, accuracy=0.541353
2025-09-14 05:38:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:38:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:38:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:38:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:38:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:38:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:38:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.074203, avg_loss=0.701855, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:38:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:38:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:38:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:38:27 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:38:27 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:38:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:38:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:38:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:38:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=92.423820, avg_loss=0.694916, seen=133, correct=75, accuracy=0.563910
2025-09-14 05:38:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:38:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:38:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:38:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:38:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:38:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:38:36 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.544765, avg_loss=0.688619, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:38:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:38:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:38:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:38:48 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:38:48 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:38:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:38:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:38:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:38:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.548889, avg_loss=0.688338, seen=133, correct=77, accuracy=0.578947
2025-09-14 05:38:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:38:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:38:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:38:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:38:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:38:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:38:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.589153, avg_loss=0.714729, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:38:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:38:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:38:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:39:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:39:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:39:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:39:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:39:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:39:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:39:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.344185, avg_loss=0.686798, seen=133, correct=73, accuracy=0.548872
2025-09-14 05:39:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:39:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:39:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:39:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:39:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:39:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:39:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.372192, avg_loss=0.709305, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:39:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:39:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:39:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:39:32 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:39:32 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:39:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:39:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:39:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:39:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=94.083054, avg_loss=0.707391, seen=133, correct=78, accuracy=0.586466
2025-09-14 05:39:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:39:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:39:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:39:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:39:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:39:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:39:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.159740, avg_loss=0.678994, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:39:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:39:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:39:44 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:39:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:39:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:39:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:39:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:39:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:40:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=94.543228, avg_loss=0.710851, seen=133, correct=74, accuracy=0.556391
2025-09-14 05:40:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:40:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:40:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:40:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:40:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.981503, avg_loss=0.674538, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:40:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:40:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:40:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:40:17 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:40:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:40:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:40:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=93.310349, avg_loss=0.701582, seen=133, correct=70, accuracy=0.526316
2025-09-14 05:40:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:40:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:40:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:40:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:40:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.290333, avg_loss=0.707258, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:40:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:40:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:40:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:40:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:40:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 05:40:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:40:43 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.923393, avg_loss=0.691153, seen=133, correct=75, accuracy=0.563910
2025-09-14 05:40:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:40:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:40:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:40:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:40:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.513525, avg_loss=0.712838, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:40:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:40:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:40:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:40:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:40:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:40:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:40:50 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #50', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:40:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:40:50 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 4 for training...
2025-09-14 05:40:52 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:40:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:40:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=4, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:40:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.269842, avg_loss=0.660895, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:40:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:40:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:40:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:40:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:40:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.471088, avg_loss=0.761777, seen=40, correct=17, accuracy=0.425000
2025-09-14 05:40:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:40:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:40:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:40:57 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:40:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=54, total=213)
2025-09-14 05:40:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:40:57 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:40:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:40:57 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=27, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:41:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:41:07 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:41:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:41:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:41:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:41:09 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.960158, avg_loss=0.723651, seen=11, correct=5, accuracy=0.454545
2025-09-14 05:41:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:41:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:41:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:41:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:41:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:41:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:41:13 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.221645, avg_loss=0.680541, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:41:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:41:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:41:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:41:25 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:41:25 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:41:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:41:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:41:26 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=8.152645, avg_loss=0.741150, seen=11, correct=5, accuracy=0.454545
2025-09-14 05:41:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:41:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:41:29 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:41:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:41:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:41:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:41:31 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.782871, avg_loss=0.669572, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:41:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:41:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:41:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:41:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:41:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:41:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:41:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:41:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:41:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.737327, avg_loss=0.703393, seen=11, correct=6, accuracy=0.545455
2025-09-14 05:41:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:41:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:41:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:41:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:41:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:41:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:41:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.006737, avg_loss=0.675168, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:41:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:41:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:41:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:41:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:42:02 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:42:02 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:42:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:42:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:42:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:42:03 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.647615, avg_loss=0.695238, seen=11, correct=6, accuracy=0.545455
2025-09-14 05:42:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:42:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:42:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:42:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:42:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:42:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:42:07 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.627058, avg_loss=0.690676, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:42:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:42:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:42:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:42:19 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:42:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:42:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:42:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:42:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.570191, avg_loss=0.688199, seen=11, correct=6, accuracy=0.545455
2025-09-14 05:42:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:42:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:42:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:42:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:42:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:42:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:42:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.671574, avg_loss=0.691789, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:42:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:42:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:42:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:42:38 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:42:38 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:42:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:42:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:42:38 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.431047, avg_loss=0.675550, seen=11, correct=5, accuracy=0.454545
2025-09-14 05:42:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:42:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:42:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:42:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:42:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:42:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:42:42 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.774727, avg_loss=0.694368, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:42:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:42:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:42:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:42:54 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:42:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:42:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:42:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:42:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:42:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.406570, avg_loss=0.673325, seen=11, correct=6, accuracy=0.545455
2025-09-14 05:42:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:42:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:42:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:42:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:42:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:42:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:42:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:42:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.806503, avg_loss=0.695163, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:42:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:42:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:43:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:43:10 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:43:11 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:43:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:43:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:43:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:43:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.454865, avg_loss=0.677715, seen=11, correct=4, accuracy=0.363636
2025-09-14 05:43:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:43:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:43:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:43:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:43:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:43:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:43:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.967691, avg_loss=0.699192, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:43:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:43:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:43:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:43:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:43:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:43:29 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:43:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:43:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:43:30 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.299694, avg_loss=0.663609, seen=11, correct=5, accuracy=0.454545
2025-09-14 05:43:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:43:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:43:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:43:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:43:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:43:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:43:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.560909, avg_loss=0.714023, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:43:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:43:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:43:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:43:44 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:43:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:43:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:43:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:45 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:43:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:43:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.286865, avg_loss=0.662442, seen=11, correct=4, accuracy=0.363636
2025-09-14 05:43:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:43:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:43:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:43:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:43:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:43:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:43:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.618423, avg_loss=0.715461, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:43:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:43:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:43:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:43:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:43:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:43:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:43:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2324MB allocated=2217MB
2025-09-14 05:43:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:43:53 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #4', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:43:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:43:53 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 1 for training...
2025-09-14 05:43:54 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:43:54 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:43:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:43:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=27, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:43:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:43:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.267143, avg_loss=0.659364, seen=146, correct=87, accuracy=0.595890
2025-09-14 05:43:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:43:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:43:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:44:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:44:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:44:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:44:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:44:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.258568, avg_loss=0.656464, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:44:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:44:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:44:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:44:03 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:44:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=699, total=2793)
2025-09-14 05:44:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:04 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:44:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:44:04 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=350, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:44:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:44:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:44:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:44:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:44:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:44:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.623505, avg_loss=0.661805, seen=146, correct=84, accuracy=0.575342
2025-09-14 05:44:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:44:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:44:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:44:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:44:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:44:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:44:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.014797, avg_loss=0.675370, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:44:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:44:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:25 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:44:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:44:36 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:44:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:44:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:44:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:44:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:44:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=97.924049, avg_loss=0.670713, seen=146, correct=80, accuracy=0.547945
2025-09-14 05:44:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:44:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:44:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:44:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:44:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:44:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:44:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.392235, avg_loss=0.684806, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:44:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:44:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:44:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:44:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:45:01 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:45:01 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:45:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:45:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:45:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:45:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=97.835953, avg_loss=0.670109, seen=146, correct=81, accuracy=0.554795
2025-09-14 05:45:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:45:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:45:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:45:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:45:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:45:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:45:11 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.708025, avg_loss=0.667701, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:45:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:45:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:45:14 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:45:22 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:45:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:45:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:45:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:45:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:45:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=97.160988, avg_loss=0.665486, seen=146, correct=79, accuracy=0.541096
2025-09-14 05:45:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:45:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:45:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:45:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:45:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:45:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:45:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.143578, avg_loss=0.653589, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:45:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:45:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:45:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:45:45 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:45:46 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:45:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:45:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:45:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:45:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.401398, avg_loss=0.660284, seen=146, correct=83, accuracy=0.568493
2025-09-14 05:45:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:45:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:45:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:45:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:45:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:45:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:45:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.207022, avg_loss=0.655176, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:45:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:45:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:45:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:45:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:46:09 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:46:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:46:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:46:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:46:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:46:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=94.400093, avg_loss=0.646576, seen=146, correct=87, accuracy=0.595890
2025-09-14 05:46:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:46:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:46:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:46:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:46:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:19 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:46:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:46:20 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.982510, avg_loss=0.649563, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:46:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:46:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:46:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:46:33 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:46:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:46:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:46:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:46:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:46:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.140091, avg_loss=0.637946, seen=146, correct=88, accuracy=0.602740
2025-09-14 05:46:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:46:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:46:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:46:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:46:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:46:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:46:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.691456, avg_loss=0.667286, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:46:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:46:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:46:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:46:57 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:46:57 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:46:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:46:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:46:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:47:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:47:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.508148, avg_loss=0.640467, seen=146, correct=90, accuracy=0.616438
2025-09-14 05:47:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:47:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:47:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:47:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:47:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:47:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.563087, avg_loss=0.664077, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:47:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:47:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:47:08 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:47:17 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:47:19 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:47:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:47:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:47:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:47:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=93.828979, avg_loss=0.642664, seen=146, correct=90, accuracy=0.616438
2025-09-14 05:47:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:47:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:47:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:47:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:47:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:47:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:47:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.508955, avg_loss=0.662724, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:47:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:47:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:47:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:47:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:47:43 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:47:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:47:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:47:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:47:49 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=94.046814, avg_loss=0.644156, seen=146, correct=86, accuracy=0.589041
2025-09-14 05:47:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:47:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:50 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:47:51 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:47:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:47:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:47:51 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.725681, avg_loss=0.668142, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:47:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:47:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:47:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:47:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:47:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:47:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:47:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:47:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:47:55 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #1', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:47:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:47:55 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 2 for training...
2025-09-14 05:47:56 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:47:56 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:47:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:47:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=8, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:47:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:47:58 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.206394, avg_loss=0.678400, seen=46, correct=25, accuracy=0.543478
2025-09-14 05:47:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:47:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:47:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:48:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:48:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:48:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:48:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:48:02 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.752172, avg_loss=0.718804, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:48:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:48:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:48:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:48:03 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:48:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=220, total=880)
2025-09-14 05:48:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:03 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:48:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:48:03 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=110, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:48:13 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:48:13 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:48:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:48:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:48:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:48:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=30.997219, avg_loss=0.673853, seen=46, correct=26, accuracy=0.565217
2025-09-14 05:48:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:48:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:48:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:48:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:48:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:48:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:48:19 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.777700, avg_loss=0.669443, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:48:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:48:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:48:22 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:48:31 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:48:33 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:48:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:48:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:48:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:48:35 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.481855, avg_loss=0.684388, seen=46, correct=24, accuracy=0.521739
2025-09-14 05:48:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:48:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:48:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:48:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:48:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:48:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:48:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.371746, avg_loss=0.684294, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:48:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:48:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:48:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:48:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:48:52 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:48:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:48:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:48:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:48:53 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.821234, avg_loss=0.691766, seen=46, correct=25, accuracy=0.543478
2025-09-14 05:48:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:48:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:48:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:48:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:48:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:48:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:48:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.693371, avg_loss=0.667334, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:48:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:48:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:48:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:48:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:49:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:49:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:49:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:49:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:49:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:49:10 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=32.216148, avg_loss=0.700351, seen=46, correct=22, accuracy=0.478261
2025-09-14 05:49:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:49:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:49:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:49:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:49:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:49:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:49:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.947561, avg_loss=0.698689, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:49:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:49:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:49:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:49:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:49:26 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:49:26 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:49:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:26 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:49:28 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:49:28 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.184662, avg_loss=0.677927, seen=46, correct=20, accuracy=0.434783
2025-09-14 05:49:28 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:49:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:29 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:49:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:49:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:49:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:49:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:49:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.973898, avg_loss=0.699347, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:49:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:49:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:49:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:49:43 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:49:44 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:49:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:49:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:49:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:49:46 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.287109, avg_loss=0.680155, seen=46, correct=21, accuracy=0.456522
2025-09-14 05:49:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:49:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:49:48 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:49:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:49:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:49:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:49:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.656109, avg_loss=0.691403, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:49:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:49:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:49:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:49:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:50:03 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:50:03 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:50:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:50:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:50:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:50:06 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.177511, avg_loss=0.677772, seen=46, correct=24, accuracy=0.521739
2025-09-14 05:50:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:50:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:50:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:50:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:50:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:50:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:50:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.040108, avg_loss=0.676003, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:50:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:50:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:50:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:50:21 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:50:23 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:50:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:50:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:50:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:50:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.715450, avg_loss=0.689466, seen=46, correct=25, accuracy=0.543478
2025-09-14 05:50:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:50:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:50:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:50:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:50:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:27 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:50:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:50:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.688801, avg_loss=0.667220, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:50:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:50:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:50:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:50:41 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:50:41 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:50:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:50:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:50:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:50:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.862989, avg_loss=0.692674, seen=46, correct=23, accuracy=0.500000
2025-09-14 05:50:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:50:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:50:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:50:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:50:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:50:49 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:50:49 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.827141, avg_loss=0.670679, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:50:49 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:50:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:50:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:50:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:50:58 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:51:00 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:51:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 05:51:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 05:51:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.113888, avg_loss=0.676389, seen=46, correct=24, accuracy=0.521739
2025-09-14 05:51:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:51:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:51:04 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:51:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:51:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.659271, avg_loss=0.691482, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:51:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:51:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:51:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:51:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:51:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2306MB allocated=2217MB
2025-09-14 05:51:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:51:09 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #48', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:51:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:51:09 (federatedscope.llm.llm_local.client:178) INFO: Activate the adapter 0 for training...
2025-09-14 05:51:10 (federatedscope.llm.trainer.trainer:385) INFO: [mid-eval] every_n_train_steps=10
2025-09-14 05:51:10 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=0, splits=['val', 'test']
2025-09-14 05:51:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:51:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=50, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:51:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.472004, avg_loss=0.634720, seen=100, correct=62, accuracy=0.620000
2025-09-14 05:51:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:51:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:51:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:51:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=20, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:51:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.892923, avg_loss=0.747323, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:51:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:51:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:51:17 (federatedscope.llm.trainer.trainer:408) INFO: [Stateless LR Controller] In Round #0, planning to set LR to 1.00e-05
2025-09-14 05:51:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'train' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=476, total=1901)
2025-09-14 05:51:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-train][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:17 (federatedscope.llm.trainer.trainer:787) INFO: Successfully applied new LR 1.00e-05 to the optimizer.
2025-09-14 05:51:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=100, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:18 (federatedscope.llm.trainer.trainer:522) INFO: [run-batch-setup] split=train, len(loader)=238, num_batches(ctx)=100, grad_accum_step=2, will_run_step(loops)=200
2025-09-14 05:51:26 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=10
2025-09-14 05:51:28 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=10, splits=['val', 'test']
2025-09-14 05:51:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:51:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:51:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=71.597153, avg_loss=0.715972, seen=100, correct=58, accuracy=0.580000
2025-09-14 05:51:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:51:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:51:34 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:51:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:51:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.995529, avg_loss=0.724888, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:51:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:51:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:51:47 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=20
2025-09-14 05:51:47 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=20, splits=['val', 'test']
2025-09-14 05:51:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:51:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:51:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=72.100517, avg_loss=0.721005, seen=100, correct=55, accuracy=0.550000
2025-09-14 05:51:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:51:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:51:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:51:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:51:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:51:54 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.356348, avg_loss=0.708909, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:51:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:51:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:51:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:51:56 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:52:06 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=30
2025-09-14 05:52:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=30, splits=['val', 'test']
2025-09-14 05:52:08 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:52:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:52:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:52:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=71.999954, avg_loss=0.720000, seen=100, correct=58, accuracy=0.580000
2025-09-14 05:52:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:52:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:52:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:52:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:52:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:52:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:52:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.220863, avg_loss=0.730522, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:52:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:52:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:52:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:52:29 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=40
2025-09-14 05:52:29 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=40, splits=['val', 'test']
2025-09-14 05:52:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:52:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:52:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:52:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=67.853279, avg_loss=0.678533, seen=100, correct=58, accuracy=0.580000
2025-09-14 05:52:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:52:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:52:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:52:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:52:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:52:37 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:52:37 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.441689, avg_loss=0.761042, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:52:37 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:52:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:39 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:52:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:52:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=50
2025-09-14 05:52:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=50, splits=['val', 'test']
2025-09-14 05:52:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:52:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:52:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:52:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=64.568520, avg_loss=0.645685, seen=100, correct=64, accuracy=0.640000
2025-09-14 05:52:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:52:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:52:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:52:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:52:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:52:58 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:52:58 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.197248, avg_loss=0.729931, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:52:58 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:52:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:52:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:53:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:53:07 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=60
2025-09-14 05:53:08 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=60, splits=['val', 'test']
2025-09-14 05:53:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:53:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:53:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:53:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=63.892147, avg_loss=0.638921, seen=100, correct=68, accuracy=0.680000
2025-09-14 05:53:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:53:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:53:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:53:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:53:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:53:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:53:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.558847, avg_loss=0.713971, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:53:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:53:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:53:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:53:30 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=70
2025-09-14 05:53:30 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=70, splits=['val', 'test']
2025-09-14 05:53:30 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:53:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:30 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:53:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:53:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=65.368683, avg_loss=0.653687, seen=100, correct=62, accuracy=0.620000
2025-09-14 05:53:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:53:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:53:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:53:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:53:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:53:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:53:39 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.276501, avg_loss=0.706913, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:53:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:53:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:53:40 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:53:51 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=80
2025-09-14 05:53:51 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=80, splits=['val', 'test']
2025-09-14 05:53:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:53:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:53:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:53:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=69.276764, avg_loss=0.692768, seen=100, correct=57, accuracy=0.570000
2025-09-14 05:53:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:53:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:53:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:53:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:53:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:53:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:54:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:54:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.157761, avg_loss=0.703944, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:54:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:54:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:54:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:54:14 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=90
2025-09-14 05:54:14 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=90, splits=['val', 'test']
2025-09-14 05:54:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:54:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:54:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:54:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=70.048874, avg_loss=0.700489, seen=100, correct=55, accuracy=0.550000
2025-09-14 05:54:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:54:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:21 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:54:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:54:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:54:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:54:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:54:23 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.272543, avg_loss=0.731814, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:54:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:54:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:54:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:54:35 (federatedscope.llm.trainer.trainer:1061) INFO: [MID-EVAL-TRIGGER] (inline) at optimizer_step=100
2025-09-14 05:54:36 (federatedscope.llm.trainer.trainer:1422) INFO: [mid-eval] start: is_main=True, step=100, splits=['val', 'test']
2025-09-14 05:54:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 05:54:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:54:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 05:54:40 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=65.630035, avg_loss=0.656300, seen=100, correct=58, accuracy=0.580000
2025-09-14 05:54:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:54:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:54:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:54:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:54:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:54:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:54:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.172892, avg_loss=0.729322, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:54:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:54:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:54:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:54:46 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=0
2025-09-14 05:54:46 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: []
2025-09-14 05:54:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:47 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:54:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2217MB
2025-09-14 05:54:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'train', 'Rank': '0/4', 'Local': True, 'Results': {}}
2025-09-14 05:54:47 (federatedscope.core.workers.client:244) INFO: {'Role': 'Client #45', 'Round': 0, 'Results_raw': {}}
2025-09-14 05:54:48 (federatedscope.core.workers.server:433) INFO: Server: Training is finished! Starting evaluation.
2025-09-14 05:54:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:54:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=146)
2025-09-14 05:54:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=73, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:54:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 05:54:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=146, loss_sum=96.267143, avg_loss=0.659364, seen=146, correct=87, accuracy=0.595890
2025-09-14 05:54:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:54:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:55 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:54:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:54:55 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 146, 'val_loss': 96.26714324951172, 'val_avg_loss': 0.6593639948596693, 'val_seen': 146, 'val_correct': 87, 'val_acc': 0.5958904109589042}
2025-09-14 05:54:55 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:54:55 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 146, 'val_loss': 96.26714324951172, 'val_avg_loss': 0.6593639948596693, 'val_seen': 146, 'val_correct': 87, 'val_acc': 0.5958904109589042}
2025-09-14 05:54:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 37, 'val_loss': 23.022123217582703, 'val_avg_loss': 0.6222195464211542, 'val_seen': 37, 'val_correct': 24, 'val_acc': 0.6486486486486487}}
2025-09-14 05:54:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 146, 'val_loss': 96.26714324951172, 'val_avg_loss': 0.6593639948596693, 'val_seen': 146, 'val_correct': 87, 'val_acc': 0.5958904109589042}}
2025-09-14 05:54:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:54:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:54:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:54:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.258568, avg_loss=0.656464, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:54:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:54:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:54:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:54:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:54:58 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.258567810058594, 'test_avg_loss': 0.6564641952514648, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 05:54:58 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 146, 'val_loss': 96.26714324951172, 'val_avg_loss': 0.6593639948596693, 'val_seen': 146, 'val_correct': 87, 'val_acc': 0.5958904109589042}
2025-09-14 05:54:58 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.258567810058594, 'test_avg_loss': 0.6564641952514648, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 05:54:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.853905439376831, 'test_avg_loss': 0.5853905439376831, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:54:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #1', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.258567810058594, 'test_avg_loss': 0.6564641952514648, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 05:54:58 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.258567810058594, 'test_avg_loss': 0.6564641952514648, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 146, 'val_loss': 96.26714324951172, 'val_avg_loss': 0.6593639948596693, 'val_seen': 146, 'val_correct': 87, 'val_acc': 0.5958904109589042, 'test_total': 40, 'test_loss': 26.258567810058594, 'test_avg_loss': 0.6564641952514648, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 05:54:58 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:54:59 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:55:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:55:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.420978, avg_loss=0.674634, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:55:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:02 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 7.420977592468262, 'val_avg_loss': 0.6746343265880238, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 05:55:02 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:55:02 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 7.420977592468262, 'val_avg_loss': 0.6746343265880238, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 05:55:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 1.9998215436935425, 'val_avg_loss': 0.6666071812311808, 'val_seen': 3, 'val_correct': 1, 'val_acc': 0.3333333333333333}}
2025-09-14 05:55:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 7.420977592468262, 'val_avg_loss': 0.6746343265880238, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}}
2025-09-14 05:55:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:55:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:55:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.547169, avg_loss=0.638679, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:55:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:05 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.547168731689453, 'test_avg_loss': 0.6386792182922363, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:55:05 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 7.420977592468262, 'val_avg_loss': 0.6746343265880238, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 05:55:05 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.547168731689453, 'test_avg_loss': 0.6386792182922363, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:55:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.838763952255249, 'test_avg_loss': 0.5838763952255249, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 05:55:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #2', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.547168731689453, 'test_avg_loss': 0.6386792182922363, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 05:55:05 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.547168731689453, 'test_avg_loss': 0.6386792182922363, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 11, 'val_loss': 7.420977592468262, 'val_avg_loss': 0.6746343265880238, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364, 'test_total': 40, 'test_loss': 25.547168731689453, 'test_avg_loss': 0.6386792182922363, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:55:05 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:55:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:55:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=9, total=36)
2025-09-14 05:55:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=18, num_train_batch_last_epoch=200, num_train_epoch=6, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:07 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=9
2025-09-14 05:55:07 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=36, loss_sum=25.983858, avg_loss=0.721774, seen=36, correct=21, accuracy=0.583333
2025-09-14 05:55:07 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 36, 'val_loss': 25.983858108520508, 'val_avg_loss': 0.7217738363477919, 'val_seen': 36, 'val_correct': 21, 'val_acc': 0.5833333333333334}
2025-09-14 05:55:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:55:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 36, 'val_loss': 25.983858108520508, 'val_avg_loss': 0.7217738363477919, 'val_seen': 36, 'val_correct': 21, 'val_acc': 0.5833333333333334}
2025-09-14 05:55:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 9, 'val_loss': 7.363757759332657, 'val_avg_loss': 0.8181953065925174, 'val_seen': 9, 'val_correct': 5, 'val_acc': 0.5555555555555556}}
2025-09-14 05:55:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 36, 'val_loss': 25.983858108520508, 'val_avg_loss': 0.7217738363477919, 'val_seen': 36, 'val_correct': 21, 'val_acc': 0.5833333333333334}}
2025-09-14 05:55:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:55:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:55:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.077810, avg_loss=0.676945, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:55:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:12 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.077810287475586, 'test_avg_loss': 0.6769452571868897, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:55:12 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 36, 'val_loss': 25.983858108520508, 'val_avg_loss': 0.7217738363477919, 'val_seen': 36, 'val_correct': 21, 'val_acc': 0.5833333333333334}
2025-09-14 05:55:12 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.077810287475586, 'test_avg_loss': 0.6769452571868897, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:55:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.041492700576782, 'test_avg_loss': 0.8041492700576782, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 05:55:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #3', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.077810287475586, 'test_avg_loss': 0.6769452571868897, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-14 05:55:12 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.077810287475586, 'test_avg_loss': 0.6769452571868897, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 36, 'val_loss': 25.983858108520508, 'val_avg_loss': 0.7217738363477919, 'val_seen': 36, 'val_correct': 21, 'val_acc': 0.5833333333333334, 'test_total': 40, 'test_loss': 27.077810287475586, 'test_avg_loss': 0.6769452571868897, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:55:12 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:55:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:55:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:55:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:13 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:55:13 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=7.269842, avg_loss=0.660895, seen=11, correct=7, accuracy=0.636364
2025-09-14 05:55:13 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:14 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:15 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:15 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 7.269842147827148, 'val_avg_loss': 0.660894740711559, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 05:55:15 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:55:15 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 7.269842147827148, 'val_avg_loss': 0.660894740711559, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 05:55:15 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 2.6519536077976227, 'val_avg_loss': 0.8839845359325409, 'val_seen': 3, 'val_correct': 1, 'val_acc': 0.3333333333333333}}
2025-09-14 05:55:15 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 7.269842147827148, 'val_avg_loss': 0.660894740711559, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}}
2025-09-14 05:55:15 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:55:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:15 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:55:16 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.471088, avg_loss=0.761777, seen=40, correct=17, accuracy=0.425000
2025-09-14 05:55:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:18 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.471088409423828, 'test_avg_loss': 0.7617772102355957, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 05:55:18 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 7.269842147827148, 'val_avg_loss': 0.660894740711559, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364}
2025-09-14 05:55:18 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.471088409423828, 'test_avg_loss': 0.7617772102355957, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 05:55:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.89966607093811, 'test_avg_loss': 0.889966607093811, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-09-14 05:55:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #4', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.471088409423828, 'test_avg_loss': 0.7617772102355957, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}}
2025-09-14 05:55:18 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.471088409423828, 'test_avg_loss': 0.7617772102355957, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}, metrics={'val_total': 11, 'val_loss': 7.269842147827148, 'val_avg_loss': 0.660894740711559, 'val_seen': 11, 'val_correct': 7, 'val_acc': 0.6363636363636364, 'test_total': 40, 'test_loss': 30.471088409423828, 'test_avg_loss': 0.7617772102355957, 'test_seen': 40, 'test_correct': 17, 'test_acc': 0.425}
2025-09-14 05:55:18 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:55:18 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:55:19 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=4, total=14)
2025-09-14 05:55:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=7, num_train_batch_last_epoch=200, num_train_epoch=15, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:20 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=4
2025-09-14 05:55:20 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=14, loss_sum=9.448443, avg_loss=0.674889, seen=14, correct=9, accuracy=0.642857
2025-09-14 05:55:20 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:21 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:21 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 14, 'val_loss': 9.448443412780762, 'val_avg_loss': 0.6748888151986259, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-14 05:55:21 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:55:21 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 14, 'val_loss': 9.448443412780762, 'val_avg_loss': 0.6748888151986259, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-14 05:55:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 4, 'val_loss': 2.1920331716537476, 'val_avg_loss': 0.5480082929134369, 'val_seen': 4, 'val_correct': 4, 'val_acc': 1.0}}
2025-09-14 05:55:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 14, 'val_loss': 9.448443412780762, 'val_avg_loss': 0.6748888151986259, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}}
2025-09-14 05:55:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:55:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:55:22 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.812710, avg_loss=0.670318, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:55:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:24 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.81270980834961, 'test_avg_loss': 0.6703177452087402, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:55:24 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 14, 'val_loss': 9.448443412780762, 'val_avg_loss': 0.6748888151986259, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429}
2025-09-14 05:55:24 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.81270980834961, 'test_avg_loss': 0.6703177452087402, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:55:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.803637266159058, 'test_avg_loss': 0.6803637266159057, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 05:55:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #5', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.81270980834961, 'test_avg_loss': 0.6703177452087402, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 05:55:24 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.81270980834961, 'test_avg_loss': 0.6703177452087402, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 14, 'val_loss': 9.448443412780762, 'val_avg_loss': 0.6748888151986259, 'val_seen': 14, 'val_correct': 9, 'val_acc': 0.6428571428571429, 'test_total': 40, 'test_loss': 26.81270980834961, 'test_avg_loss': 0.6703177452087402, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:55:24 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:55:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:55:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=134)
2025-09-14 05:55:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:55:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=134, loss_sum=86.282585, avg_loss=0.643900, seen=134, correct=88, accuracy=0.656716
2025-09-14 05:55:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:30 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:30 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 134, 'val_loss': 86.28258514404297, 'val_avg_loss': 0.643899889134649, 'val_seen': 134, 'val_correct': 88, 'val_acc': 0.6567164179104478}
2025-09-14 05:55:30 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:55:30 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 134, 'val_loss': 86.28258514404297, 'val_avg_loss': 0.643899889134649, 'val_seen': 134, 'val_correct': 88, 'val_acc': 0.6567164179104478}
2025-09-14 05:55:30 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 21.944303452968597, 'val_avg_loss': 0.645420689793194, 'val_seen': 34, 'val_correct': 23, 'val_acc': 0.6764705882352942}}
2025-09-14 05:55:30 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 134, 'val_loss': 86.28258514404297, 'val_avg_loss': 0.643899889134649, 'val_seen': 134, 'val_correct': 88, 'val_acc': 0.6567164179104478}}
2025-09-14 05:55:31 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:55:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:31 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:55:32 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.395231, avg_loss=0.734881, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:55:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.395231246948242, 'test_avg_loss': 0.734880781173706, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:55:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 134, 'val_loss': 86.28258514404297, 'val_avg_loss': 0.643899889134649, 'val_seen': 134, 'val_correct': 88, 'val_acc': 0.6567164179104478}
2025-09-14 05:55:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.395231246948242, 'test_avg_loss': 0.734880781173706, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:55:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.331277251243591, 'test_avg_loss': 0.7331277251243591, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 05:55:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #6', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.395231246948242, 'test_avg_loss': 0.734880781173706, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-14 05:55:34 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.395231246948242, 'test_avg_loss': 0.734880781173706, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 134, 'val_loss': 86.28258514404297, 'val_avg_loss': 0.643899889134649, 'val_seen': 134, 'val_correct': 88, 'val_acc': 0.6567164179104478, 'test_total': 40, 'test_loss': 29.395231246948242, 'test_avg_loss': 0.734880781173706, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:55:34 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:55:34 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:55:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=15, total=57)
2025-09-14 05:55:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=29, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=15
2025-09-14 05:55:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=57, loss_sum=36.800781, avg_loss=0.645628, seen=57, correct=36, accuracy=0.631579
2025-09-14 05:55:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:38 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 57, 'val_loss': 36.80078125, 'val_avg_loss': 0.6456277412280702, 'val_seen': 57, 'val_correct': 36, 'val_acc': 0.631578947368421}
2025-09-14 05:55:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:55:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 57, 'val_loss': 36.80078125, 'val_avg_loss': 0.6456277412280702, 'val_seen': 57, 'val_correct': 36, 'val_acc': 0.631578947368421}
2025-09-14 05:55:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 15, 'val_loss': 9.56659710407257, 'val_avg_loss': 0.6377731402715047, 'val_seen': 15, 'val_correct': 10, 'val_acc': 0.6666666666666666}}
2025-09-14 05:55:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 57, 'val_loss': 36.80078125, 'val_avg_loss': 0.6456277412280702, 'val_seen': 57, 'val_correct': 36, 'val_acc': 0.631578947368421}}
2025-09-14 05:55:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:55:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:40 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:55:40 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.477499, avg_loss=0.611937, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:55:40 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:41 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.47749900817871, 'test_avg_loss': 0.6119374752044677, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:55:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 57, 'val_loss': 36.80078125, 'val_avg_loss': 0.6456277412280702, 'val_seen': 57, 'val_correct': 36, 'val_acc': 0.631578947368421}
2025-09-14 05:55:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.47749900817871, 'test_avg_loss': 0.6119374752044677, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:55:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.391500234603882, 'test_avg_loss': 0.5391500234603882, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 05:55:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #7', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.47749900817871, 'test_avg_loss': 0.6119374752044677, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-14 05:55:41 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.47749900817871, 'test_avg_loss': 0.6119374752044677, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 57, 'val_loss': 36.80078125, 'val_avg_loss': 0.6456277412280702, 'val_seen': 57, 'val_correct': 36, 'val_acc': 0.631578947368421, 'test_total': 40, 'test_loss': 24.47749900817871, 'test_avg_loss': 0.6119374752044677, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:55:41 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:55:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:55:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=69)
2025-09-14 05:55:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=35, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:55:45 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=69, loss_sum=49.002586, avg_loss=0.710182, seen=69, correct=40, accuracy=0.579710
2025-09-14 05:55:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:46 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 69, 'val_loss': 49.002586364746094, 'val_avg_loss': 0.7101824110832767, 'val_seen': 69, 'val_correct': 40, 'val_acc': 0.5797101449275363}
2025-09-14 05:55:46 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:55:46 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 69, 'val_loss': 49.002586364746094, 'val_avg_loss': 0.7101824110832767, 'val_seen': 69, 'val_correct': 40, 'val_acc': 0.5797101449275363}
2025-09-14 05:55:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 18, 'val_loss': 11.416254043579102, 'val_avg_loss': 0.6342363357543945, 'val_seen': 18, 'val_correct': 14, 'val_acc': 0.7777777777777778}}
2025-09-14 05:55:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 69, 'val_loss': 49.002586364746094, 'val_avg_loss': 0.7101824110832767, 'val_seen': 69, 'val_correct': 40, 'val_acc': 0.5797101449275363}}
2025-09-14 05:55:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:55:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:55:48 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.282154, avg_loss=0.732054, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:55:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:50 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.282154083251953, 'test_avg_loss': 0.7320538520812988, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:55:50 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 69, 'val_loss': 49.002586364746094, 'val_avg_loss': 0.7101824110832767, 'val_seen': 69, 'val_correct': 40, 'val_acc': 0.5797101449275363}
2025-09-14 05:55:50 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.282154083251953, 'test_avg_loss': 0.7320538520812988, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:55:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.835299015045166, 'test_avg_loss': 0.8835299015045166, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 05:55:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #8', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.282154083251953, 'test_avg_loss': 0.7320538520812988, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 05:55:50 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.282154083251953, 'test_avg_loss': 0.7320538520812988, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 69, 'val_loss': 49.002586364746094, 'val_avg_loss': 0.7101824110832767, 'val_seen': 69, 'val_correct': 40, 'val_acc': 0.5797101449275363, 'test_total': 40, 'test_loss': 29.282154083251953, 'test_avg_loss': 0.7320538520812988, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:55:50 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:55:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:55:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 05:55:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:55:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 05:55:57 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=122.974632, avg_loss=0.654120, seen=188, correct=115, accuracy=0.611702
2025-09-14 05:55:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:55:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:58 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:55:59 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:55:59 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 188, 'val_loss': 122.9746322631836, 'val_avg_loss': 0.6541203843786362, 'val_seen': 188, 'val_correct': 115, 'val_acc': 0.6117021276595744}
2025-09-14 05:55:59 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:55:59 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 188, 'val_loss': 122.9746322631836, 'val_avg_loss': 0.6541203843786362, 'val_seen': 188, 'val_correct': 115, 'val_acc': 0.6117021276595744}
2025-09-14 05:55:59 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 47, 'val_loss': 32.09775388240814, 'val_avg_loss': 0.6829309336682583, 'val_seen': 47, 'val_correct': 28, 'val_acc': 0.5957446808510638}}
2025-09-14 05:55:59 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 188, 'val_loss': 122.9746322631836, 'val_avg_loss': 0.6541203843786362, 'val_seen': 188, 'val_correct': 115, 'val_acc': 0.6117021276595744}}
2025-09-14 05:55:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:55:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:55:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:56:00 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.637611, avg_loss=0.665940, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:56:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:56:02 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.637611389160156, 'test_avg_loss': 0.6659402847290039, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:02 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 188, 'val_loss': 122.9746322631836, 'val_avg_loss': 0.6541203843786362, 'val_seen': 188, 'val_correct': 115, 'val_acc': 0.6117021276595744}
2025-09-14 05:56:02 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.637611389160156, 'test_avg_loss': 0.6659402847290039, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.210943162441254, 'test_avg_loss': 0.6210943162441254, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:56:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #9', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.637611389160156, 'test_avg_loss': 0.6659402847290039, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 05:56:02 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.637611389160156, 'test_avg_loss': 0.6659402847290039, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 188, 'val_loss': 122.9746322631836, 'val_avg_loss': 0.6541203843786362, 'val_seen': 188, 'val_correct': 115, 'val_acc': 0.6117021276595744, 'test_total': 40, 'test_loss': 26.637611389160156, 'test_avg_loss': 0.6659402847290039, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:02 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:56:02 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:56:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=16, total=63)
2025-09-14 05:56:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=32, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=16
2025-09-14 05:56:04 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=63, loss_sum=40.276436, avg_loss=0.639309, seen=63, correct=39, accuracy=0.619048
2025-09-14 05:56:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:56:06 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 63, 'val_loss': 40.27643585205078, 'val_avg_loss': 0.6393085055881076, 'val_seen': 63, 'val_correct': 39, 'val_acc': 0.6190476190476191}
2025-09-14 05:56:06 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:56:06 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 63, 'val_loss': 40.27643585205078, 'val_avg_loss': 0.6393085055881076, 'val_seen': 63, 'val_correct': 39, 'val_acc': 0.6190476190476191}
2025-09-14 05:56:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 16, 'val_loss': 9.977289199829102, 'val_avg_loss': 0.6235805749893188, 'val_seen': 16, 'val_correct': 12, 'val_acc': 0.75}}
2025-09-14 05:56:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 63, 'val_loss': 40.27643585205078, 'val_avg_loss': 0.6393085055881076, 'val_seen': 63, 'val_correct': 39, 'val_acc': 0.6190476190476191}}
2025-09-14 05:56:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:56:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:56:08 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.735237, avg_loss=0.593381, seen=40, correct=27, accuracy=0.675000
2025-09-14 05:56:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:09 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2298MB allocated=2200MB
2025-09-14 05:56:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 23.73523712158203, 'test_avg_loss': 0.5933809280395508, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 05:56:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 63, 'val_loss': 40.27643585205078, 'val_avg_loss': 0.6393085055881076, 'val_seen': 63, 'val_correct': 39, 'val_acc': 0.6190476190476191}
2025-09-14 05:56:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 23.73523712158203, 'test_avg_loss': 0.5933809280395508, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 05:56:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.392060041427612, 'test_avg_loss': 0.6392060041427612, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 05:56:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #10', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 23.73523712158203, 'test_avg_loss': 0.5933809280395508, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-14 05:56:09 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 23.73523712158203, 'test_avg_loss': 0.5933809280395508, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 63, 'val_loss': 40.27643585205078, 'val_avg_loss': 0.6393085055881076, 'val_seen': 63, 'val_correct': 39, 'val_acc': 0.6190476190476191, 'test_total': 40, 'test_loss': 23.73523712158203, 'test_avg_loss': 0.5933809280395508, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 05:56:09 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:56:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:56:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=32)
2025-09-14 05:56:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=16, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 05:56:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=32, loss_sum=19.350216, avg_loss=0.604694, seen=32, correct=22, accuracy=0.687500
2025-09-14 05:56:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2200MB
2025-09-14 05:56:13 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 32, 'val_loss': 19.350215911865234, 'val_avg_loss': 0.6046942472457886, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}
2025-09-14 05:56:13 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:56:13 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 32, 'val_loss': 19.350215911865234, 'val_avg_loss': 0.6046942472457886, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}
2025-09-14 05:56:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 8, 'val_loss': 4.975671887397766, 'val_avg_loss': 0.6219589859247208, 'val_seen': 8, 'val_correct': 4, 'val_acc': 0.5}}
2025-09-14 05:56:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 32, 'val_loss': 19.350215911865234, 'val_avg_loss': 0.6046942472457886, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}}
2025-09-14 05:56:14 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:56:14 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.862164, avg_loss=0.621554, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:56:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2200MB
2025-09-14 05:56:17 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.862163543701172, 'test_avg_loss': 0.6215540885925293, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:17 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 32, 'val_loss': 19.350215911865234, 'val_avg_loss': 0.6046942472457886, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875}
2025-09-14 05:56:17 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.862163543701172, 'test_avg_loss': 0.6215540885925293, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.5328986048698425, 'test_avg_loss': 0.6532898604869842, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 05:56:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #11', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.862163543701172, 'test_avg_loss': 0.6215540885925293, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 05:56:17 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.862163543701172, 'test_avg_loss': 0.6215540885925293, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 32, 'val_loss': 19.350215911865234, 'val_avg_loss': 0.6046942472457886, 'val_seen': 32, 'val_correct': 22, 'val_acc': 0.6875, 'test_total': 40, 'test_loss': 24.862163543701172, 'test_avg_loss': 0.6215540885925293, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:17 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:56:17 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:56:18 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=35, total=137)
2025-09-14 05:56:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=69, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=35
2025-09-14 05:56:21 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=137, loss_sum=90.622406, avg_loss=0.661477, seen=137, correct=84, accuracy=0.613139
2025-09-14 05:56:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2200MB
2025-09-14 05:56:23 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 137, 'val_loss': 90.62240600585938, 'val_avg_loss': 0.6614774161011633, 'val_seen': 137, 'val_correct': 84, 'val_acc': 0.6131386861313869}
2025-09-14 05:56:23 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:56:23 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 137, 'val_loss': 90.62240600585938, 'val_avg_loss': 0.6614774161011633, 'val_seen': 137, 'val_correct': 84, 'val_acc': 0.6131386861313869}
2025-09-14 05:56:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 35, 'val_loss': 24.9022536277771, 'val_avg_loss': 0.7114929607936314, 'val_seen': 35, 'val_correct': 20, 'val_acc': 0.5714285714285714}}
2025-09-14 05:56:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 137, 'val_loss': 90.62240600585938, 'val_avg_loss': 0.6614774161011633, 'val_seen': 137, 'val_correct': 84, 'val_acc': 0.6131386861313869}}
2025-09-14 05:56:23 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:56:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:23 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:56:25 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.351845, avg_loss=0.658796, seen=40, correct=27, accuracy=0.675000
2025-09-14 05:56:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:26 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2296MB allocated=2200MB
2025-09-14 05:56:26 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.351844787597656, 'test_avg_loss': 0.6587961196899415, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 05:56:26 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 137, 'val_loss': 90.62240600585938, 'val_avg_loss': 0.6614774161011633, 'val_seen': 137, 'val_correct': 84, 'val_acc': 0.6131386861313869}
2025-09-14 05:56:26 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.351844787597656, 'test_avg_loss': 0.6587961196899415, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 05:56:26 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 4.99066549539566, 'test_avg_loss': 0.49906654953956603, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 05:56:26 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #12', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.351844787597656, 'test_avg_loss': 0.6587961196899415, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}}
2025-09-14 05:56:26 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.351844787597656, 'test_avg_loss': 0.6587961196899415, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}, metrics={'val_total': 137, 'val_loss': 90.62240600585938, 'val_avg_loss': 0.6614774161011633, 'val_seen': 137, 'val_correct': 84, 'val_acc': 0.6131386861313869, 'test_total': 40, 'test_loss': 26.351844787597656, 'test_avg_loss': 0.6587961196899415, 'test_seen': 40, 'test_correct': 27, 'test_acc': 0.675}
2025-09-14 05:56:26 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:56:26 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:56:27 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=18, total=72)
2025-09-14 05:56:27 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=36, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=18
2025-09-14 05:56:29 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=72, loss_sum=48.656120, avg_loss=0.675779, seen=72, correct=38, accuracy=0.527778
2025-09-14 05:56:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:31 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2200MB
2025-09-14 05:56:32 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 72, 'val_loss': 48.65612030029297, 'val_avg_loss': 0.6757794486151801, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-14 05:56:32 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:56:32 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 72, 'val_loss': 48.65612030029297, 'val_avg_loss': 0.6757794486151801, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-14 05:56:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 18, 'val_loss': 11.543732106685638, 'val_avg_loss': 0.6413184503714243, 'val_seen': 18, 'val_correct': 9, 'val_acc': 0.5}}
2025-09-14 05:56:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 72, 'val_loss': 48.65612030029297, 'val_avg_loss': 0.6757794486151801, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}}
2025-09-14 05:56:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:56:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:56:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.046883, avg_loss=0.651172, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:56:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2294MB allocated=2200MB
2025-09-14 05:56:35 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.04688262939453, 'test_avg_loss': 0.6511720657348633, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:56:35 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 72, 'val_loss': 48.65612030029297, 'val_avg_loss': 0.6757794486151801, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778}
2025-09-14 05:56:35 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.04688262939453, 'test_avg_loss': 0.6511720657348633, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:56:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.8753422498703, 'test_avg_loss': 0.58753422498703, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:56:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #13', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.04688262939453, 'test_avg_loss': 0.6511720657348633, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 05:56:35 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.04688262939453, 'test_avg_loss': 0.6511720657348633, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 72, 'val_loss': 48.65612030029297, 'val_avg_loss': 0.6757794486151801, 'val_seen': 72, 'val_correct': 38, 'val_acc': 0.5277777777777778, 'test_total': 40, 'test_loss': 26.04688262939453, 'test_avg_loss': 0.6511720657348633, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:56:35 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:56:35 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:56:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=40, total=160)
2025-09-14 05:56:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=80, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:39 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=40
2025-09-14 05:56:39 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=160, loss_sum=106.653389, avg_loss=0.666584, seen=160, correct=97, accuracy=0.606250
2025-09-14 05:56:39 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 05:56:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 160, 'val_loss': 106.65338897705078, 'val_avg_loss': 0.6665836811065674, 'val_seen': 160, 'val_correct': 97, 'val_acc': 0.60625}
2025-09-14 05:56:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:56:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 160, 'val_loss': 106.65338897705078, 'val_avg_loss': 0.6665836811065674, 'val_seen': 160, 'val_correct': 97, 'val_acc': 0.60625}
2025-09-14 05:56:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 40, 'val_loss': 25.188797533512115, 'val_avg_loss': 0.6297199383378029, 'val_seen': 40, 'val_correct': 22, 'val_acc': 0.55}}
2025-09-14 05:56:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 160, 'val_loss': 106.65338897705078, 'val_avg_loss': 0.6665836811065674, 'val_seen': 160, 'val_correct': 97, 'val_acc': 0.60625}}
2025-09-14 05:56:41 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:56:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:41 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:43 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:56:43 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=23.658571, avg_loss=0.591464, seen=40, correct=29, accuracy=0.725000
2025-09-14 05:56:43 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:44 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:45 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 05:56:45 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 05:56:45 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 160, 'val_loss': 106.65338897705078, 'val_avg_loss': 0.6665836811065674, 'val_seen': 160, 'val_correct': 97, 'val_acc': 0.60625}
2025-09-14 05:56:45 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 05:56:45 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.6152743101119995, 'test_avg_loss': 0.5615274310112, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-09-14 05:56:45 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #14', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}}
2025-09-14 05:56:45 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}, metrics={'val_total': 160, 'val_loss': 106.65338897705078, 'val_avg_loss': 0.6665836811065674, 'val_seen': 160, 'val_correct': 97, 'val_acc': 0.60625, 'test_total': 40, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_seen': 40, 'test_correct': 29, 'test_acc': 0.725}
2025-09-14 05:56:45 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:56:45 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:56:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:56:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:56:52 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=134.858810, avg_loss=0.674294, seen=200, correct=122, accuracy=0.610000
2025-09-14 05:56:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:53 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:54 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 05:56:54 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 134.8588104248047, 'val_avg_loss': 0.6742940521240235, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-14 05:56:54 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:56:54 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 134.8588104248047, 'val_avg_loss': 0.6742940521240235, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-14 05:56:54 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 31.806277632713318, 'val_avg_loss': 0.6361255526542664, 'val_seen': 50, 'val_correct': 33, 'val_acc': 0.66}}
2025-09-14 05:56:54 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 134.8588104248047, 'val_avg_loss': 0.6742940521240235, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}}
2025-09-14 05:56:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:56:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:56:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:56:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.461243, avg_loss=0.686531, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:56:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:56:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:56:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 05:56:58 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.46124267578125, 'test_avg_loss': 0.6865310668945312, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:58 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 134.8588104248047, 'val_avg_loss': 0.6742940521240235, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61}
2025-09-14 05:56:58 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.46124267578125, 'test_avg_loss': 0.6865310668945312, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.101346969604492, 'test_avg_loss': 0.6101346969604492, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 05:56:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #15', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.46124267578125, 'test_avg_loss': 0.6865310668945312, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 05:56:58 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.46124267578125, 'test_avg_loss': 0.6865310668945312, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 200, 'val_loss': 134.8588104248047, 'val_avg_loss': 0.6742940521240235, 'val_seen': 200, 'val_correct': 122, 'val_acc': 0.61, 'test_total': 40, 'test_loss': 27.46124267578125, 'test_avg_loss': 0.6865310668945312, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:56:58 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:56:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:56:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=136)
2025-09-14 05:56:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:56:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:57:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=136, loss_sum=88.860985, avg_loss=0.653390, seen=136, correct=82, accuracy=0.602941
2025-09-14 05:57:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:04 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 05:57:04 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 136, 'val_loss': 88.8609848022461, 'val_avg_loss': 0.6533895941341624, 'val_seen': 136, 'val_correct': 82, 'val_acc': 0.6029411764705882}
2025-09-14 05:57:04 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:57:04 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 136, 'val_loss': 88.8609848022461, 'val_avg_loss': 0.6533895941341624, 'val_seen': 136, 'val_correct': 82, 'val_acc': 0.6029411764705882}
2025-09-14 05:57:04 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 22.179750204086304, 'val_avg_loss': 0.6523455942378324, 'val_seen': 34, 'val_correct': 25, 'val_acc': 0.7352941176470589}}
2025-09-14 05:57:04 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 136, 'val_loss': 88.8609848022461, 'val_avg_loss': 0.6533895941341624, 'val_seen': 136, 'val_correct': 82, 'val_acc': 0.6029411764705882}}
2025-09-14 05:57:05 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:57:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:05 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:06 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:57:06 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.079582, avg_loss=0.676990, seen=40, correct=22, accuracy=0.550000
2025-09-14 05:57:06 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:07 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 05:57:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.07958221435547, 'test_avg_loss': 0.6769895553588867, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:57:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 136, 'val_loss': 88.8609848022461, 'val_avg_loss': 0.6533895941341624, 'val_seen': 136, 'val_correct': 82, 'val_acc': 0.6029411764705882}
2025-09-14 05:57:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.07958221435547, 'test_avg_loss': 0.6769895553588867, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:57:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.862983345985413, 'test_avg_loss': 0.6862983345985413, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 05:57:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #16', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.07958221435547, 'test_avg_loss': 0.6769895553588867, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 05:57:09 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.07958221435547, 'test_avg_loss': 0.6769895553588867, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 136, 'val_loss': 88.8609848022461, 'val_avg_loss': 0.6533895941341624, 'val_seen': 136, 'val_correct': 82, 'val_acc': 0.6029411764705882, 'test_total': 40, 'test_loss': 27.07958221435547, 'test_avg_loss': 0.6769895553588867, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 05:57:09 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:57:09 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:57:10 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:57:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:10 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:57:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.414230, avg_loss=0.647071, seen=200, correct=124, accuracy=0.620000
2025-09-14 05:57:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 05:57:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 129.4142303466797, 'val_avg_loss': 0.6470711517333985, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-14 05:57:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:57:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 129.4142303466797, 'val_avg_loss': 0.6470711517333985, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-14 05:57:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 30.50022953748703, 'val_avg_loss': 0.6100045907497406, 'val_seen': 50, 'val_correct': 34, 'val_acc': 0.68}}
2025-09-14 05:57:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 129.4142303466797, 'val_avg_loss': 0.6470711517333985, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}}
2025-09-14 05:57:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:57:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:57:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.641350, avg_loss=0.691034, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:57:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:19 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2292MB allocated=2200MB
2025-09-14 05:57:20 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.64134979248047, 'test_avg_loss': 0.6910337448120117, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 05:57:20 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 129.4142303466797, 'val_avg_loss': 0.6470711517333985, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62}
2025-09-14 05:57:20 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.64134979248047, 'test_avg_loss': 0.6910337448120117, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 05:57:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.851858019828796, 'test_avg_loss': 0.7851858019828797, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 05:57:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #17', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.64134979248047, 'test_avg_loss': 0.6910337448120117, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 05:57:20 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.64134979248047, 'test_avg_loss': 0.6910337448120117, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 200, 'val_loss': 129.4142303466797, 'val_avg_loss': 0.6470711517333985, 'val_seen': 200, 'val_correct': 124, 'val_acc': 0.62, 'test_total': 40, 'test_loss': 27.64134979248047, 'test_avg_loss': 0.6910337448120117, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 05:57:20 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:57:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:57:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=135)
2025-09-14 05:57:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=68, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 05:57:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=135, loss_sum=94.245239, avg_loss=0.698113, seen=135, correct=74, accuracy=0.548148
2025-09-14 05:57:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:57:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 135, 'val_loss': 94.2452392578125, 'val_avg_loss': 0.6981128833912037, 'val_seen': 135, 'val_correct': 74, 'val_acc': 0.5481481481481482}
2025-09-14 05:57:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:57:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 135, 'val_loss': 94.2452392578125, 'val_avg_loss': 0.6981128833912037, 'val_seen': 135, 'val_correct': 74, 'val_acc': 0.5481481481481482}
2025-09-14 05:57:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 23.718068718910217, 'val_avg_loss': 0.6975902564385358, 'val_seen': 34, 'val_correct': 19, 'val_acc': 0.5588235294117647}}
2025-09-14 05:57:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 135, 'val_loss': 94.2452392578125, 'val_avg_loss': 0.6981128833912037, 'val_seen': 135, 'val_correct': 74, 'val_acc': 0.5481481481481482}}
2025-09-14 05:57:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:57:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:29 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:57:29 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.385050, avg_loss=0.609626, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:57:29 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:29 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:30 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:31 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:57:31 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.38504981994629, 'test_avg_loss': 0.6096262454986572, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:57:31 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 135, 'val_loss': 94.2452392578125, 'val_avg_loss': 0.6981128833912037, 'val_seen': 135, 'val_correct': 74, 'val_acc': 0.5481481481481482}
2025-09-14 05:57:31 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.38504981994629, 'test_avg_loss': 0.6096262454986572, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:57:31 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.861854076385498, 'test_avg_loss': 0.5861854076385498, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:57:31 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #18', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.38504981994629, 'test_avg_loss': 0.6096262454986572, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-14 05:57:31 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.38504981994629, 'test_avg_loss': 0.6096262454986572, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 135, 'val_loss': 94.2452392578125, 'val_avg_loss': 0.6981128833912037, 'val_seen': 135, 'val_correct': 74, 'val_acc': 0.5481481481481482, 'test_total': 40, 'test_loss': 24.38504981994629, 'test_avg_loss': 0.6096262454986572, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:57:31 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:57:31 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:57:32 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 05:57:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:32 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 05:57:34 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.292328, avg_loss=0.675385, seen=110, correct=60, accuracy=0.545455
2025-09-14 05:57:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:36 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:57:36 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 110, 'val_loss': 74.29232788085938, 'val_avg_loss': 0.6753847989169034, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-14 05:57:36 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:57:36 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 110, 'val_loss': 74.29232788085938, 'val_avg_loss': 0.6753847989169034, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-14 05:57:36 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 18.77069401741028, 'val_avg_loss': 0.6703819291932243, 'val_seen': 28, 'val_correct': 16, 'val_acc': 0.5714285714285714}}
2025-09-14 05:57:36 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 110, 'val_loss': 74.29232788085938, 'val_avg_loss': 0.6753847989169034, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}}
2025-09-14 05:57:36 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:57:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:36 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:57:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.987919, avg_loss=0.749698, seen=40, correct=16, accuracy=0.400000
2025-09-14 05:57:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:38 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:39 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:57:39 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.987918853759766, 'test_avg_loss': 0.7496979713439942, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-14 05:57:39 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 110, 'val_loss': 74.29232788085938, 'val_avg_loss': 0.6753847989169034, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454}
2025-09-14 05:57:39 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.987918853759766, 'test_avg_loss': 0.7496979713439942, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-14 05:57:39 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.134006142616272, 'test_avg_loss': 0.8134006142616272, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 05:57:39 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #19', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.987918853759766, 'test_avg_loss': 0.7496979713439942, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}}
2025-09-14 05:57:39 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.987918853759766, 'test_avg_loss': 0.7496979713439942, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}, metrics={'val_total': 110, 'val_loss': 74.29232788085938, 'val_avg_loss': 0.6753847989169034, 'val_seen': 110, 'val_correct': 60, 'val_acc': 0.5454545454545454, 'test_total': 40, 'test_loss': 29.987918853759766, 'test_avg_loss': 0.7496979713439942, 'test_seen': 40, 'test_correct': 16, 'test_acc': 0.4}
2025-09-14 05:57:39 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:57:39 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:57:40 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=32, total=126)
2025-09-14 05:57:40 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:40 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=63, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=32
2025-09-14 05:57:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=126, loss_sum=85.256042, avg_loss=0.676635, seen=126, correct=70, accuracy=0.555556
2025-09-14 05:57:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:57:46 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 126, 'val_loss': 85.25604248046875, 'val_avg_loss': 0.676635257781498, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}
2025-09-14 05:57:46 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:57:46 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 126, 'val_loss': 85.25604248046875, 'val_avg_loss': 0.676635257781498, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}
2025-09-14 05:57:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 32, 'val_loss': 21.759181559085846, 'val_avg_loss': 0.6799744237214327, 'val_seen': 32, 'val_correct': 17, 'val_acc': 0.53125}}
2025-09-14 05:57:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 126, 'val_loss': 85.25604248046875, 'val_avg_loss': 0.676635257781498, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}}
2025-09-14 05:57:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:57:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:57:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.611897, avg_loss=0.640297, seen=40, correct=28, accuracy=0.700000
2025-09-14 05:57:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:57:49 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.611896514892578, 'test_avg_loss': 0.6402974128723145, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:57:49 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 126, 'val_loss': 85.25604248046875, 'val_avg_loss': 0.676635257781498, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556}
2025-09-14 05:57:49 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.611896514892578, 'test_avg_loss': 0.6402974128723145, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:57:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.193733334541321, 'test_avg_loss': 0.619373333454132, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:57:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #20', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.611896514892578, 'test_avg_loss': 0.6402974128723145, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-14 05:57:49 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.611896514892578, 'test_avg_loss': 0.6402974128723145, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 126, 'val_loss': 85.25604248046875, 'val_avg_loss': 0.676635257781498, 'val_seen': 126, 'val_correct': 70, 'val_acc': 0.5555555555555556, 'test_total': 40, 'test_loss': 25.611896514892578, 'test_avg_loss': 0.6402974128723145, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 05:57:49 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:57:49 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:57:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=39, total=153)
2025-09-14 05:57:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=77, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=39
2025-09-14 05:57:55 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=153, loss_sum=103.045158, avg_loss=0.673498, seen=153, correct=90, accuracy=0.588235
2025-09-14 05:57:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:56 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:57:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:57:57 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 153, 'val_loss': 103.04515838623047, 'val_avg_loss': 0.6734977672302646, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-14 05:57:57 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:57:57 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 153, 'val_loss': 103.04515838623047, 'val_avg_loss': 0.6734977672302646, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-14 05:57:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 39, 'val_loss': 26.106261372566223, 'val_avg_loss': 0.6693913172452878, 'val_seen': 39, 'val_correct': 23, 'val_acc': 0.5897435897435898}}
2025-09-14 05:57:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 153, 'val_loss': 103.04515838623047, 'val_avg_loss': 0.6734977672302646, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}}
2025-09-14 05:57:57 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:57:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:57 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:57:57 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:57:57 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.278633, avg_loss=0.706966, seen=40, correct=21, accuracy=0.525000
2025-09-14 05:57:57 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:57:57 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:57:59 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:00 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:00 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.27863311767578, 'test_avg_loss': 0.7069658279418946, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:58:00 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 153, 'val_loss': 103.04515838623047, 'val_avg_loss': 0.6734977672302646, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471}
2025-09-14 05:58:00 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.27863311767578, 'test_avg_loss': 0.7069658279418946, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:58:00 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.758159816265106, 'test_avg_loss': 0.6758159816265106, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 05:58:00 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #21', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.27863311767578, 'test_avg_loss': 0.7069658279418946, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-14 05:58:00 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.27863311767578, 'test_avg_loss': 0.7069658279418946, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 153, 'val_loss': 103.04515838623047, 'val_avg_loss': 0.6734977672302646, 'val_seen': 153, 'val_correct': 90, 'val_acc': 0.5882352941176471, 'test_total': 40, 'test_loss': 28.27863311767578, 'test_avg_loss': 0.7069658279418946, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 05:58:00 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:58:00 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:58:01 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=3, total=11)
2025-09-14 05:58:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:01 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=6, num_train_batch_last_epoch=200, num_train_epoch=17, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:01 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=3
2025-09-14 05:58:01 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=11, loss_sum=6.796032, avg_loss=0.617821, seen=11, correct=8, accuracy=0.727273
2025-09-14 05:58:01 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:01 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:02 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:03 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 11, 'val_loss': 6.796031951904297, 'val_avg_loss': 0.6178210865367543, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}
2025-09-14 05:58:03 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:58:03 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 11, 'val_loss': 6.796031951904297, 'val_avg_loss': 0.6178210865367543, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}
2025-09-14 05:58:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 3, 'val_loss': 1.4507781863212585, 'val_avg_loss': 0.48359272877375287, 'val_seen': 3, 'val_correct': 3, 'val_acc': 1.0}}
2025-09-14 05:58:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 11, 'val_loss': 6.796031951904297, 'val_avg_loss': 0.6178210865367543, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}}
2025-09-14 05:58:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:58:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:03 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:58:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=30.753204, avg_loss=0.768830, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:58:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:05 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:06 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 30.753204345703125, 'test_avg_loss': 0.7688301086425782, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:58:06 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 11, 'val_loss': 6.796031951904297, 'val_avg_loss': 0.6178210865367543, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273}
2025-09-14 05:58:06 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 30.753204345703125, 'test_avg_loss': 0.7688301086425782, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:58:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.854738473892212, 'test_avg_loss': 0.7854738473892212, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 05:58:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #22', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 30.753204345703125, 'test_avg_loss': 0.7688301086425782, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-14 05:58:06 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 30.753204345703125, 'test_avg_loss': 0.7688301086425782, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 11, 'val_loss': 6.796031951904297, 'val_avg_loss': 0.6178210865367543, 'val_seen': 11, 'val_correct': 8, 'val_acc': 0.7272727272727273, 'test_total': 40, 'test_loss': 30.753204345703125, 'test_avg_loss': 0.7688301086425782, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:58:06 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:58:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:58:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=8, total=30)
2025-09-14 05:58:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=15, num_train_batch_last_epoch=200, num_train_epoch=7, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:08 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=8
2025-09-14 05:58:08 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=30, loss_sum=18.340237, avg_loss=0.611341, seen=30, correct=21, accuracy=0.700000
2025-09-14 05:58:08 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:08 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:08 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:09 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:09 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 30, 'val_loss': 18.34023666381836, 'val_avg_loss': 0.6113412221272786, 'val_seen': 30, 'val_correct': 21, 'val_acc': 0.7}
2025-09-14 05:58:09 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:58:09 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 30, 'val_loss': 18.34023666381836, 'val_avg_loss': 0.6113412221272786, 'val_seen': 30, 'val_correct': 21, 'val_acc': 0.7}
2025-09-14 05:58:09 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 8, 'val_loss': 5.366581678390503, 'val_avg_loss': 0.6708227097988129, 'val_seen': 8, 'val_correct': 4, 'val_acc': 0.5}}
2025-09-14 05:58:09 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 30, 'val_loss': 18.34023666381836, 'val_avg_loss': 0.6113412221272786, 'val_seen': 30, 'val_correct': 21, 'val_acc': 0.7}}
2025-09-14 05:58:09 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:58:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:09 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:10 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:58:10 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.244785, avg_loss=0.631120, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:58:10 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:10 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:11 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:12 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:12 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.24478530883789, 'test_avg_loss': 0.6311196327209473, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:58:12 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 30, 'val_loss': 18.34023666381836, 'val_avg_loss': 0.6113412221272786, 'val_seen': 30, 'val_correct': 21, 'val_acc': 0.7}
2025-09-14 05:58:12 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.24478530883789, 'test_avg_loss': 0.6311196327209473, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:58:12 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.253681838512421, 'test_avg_loss': 0.625368183851242, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 05:58:12 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #23', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.24478530883789, 'test_avg_loss': 0.6311196327209473, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 05:58:12 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.24478530883789, 'test_avg_loss': 0.6311196327209473, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 30, 'val_loss': 18.34023666381836, 'val_avg_loss': 0.6113412221272786, 'val_seen': 30, 'val_correct': 21, 'val_acc': 0.7, 'test_total': 40, 'test_loss': 25.24478530883789, 'test_avg_loss': 0.6311196327209473, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:58:12 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:58:12 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:58:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:58:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:58:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.850082, avg_loss=0.679250, seen=200, correct=110, accuracy=0.550000
2025-09-14 05:58:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:20 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 135.85008239746094, 'val_avg_loss': 0.6792504119873047, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-14 05:58:20 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:58:20 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 135.85008239746094, 'val_avg_loss': 0.6792504119873047, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-14 05:58:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 32.12375420331955, 'val_avg_loss': 0.642475084066391, 'val_seen': 50, 'val_correct': 31, 'val_acc': 0.62}}
2025-09-14 05:58:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 135.85008239746094, 'val_avg_loss': 0.6792504119873047, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}}
2025-09-14 05:58:21 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:58:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.554890, avg_loss=0.713872, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:58:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:22 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:23 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.554889678955078, 'test_avg_loss': 0.713872241973877, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:58:23 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 135.85008239746094, 'val_avg_loss': 0.6792504119873047, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55}
2025-09-14 05:58:23 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.554889678955078, 'test_avg_loss': 0.713872241973877, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:58:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.5662161111831665, 'test_avg_loss': 0.6566216111183166, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:58:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #24', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.554889678955078, 'test_avg_loss': 0.713872241973877, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-14 05:58:23 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.554889678955078, 'test_avg_loss': 0.713872241973877, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 200, 'val_loss': 135.85008239746094, 'val_avg_loss': 0.6792504119873047, 'val_seen': 200, 'val_correct': 110, 'val_acc': 0.55, 'test_total': 40, 'test_loss': 28.554889678955078, 'test_avg_loss': 0.713872241973877, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:58:23 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:58:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:58:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:58:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:58:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=141.120956, avg_loss=0.705605, seen=200, correct=111, accuracy=0.555000
2025-09-14 05:58:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:32 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 141.12095642089844, 'val_avg_loss': 0.7056047821044922, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-14 05:58:32 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:58:32 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 141.12095642089844, 'val_avg_loss': 0.7056047821044922, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-14 05:58:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.36227750778198, 'val_avg_loss': 0.7072455501556396, 'val_seen': 50, 'val_correct': 29, 'val_acc': 0.58}}
2025-09-14 05:58:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 141.12095642089844, 'val_avg_loss': 0.7056047821044922, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}}
2025-09-14 05:58:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:58:33 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.888559, avg_loss=0.722214, seen=40, correct=15, accuracy=0.375000
2025-09-14 05:58:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:35 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:35 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:35 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.888559341430664, 'test_avg_loss': 0.7222139835357666, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-14 05:58:35 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 141.12095642089844, 'val_avg_loss': 0.7056047821044922, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555}
2025-09-14 05:58:35 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.888559341430664, 'test_avg_loss': 0.7222139835357666, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-14 05:58:35 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.205465912818909, 'test_avg_loss': 0.7205465912818909, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-09-14 05:58:35 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #25', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.888559341430664, 'test_avg_loss': 0.7222139835357666, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}}
2025-09-14 05:58:35 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.888559341430664, 'test_avg_loss': 0.7222139835357666, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}, metrics={'val_total': 200, 'val_loss': 141.12095642089844, 'val_avg_loss': 0.7056047821044922, 'val_seen': 200, 'val_correct': 111, 'val_acc': 0.555, 'test_total': 40, 'test_loss': 28.888559341430664, 'test_avg_loss': 0.7222139835357666, 'test_seen': 40, 'test_correct': 15, 'test_acc': 0.375}
2025-09-14 05:58:35 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:58:36 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:58:37 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=41, total=161)
2025-09-14 05:58:37 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:37 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=81, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:42 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=41
2025-09-14 05:58:42 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=161, loss_sum=101.891907, avg_loss=0.632869, seen=161, correct=110, accuracy=0.683230
2025-09-14 05:58:42 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:43 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 161, 'val_loss': 101.89190673828125, 'val_avg_loss': 0.6328689859520574, 'val_seen': 161, 'val_correct': 110, 'val_acc': 0.6832298136645962}
2025-09-14 05:58:43 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:58:43 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 161, 'val_loss': 101.89190673828125, 'val_avg_loss': 0.6328689859520574, 'val_seen': 161, 'val_correct': 110, 'val_acc': 0.6832298136645962}
2025-09-14 05:58:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 41, 'val_loss': 25.304348468780518, 'val_avg_loss': 0.6171792309458662, 'val_seen': 41, 'val_correct': 27, 'val_acc': 0.6585365853658537}}
2025-09-14 05:58:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 161, 'val_loss': 101.89190673828125, 'val_avg_loss': 0.6328689859520574, 'val_seen': 161, 'val_correct': 110, 'val_acc': 0.6832298136645962}}
2025-09-14 05:58:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:58:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:58:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=24.300503, avg_loss=0.607513, seen=40, correct=26, accuracy=0.650000
2025-09-14 05:58:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:47 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:47 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 24.30050277709961, 'test_avg_loss': 0.6075125694274902, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 05:58:47 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 161, 'val_loss': 101.89190673828125, 'val_avg_loss': 0.6328689859520574, 'val_seen': 161, 'val_correct': 110, 'val_acc': 0.6832298136645962}
2025-09-14 05:58:47 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 24.30050277709961, 'test_avg_loss': 0.6075125694274902, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 05:58:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.941683769226074, 'test_avg_loss': 0.5941683769226074, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:58:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #26', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 24.30050277709961, 'test_avg_loss': 0.6075125694274902, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 05:58:47 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 24.30050277709961, 'test_avg_loss': 0.6075125694274902, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 161, 'val_loss': 101.89190673828125, 'val_avg_loss': 0.6328689859520574, 'val_seen': 161, 'val_correct': 110, 'val_acc': 0.6832298136645962, 'test_total': 40, 'test_loss': 24.30050277709961, 'test_avg_loss': 0.6075125694274902, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 05:58:47 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:58:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:58:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=31, total=123)
2025-09-14 05:58:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=62, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:51 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=31
2025-09-14 05:58:51 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=123, loss_sum=79.224075, avg_loss=0.644098, seen=123, correct=77, accuracy=0.626016
2025-09-14 05:58:51 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:53 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 123, 'val_loss': 79.22407531738281, 'val_avg_loss': 0.6440981733120554, 'val_seen': 123, 'val_correct': 77, 'val_acc': 0.6260162601626016}
2025-09-14 05:58:53 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:58:53 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 123, 'val_loss': 79.22407531738281, 'val_avg_loss': 0.6440981733120554, 'val_seen': 123, 'val_correct': 77, 'val_acc': 0.6260162601626016}
2025-09-14 05:58:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 31, 'val_loss': 20.093661427497864, 'val_avg_loss': 0.6481826266934795, 'val_seen': 31, 'val_correct': 22, 'val_acc': 0.7096774193548387}}
2025-09-14 05:58:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 123, 'val_loss': 79.22407531738281, 'val_avg_loss': 0.6440981733120554, 'val_seen': 123, 'val_correct': 77, 'val_acc': 0.6260162601626016}}
2025-09-14 05:58:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:58:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:58:55 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:58:55 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.828842, avg_loss=0.645721, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:58:55 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:58:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:58:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:58:57 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.828842163085938, 'test_avg_loss': 0.6457210540771484, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:58:57 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 123, 'val_loss': 79.22407531738281, 'val_avg_loss': 0.6440981733120554, 'val_seen': 123, 'val_correct': 77, 'val_acc': 0.6260162601626016}
2025-09-14 05:58:57 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.828842163085938, 'test_avg_loss': 0.6457210540771484, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:58:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.67755264043808, 'test_avg_loss': 0.5677552640438079, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:58:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #27', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.828842163085938, 'test_avg_loss': 0.6457210540771484, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 05:58:57 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.828842163085938, 'test_avg_loss': 0.6457210540771484, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 123, 'val_loss': 79.22407531738281, 'val_avg_loss': 0.6440981733120554, 'val_seen': 123, 'val_correct': 77, 'val_acc': 0.6260162601626016, 'test_total': 40, 'test_loss': 25.828842163085938, 'test_avg_loss': 0.6457210540771484, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:58:57 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:58:57 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:58:59 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=75)
2025-09-14 05:58:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:58:59 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=38, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 05:59:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=75, loss_sum=48.962555, avg_loss=0.652834, seen=75, correct=45, accuracy=0.600000
2025-09-14 05:59:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:59:01 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 75, 'val_loss': 48.962554931640625, 'val_avg_loss': 0.6528340657552083, 'val_seen': 75, 'val_correct': 45, 'val_acc': 0.6}
2025-09-14 05:59:01 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:59:01 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 75, 'val_loss': 48.962554931640625, 'val_avg_loss': 0.6528340657552083, 'val_seen': 75, 'val_correct': 45, 'val_acc': 0.6}
2025-09-14 05:59:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 19, 'val_loss': 13.344232559204102, 'val_avg_loss': 0.7023280294317948, 'val_seen': 19, 'val_correct': 10, 'val_acc': 0.5263157894736842}}
2025-09-14 05:59:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 75, 'val_loss': 48.962554931640625, 'val_avg_loss': 0.6528340657552083, 'val_seen': 75, 'val_correct': 45, 'val_acc': 0.6}}
2025-09-14 05:59:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:59:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:03 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:59:03 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.138943, avg_loss=0.678474, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:59:03 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2290MB allocated=2200MB
2025-09-14 05:59:05 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.13894271850586, 'test_avg_loss': 0.6784735679626465, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:59:05 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 75, 'val_loss': 48.962554931640625, 'val_avg_loss': 0.6528340657552083, 'val_seen': 75, 'val_correct': 45, 'val_acc': 0.6}
2025-09-14 05:59:05 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.13894271850586, 'test_avg_loss': 0.6784735679626465, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:59:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.815267205238342, 'test_avg_loss': 0.5815267205238343, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 05:59:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #28', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.13894271850586, 'test_avg_loss': 0.6784735679626465, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 05:59:05 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.13894271850586, 'test_avg_loss': 0.6784735679626465, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 75, 'val_loss': 48.962554931640625, 'val_avg_loss': 0.6528340657552083, 'val_seen': 75, 'val_correct': 45, 'val_acc': 0.6, 'test_total': 40, 'test_loss': 27.13894271850586, 'test_avg_loss': 0.6784735679626465, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:59:05 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:59:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:59:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:59:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:12 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 05:59:12 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=136.639847, avg_loss=0.683199, seen=200, correct=117, accuracy=0.585000
2025-09-14 05:59:12 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:13 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:13 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 136.6398468017578, 'val_avg_loss': 0.683199234008789, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 05:59:13 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:59:13 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 136.6398468017578, 'val_avg_loss': 0.683199234008789, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 05:59:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.564396381378174, 'val_avg_loss': 0.7112879276275634, 'val_seen': 50, 'val_correct': 29, 'val_acc': 0.58}}
2025-09-14 05:59:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 136.6398468017578, 'val_avg_loss': 0.683199234008789, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}}
2025-09-14 05:59:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:59:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:14 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:59:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.746126, avg_loss=0.668653, seen=40, correct=25, accuracy=0.625000
2025-09-14 05:59:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:16 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.746126174926758, 'test_avg_loss': 0.668653154373169, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 05:59:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 136.6398468017578, 'val_avg_loss': 0.683199234008789, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 05:59:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.746126174926758, 'test_avg_loss': 0.668653154373169, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 05:59:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.233900666236877, 'test_avg_loss': 0.7233900666236878, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 05:59:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #29', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.746126174926758, 'test_avg_loss': 0.668653154373169, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 05:59:16 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.746126174926758, 'test_avg_loss': 0.668653154373169, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 200, 'val_loss': 136.6398468017578, 'val_avg_loss': 0.683199234008789, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585, 'test_total': 40, 'test_loss': 26.746126174926758, 'test_avg_loss': 0.668653154373169, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 05:59:16 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:59:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:59:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=43, total=170)
2025-09-14 05:59:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=85, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:22 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=43
2025-09-14 05:59:22 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=170, loss_sum=112.351288, avg_loss=0.660890, seen=170, correct=99, accuracy=0.582353
2025-09-14 05:59:22 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:24 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:24 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 170, 'val_loss': 112.35128784179688, 'val_avg_loss': 0.6608899284811581, 'val_seen': 170, 'val_correct': 99, 'val_acc': 0.5823529411764706}
2025-09-14 05:59:24 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:59:24 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 170, 'val_loss': 112.35128784179688, 'val_avg_loss': 0.6608899284811581, 'val_seen': 170, 'val_correct': 99, 'val_acc': 0.5823529411764706}
2025-09-14 05:59:24 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 43, 'val_loss': 28.235517621040344, 'val_avg_loss': 0.6566399446753568, 'val_seen': 43, 'val_correct': 25, 'val_acc': 0.5813953488372093}}
2025-09-14 05:59:24 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 170, 'val_loss': 112.35128784179688, 'val_avg_loss': 0.6608899284811581, 'val_seen': 170, 'val_correct': 99, 'val_acc': 0.5823529411764706}}
2025-09-14 05:59:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:59:24 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:24 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:59:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.057709, avg_loss=0.676443, seen=40, correct=23, accuracy=0.575000
2025-09-14 05:59:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:27 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.057708740234375, 'test_avg_loss': 0.6764427185058594, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:59:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 170, 'val_loss': 112.35128784179688, 'val_avg_loss': 0.6608899284811581, 'val_seen': 170, 'val_correct': 99, 'val_acc': 0.5823529411764706}
2025-09-14 05:59:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.057708740234375, 'test_avg_loss': 0.6764427185058594, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:59:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.902909100055695, 'test_avg_loss': 0.6902909100055694, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 05:59:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #30', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.057708740234375, 'test_avg_loss': 0.6764427185058594, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 05:59:27 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.057708740234375, 'test_avg_loss': 0.6764427185058594, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 170, 'val_loss': 112.35128784179688, 'val_avg_loss': 0.6608899284811581, 'val_seen': 170, 'val_correct': 99, 'val_acc': 0.5823529411764706, 'test_total': 40, 'test_loss': 27.057708740234375, 'test_avg_loss': 0.6764427185058594, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 05:59:27 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:59:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:59:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=49, total=193)
2025-09-14 05:59:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=97, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:33 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=49
2025-09-14 05:59:33 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=193, loss_sum=133.456726, avg_loss=0.691486, seen=193, correct=115, accuracy=0.595855
2025-09-14 05:59:33 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:34 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:34 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:34 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 193, 'val_loss': 133.45672607421875, 'val_avg_loss': 0.6914856273275582, 'val_seen': 193, 'val_correct': 115, 'val_acc': 0.5958549222797928}
2025-09-14 05:59:34 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:59:34 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 193, 'val_loss': 133.45672607421875, 'val_avg_loss': 0.6914856273275582, 'val_seen': 193, 'val_correct': 115, 'val_acc': 0.5958549222797928}
2025-09-14 05:59:34 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 49, 'val_loss': 31.988588452339172, 'val_avg_loss': 0.6528283357620239, 'val_seen': 49, 'val_correct': 32, 'val_acc': 0.6530612244897959}}
2025-09-14 05:59:34 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 193, 'val_loss': 133.45672607421875, 'val_avg_loss': 0.6914856273275582, 'val_seen': 193, 'val_correct': 115, 'val_acc': 0.5958549222797928}}
2025-09-14 05:59:35 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:35 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:35 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:59:35 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.524927, avg_loss=0.688123, seen=40, correct=19, accuracy=0.475000
2025-09-14 05:59:35 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:35 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:38 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.524927139282227, 'test_avg_loss': 0.6881231784820556, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:59:38 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 193, 'val_loss': 133.45672607421875, 'val_avg_loss': 0.6914856273275582, 'val_seen': 193, 'val_correct': 115, 'val_acc': 0.5958549222797928}
2025-09-14 05:59:38 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.524927139282227, 'test_avg_loss': 0.6881231784820556, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:59:38 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.380212783813477, 'test_avg_loss': 0.8380212783813477, 'test_seen': 10, 'test_correct': 2, 'test_acc': 0.2}}
2025-09-14 05:59:38 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #31', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.524927139282227, 'test_avg_loss': 0.6881231784820556, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-14 05:59:38 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.524927139282227, 'test_avg_loss': 0.6881231784820556, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 193, 'val_loss': 133.45672607421875, 'val_avg_loss': 0.6914856273275582, 'val_seen': 193, 'val_correct': 115, 'val_acc': 0.5958549222797928, 'test_total': 40, 'test_loss': 27.524927139282227, 'test_avg_loss': 0.6881231784820556, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 05:59:38 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:59:38 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:59:39 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=112)
2025-09-14 05:59:39 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:39 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=56, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 05:59:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=112, loss_sum=69.660576, avg_loss=0.621969, seen=112, correct=76, accuracy=0.678571
2025-09-14 05:59:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:42 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:43 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 112, 'val_loss': 69.66057586669922, 'val_avg_loss': 0.6219694273812431, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}
2025-09-14 05:59:43 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:59:43 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 112, 'val_loss': 69.66057586669922, 'val_avg_loss': 0.6219694273812431, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}
2025-09-14 05:59:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 15.537632554769516, 'val_avg_loss': 0.5549154483846256, 'val_seen': 28, 'val_correct': 22, 'val_acc': 0.7857142857142857}}
2025-09-14 05:59:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 112, 'val_loss': 69.66057586669922, 'val_avg_loss': 0.6219694273812431, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}}
2025-09-14 05:59:43 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:59:43 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:43 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:59:44 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.708698, avg_loss=0.742717, seen=40, correct=20, accuracy=0.500000
2025-09-14 05:59:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:46 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.708698272705078, 'test_avg_loss': 0.7427174568176269, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 05:59:46 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 112, 'val_loss': 69.66057586669922, 'val_avg_loss': 0.6219694273812431, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286}
2025-09-14 05:59:46 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.708698272705078, 'test_avg_loss': 0.7427174568176269, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 05:59:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.034087419509888, 'test_avg_loss': 0.8034087419509888, 'test_seen': 10, 'test_correct': 3, 'test_acc': 0.3}}
2025-09-14 05:59:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #32', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.708698272705078, 'test_avg_loss': 0.7427174568176269, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}}
2025-09-14 05:59:46 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.708698272705078, 'test_avg_loss': 0.7427174568176269, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}, metrics={'val_total': 112, 'val_loss': 69.66057586669922, 'val_avg_loss': 0.6219694273812431, 'val_seen': 112, 'val_correct': 76, 'val_acc': 0.6785714285714286, 'test_total': 40, 'test_loss': 29.708698272705078, 'test_avg_loss': 0.7427174568176269, 'test_seen': 40, 'test_correct': 20, 'test_acc': 0.5}
2025-09-14 05:59:46 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:59:46 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:59:47 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=19, total=74)
2025-09-14 05:59:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:47 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=37, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:48 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=19
2025-09-14 05:59:48 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=74, loss_sum=51.780006, avg_loss=0.699730, seen=74, correct=41, accuracy=0.554054
2025-09-14 05:59:48 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:49 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:50 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 74, 'val_loss': 51.780006408691406, 'val_avg_loss': 0.6997298163336676, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}
2025-09-14 05:59:50 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 05:59:50 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 74, 'val_loss': 51.780006408691406, 'val_avg_loss': 0.6997298163336676, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}
2025-09-14 05:59:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 19, 'val_loss': 13.62028455734253, 'val_avg_loss': 0.7168570819653963, 'val_seen': 19, 'val_correct': 8, 'val_acc': 0.42105263157894735}}
2025-09-14 05:59:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 74, 'val_loss': 51.780006408691406, 'val_avg_loss': 0.6997298163336676, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}}
2025-09-14 05:59:50 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 05:59:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:50 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 05:59:52 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 05:59:52 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.651522, avg_loss=0.666288, seen=40, correct=24, accuracy=0.600000
2025-09-14 05:59:52 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 05:59:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 05:59:53 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2288MB allocated=2200MB
2025-09-14 05:59:53 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.651521682739258, 'test_avg_loss': 0.6662880420684815, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:59:53 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 74, 'val_loss': 51.780006408691406, 'val_avg_loss': 0.6997298163336676, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541}
2025-09-14 05:59:53 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.651521682739258, 'test_avg_loss': 0.6662880420684815, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:59:53 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.004287242889404, 'test_avg_loss': 0.7004287242889404, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 05:59:53 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #33', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.651521682739258, 'test_avg_loss': 0.6662880420684815, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 05:59:53 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.651521682739258, 'test_avg_loss': 0.6662880420684815, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 74, 'val_loss': 51.780006408691406, 'val_avg_loss': 0.6997298163336676, 'val_seen': 74, 'val_correct': 41, 'val_acc': 0.5540540540540541, 'test_total': 40, 'test_loss': 26.651521682739258, 'test_avg_loss': 0.6662880420684815, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 05:59:53 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 05:59:53 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 05:59:54 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 05:59:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 05:59:54 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:00 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 06:00:00 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.067657, avg_loss=0.645338, seen=200, correct=128, accuracy=0.640000
2025-09-14 06:00:00 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:01 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:02 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 06:00:02 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 129.06765747070312, 'val_avg_loss': 0.6453382873535156, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-14 06:00:02 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:00:02 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 129.06765747070312, 'val_avg_loss': 0.6453382873535156, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-14 06:00:02 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 30.06751722097397, 'val_avg_loss': 0.6013503444194793, 'val_seen': 50, 'val_correct': 37, 'val_acc': 0.74}}
2025-09-14 06:00:02 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 129.06765747070312, 'val_avg_loss': 0.6453382873535156, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}}
2025-09-14 06:00:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:00:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:04 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:00:04 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.792000, avg_loss=0.644800, seen=40, correct=26, accuracy=0.650000
2025-09-14 06:00:04 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:04 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:04 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:05 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 06:00:05 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.79199981689453, 'test_avg_loss': 0.6447999954223633, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:00:05 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 129.06765747070312, 'val_avg_loss': 0.6453382873535156, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64}
2025-09-14 06:00:05 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.79199981689453, 'test_avg_loss': 0.6447999954223633, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:00:05 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.761809349060059, 'test_avg_loss': 0.6761809349060058, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 06:00:05 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #34', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.79199981689453, 'test_avg_loss': 0.6447999954223633, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 06:00:05 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.79199981689453, 'test_avg_loss': 0.6447999954223633, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 200, 'val_loss': 129.06765747070312, 'val_avg_loss': 0.6453382873535156, 'val_seen': 200, 'val_correct': 128, 'val_acc': 0.64, 'test_total': 40, 'test_loss': 25.79199981689453, 'test_avg_loss': 0.6447999954223633, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:00:05 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:00:05 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:00:06 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 06:00:06 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:06 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:11 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 06:00:11 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=127.590797, avg_loss=0.637954, seen=200, correct=130, accuracy=0.650000
2025-09-14 06:00:11 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:12 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:13 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 06:00:13 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 127.5907974243164, 'val_avg_loss': 0.637953987121582, 'val_seen': 200, 'val_correct': 130, 'val_acc': 0.65}
2025-09-14 06:00:13 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:00:13 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 127.5907974243164, 'val_avg_loss': 0.637953987121582, 'val_seen': 200, 'val_correct': 130, 'val_acc': 0.65}
2025-09-14 06:00:13 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 33.73596924543381, 'val_avg_loss': 0.6747193849086761, 'val_seen': 50, 'val_correct': 29, 'val_acc': 0.58}}
2025-09-14 06:00:13 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 127.5907974243164, 'val_avg_loss': 0.637953987121582, 'val_seen': 200, 'val_correct': 130, 'val_acc': 0.65}}
2025-09-14 06:00:13 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:00:13 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:13 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:00:15 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.701727, avg_loss=0.692543, seen=40, correct=22, accuracy=0.550000
2025-09-14 06:00:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2286MB allocated=2200MB
2025-09-14 06:00:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.70172691345215, 'test_avg_loss': 0.6925431728363037, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 06:00:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 127.5907974243164, 'val_avg_loss': 0.637953987121582, 'val_seen': 200, 'val_correct': 130, 'val_acc': 0.65}
2025-09-14 06:00:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.70172691345215, 'test_avg_loss': 0.6925431728363037, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 06:00:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.2259481549263, 'test_avg_loss': 0.52259481549263, 'test_seen': 10, 'test_correct': 8, 'test_acc': 0.8}}
2025-09-14 06:00:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #35', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.70172691345215, 'test_avg_loss': 0.6925431728363037, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 06:00:16 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.70172691345215, 'test_avg_loss': 0.6925431728363037, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 200, 'val_loss': 127.5907974243164, 'val_avg_loss': 0.637953987121582, 'val_seen': 200, 'val_correct': 130, 'val_acc': 0.65, 'test_total': 40, 'test_loss': 27.70172691345215, 'test_avg_loss': 0.6925431728363037, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 06:00:16 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:00:16 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:00:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=14, total=54)
2025-09-14 06:00:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=27, num_train_batch_last_epoch=200, num_train_epoch=4, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:19 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=14
2025-09-14 06:00:19 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=54, loss_sum=35.958366, avg_loss=0.665896, seen=54, correct=34, accuracy=0.629630
2025-09-14 06:00:19 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:19 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:00:20 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 54, 'val_loss': 35.95836639404297, 'val_avg_loss': 0.6658956739637587, 'val_seen': 54, 'val_correct': 34, 'val_acc': 0.6296296296296297}
2025-09-14 06:00:20 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:00:20 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 54, 'val_loss': 35.95836639404297, 'val_avg_loss': 0.6658956739637587, 'val_seen': 54, 'val_correct': 34, 'val_acc': 0.6296296296296297}
2025-09-14 06:00:20 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 14, 'val_loss': 9.013185143470764, 'val_avg_loss': 0.6437989388193402, 'val_seen': 14, 'val_correct': 10, 'val_acc': 0.7142857142857143}}
2025-09-14 06:00:20 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 54, 'val_loss': 35.95836639404297, 'val_avg_loss': 0.6658956739637587, 'val_seen': 54, 'val_correct': 34, 'val_acc': 0.6296296296296297}}
2025-09-14 06:00:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:00:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:21 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:21 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:00:21 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.752499, avg_loss=0.668812, seen=40, correct=25, accuracy=0.625000
2025-09-14 06:00:21 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:21 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:23 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:23 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:00:23 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.752498626708984, 'test_avg_loss': 0.6688124656677246, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 06:00:23 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 54, 'val_loss': 35.95836639404297, 'val_avg_loss': 0.6658956739637587, 'val_seen': 54, 'val_correct': 34, 'val_acc': 0.6296296296296297}
2025-09-14 06:00:23 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.752498626708984, 'test_avg_loss': 0.6688124656677246, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 06:00:23 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.7683786153793335, 'test_avg_loss': 0.6768378615379333, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 06:00:23 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #36', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.752498626708984, 'test_avg_loss': 0.6688124656677246, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 06:00:23 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.752498626708984, 'test_avg_loss': 0.6688124656677246, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 54, 'val_loss': 35.95836639404297, 'val_avg_loss': 0.6658956739637587, 'val_seen': 54, 'val_correct': 34, 'val_acc': 0.6296296296296297, 'test_total': 40, 'test_loss': 26.752498626708984, 'test_avg_loss': 0.6688124656677246, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 06:00:23 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:00:24 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:00:24 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 06:00:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:31 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 06:00:31 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.683929, avg_loss=0.678420, seen=200, correct=117, accuracy=0.585000
2025-09-14 06:00:31 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:31 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:00:33 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 135.68392944335938, 'val_avg_loss': 0.6784196472167969, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 06:00:33 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:00:33 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 135.68392944335938, 'val_avg_loss': 0.6784196472167969, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 06:00:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.74607700109482, 'val_avg_loss': 0.6949215400218963, 'val_seen': 50, 'val_correct': 30, 'val_acc': 0.6}}
2025-09-14 06:00:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 135.68392944335938, 'val_avg_loss': 0.6784196472167969, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}}
2025-09-14 06:00:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:00:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:33 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:00:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.018730, avg_loss=0.675468, seen=40, correct=26, accuracy=0.650000
2025-09-14 06:00:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:00:37 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.01873016357422, 'test_avg_loss': 0.6754682540893555, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:00:37 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 135.68392944335938, 'val_avg_loss': 0.6784196472167969, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 06:00:37 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.01873016357422, 'test_avg_loss': 0.6754682540893555, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:00:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.294438123703003, 'test_avg_loss': 0.6294438123703003, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 06:00:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #37', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.01873016357422, 'test_avg_loss': 0.6754682540893555, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 06:00:37 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.01873016357422, 'test_avg_loss': 0.6754682540893555, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 200, 'val_loss': 135.68392944335938, 'val_avg_loss': 0.6784196472167969, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585, 'test_total': 40, 'test_loss': 27.01873016357422, 'test_avg_loss': 0.6754682540893555, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:00:37 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:00:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:00:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 06:00:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:44 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 06:00:44 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=132.571884, avg_loss=0.662859, seen=200, correct=120, accuracy=0.600000
2025-09-14 06:00:44 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:45 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:00:46 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 132.57188415527344, 'val_avg_loss': 0.6628594207763672, 'val_seen': 200, 'val_correct': 120, 'val_acc': 0.6}
2025-09-14 06:00:46 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:00:46 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 132.57188415527344, 'val_avg_loss': 0.6628594207763672, 'val_seen': 200, 'val_correct': 120, 'val_acc': 0.6}
2025-09-14 06:00:46 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.70530837774277, 'val_avg_loss': 0.6941061675548553, 'val_seen': 50, 'val_correct': 30, 'val_acc': 0.6}}
2025-09-14 06:00:46 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 132.57188415527344, 'val_avg_loss': 0.6628594207763672, 'val_seen': 200, 'val_correct': 120, 'val_acc': 0.6}}
2025-09-14 06:00:46 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:00:46 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:46 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:00:47 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.696177, avg_loss=0.667404, seen=40, correct=24, accuracy=0.600000
2025-09-14 06:00:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:50 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:00:50 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.696176528930664, 'test_avg_loss': 0.6674044132232666, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:00:50 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 132.57188415527344, 'val_avg_loss': 0.6628594207763672, 'val_seen': 200, 'val_correct': 120, 'val_acc': 0.6}
2025-09-14 06:00:50 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.696176528930664, 'test_avg_loss': 0.6674044132232666, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:00:50 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.305881381034851, 'test_avg_loss': 0.7305881381034851, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 06:00:50 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #38', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.696176528930664, 'test_avg_loss': 0.6674044132232666, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 06:00:50 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.696176528930664, 'test_avg_loss': 0.6674044132232666, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 200, 'val_loss': 132.57188415527344, 'val_avg_loss': 0.6628594207763672, 'val_seen': 200, 'val_correct': 120, 'val_acc': 0.6, 'test_total': 40, 'test_loss': 26.696176528930664, 'test_avg_loss': 0.6674044132232666, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:00:50 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:00:50 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:00:51 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 06:00:51 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:51 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:54 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 06:00:54 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=58.038574, avg_loss=0.699260, seen=83, correct=42, accuracy=0.506024
2025-09-14 06:00:54 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:54 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:00:55 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 83, 'val_loss': 58.03857421875, 'val_avg_loss': 0.6992599303463856, 'val_seen': 83, 'val_correct': 42, 'val_acc': 0.5060240963855421}
2025-09-14 06:00:55 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:00:55 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 83, 'val_loss': 58.03857421875, 'val_avg_loss': 0.6992599303463856, 'val_seen': 83, 'val_correct': 42, 'val_acc': 0.5060240963855421}
2025-09-14 06:00:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 21, 'val_loss': 15.93225109577179, 'val_avg_loss': 0.7586786236081805, 'val_seen': 21, 'val_correct': 8, 'val_acc': 0.38095238095238093}}
2025-09-14 06:00:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 83, 'val_loss': 58.03857421875, 'val_avg_loss': 0.6992599303463856, 'val_seen': 83, 'val_correct': 42, 'val_acc': 0.5060240963855421}}
2025-09-14 06:00:55 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:00:55 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:55 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:00:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:00:56 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.949343, avg_loss=0.723734, seen=40, correct=18, accuracy=0.450000
2025-09-14 06:00:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:00:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:00:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:00:58 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:00:58 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.949342727661133, 'test_avg_loss': 0.7237335681915283, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 06:00:58 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 83, 'val_loss': 58.03857421875, 'val_avg_loss': 0.6992599303463856, 'val_seen': 83, 'val_correct': 42, 'val_acc': 0.5060240963855421}
2025-09-14 06:00:58 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.949342727661133, 'test_avg_loss': 0.7237335681915283, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 06:00:58 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.232404112815857, 'test_avg_loss': 0.7232404112815857, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 06:00:58 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #39', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.949342727661133, 'test_avg_loss': 0.7237335681915283, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-09-14 06:00:58 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.949342727661133, 'test_avg_loss': 0.7237335681915283, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 83, 'val_loss': 58.03857421875, 'val_avg_loss': 0.6992599303463856, 'val_seen': 83, 'val_correct': 42, 'val_acc': 0.5060240963855421, 'test_total': 40, 'test_loss': 28.949342727661133, 'test_avg_loss': 0.7237335681915283, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 06:00:58 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:00:58 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:01:00 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 06:01:00 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:00 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 06:01:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=129.944061, avg_loss=0.649720, seen=200, correct=121, accuracy=0.605000
2025-09-14 06:01:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:07 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 129.94406127929688, 'val_avg_loss': 0.6497203063964844, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 06:01:07 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:01:07 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 129.94406127929688, 'val_avg_loss': 0.6497203063964844, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 06:01:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 33.252815306186676, 'val_avg_loss': 0.6650563061237336, 'val_seen': 50, 'val_correct': 31, 'val_acc': 0.62}}
2025-09-14 06:01:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 129.94406127929688, 'val_avg_loss': 0.6497203063964844, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}}
2025-09-14 06:01:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:01:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:01:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.591911, avg_loss=0.664798, seen=40, correct=23, accuracy=0.575000
2025-09-14 06:01:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:10 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.59191131591797, 'test_avg_loss': 0.6647977828979492, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:01:10 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 129.94406127929688, 'val_avg_loss': 0.6497203063964844, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 06:01:10 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.59191131591797, 'test_avg_loss': 0.6647977828979492, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:01:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.814799785614014, 'test_avg_loss': 0.6814799785614014, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 06:01:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #40', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.59191131591797, 'test_avg_loss': 0.6647977828979492, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 06:01:10 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.59191131591797, 'test_avg_loss': 0.6647977828979492, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 200, 'val_loss': 129.94406127929688, 'val_avg_loss': 0.6497203063964844, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605, 'test_total': 40, 'test_loss': 26.59191131591797, 'test_avg_loss': 0.6647977828979492, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:01:10 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:01:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:01:12 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=30, total=119)
2025-09-14 06:01:12 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:12 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=60, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:15 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=30
2025-09-14 06:01:15 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=119, loss_sum=80.426971, avg_loss=0.675857, seen=119, correct=71, accuracy=0.596639
2025-09-14 06:01:15 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:15 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 119, 'val_loss': 80.42697143554688, 'val_avg_loss': 0.6758569028197217, 'val_seen': 119, 'val_correct': 71, 'val_acc': 0.5966386554621849}
2025-09-14 06:01:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:01:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 119, 'val_loss': 80.42697143554688, 'val_avg_loss': 0.6758569028197217, 'val_seen': 119, 'val_correct': 71, 'val_acc': 0.5966386554621849}
2025-09-14 06:01:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 30, 'val_loss': 16.678875267505646, 'val_avg_loss': 0.5559625089168548, 'val_seen': 30, 'val_correct': 24, 'val_acc': 0.8}}
2025-09-14 06:01:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 119, 'val_loss': 80.42697143554688, 'val_avg_loss': 0.6758569028197217, 'val_seen': 119, 'val_correct': 71, 'val_acc': 0.5966386554621849}}
2025-09-14 06:01:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:01:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:17 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:01:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.881981, avg_loss=0.672050, seen=40, correct=24, accuracy=0.600000
2025-09-14 06:01:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:18 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:18 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.881980895996094, 'test_avg_loss': 0.6720495223999023, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:01:18 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 119, 'val_loss': 80.42697143554688, 'val_avg_loss': 0.6758569028197217, 'val_seen': 119, 'val_correct': 71, 'val_acc': 0.5966386554621849}
2025-09-14 06:01:18 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.881980895996094, 'test_avg_loss': 0.6720495223999023, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:01:18 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.01397442817688, 'test_avg_loss': 0.701397442817688, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 06:01:18 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #41', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.881980895996094, 'test_avg_loss': 0.6720495223999023, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 06:01:18 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.881980895996094, 'test_avg_loss': 0.6720495223999023, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 119, 'val_loss': 80.42697143554688, 'val_avg_loss': 0.6758569028197217, 'val_seen': 119, 'val_correct': 71, 'val_acc': 0.5966386554621849, 'test_total': 40, 'test_loss': 26.881980895996094, 'test_avg_loss': 0.6720495223999023, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:01:18 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:01:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:01:20 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 06:01:20 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:20 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:25 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 06:01:25 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=135.220886, avg_loss=0.676104, seen=200, correct=117, accuracy=0.585000
2025-09-14 06:01:25 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:28 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:28 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 135.22088623046875, 'val_avg_loss': 0.6761044311523438, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 06:01:28 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:01:28 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 135.22088623046875, 'val_avg_loss': 0.6761044311523438, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 06:01:28 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 34.146551847457886, 'val_avg_loss': 0.6829310369491577, 'val_seen': 50, 'val_correct': 29, 'val_acc': 0.58}}
2025-09-14 06:01:28 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 135.22088623046875, 'val_avg_loss': 0.6761044311523438, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}}
2025-09-14 06:01:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:01:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:29 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:30 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:01:30 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.023922, avg_loss=0.650598, seen=40, correct=28, accuracy=0.700000
2025-09-14 06:01:30 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:30 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:32 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:32 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:32 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.023921966552734, 'test_avg_loss': 0.6505980491638184, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 06:01:32 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 135.22088623046875, 'val_avg_loss': 0.6761044311523438, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585}
2025-09-14 06:01:32 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.023921966552734, 'test_avg_loss': 0.6505980491638184, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 06:01:32 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.889937579631805, 'test_avg_loss': 0.5889937579631805, 'test_seen': 10, 'test_correct': 9, 'test_acc': 0.9}}
2025-09-14 06:01:32 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #42', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.023921966552734, 'test_avg_loss': 0.6505980491638184, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}}
2025-09-14 06:01:32 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.023921966552734, 'test_avg_loss': 0.6505980491638184, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}, metrics={'val_total': 200, 'val_loss': 135.22088623046875, 'val_avg_loss': 0.6761044311523438, 'val_seen': 200, 'val_correct': 117, 'val_acc': 0.585, 'test_total': 40, 'test_loss': 26.023921966552734, 'test_avg_loss': 0.6505980491638184, 'test_seen': 40, 'test_correct': 28, 'test_acc': 0.7}
2025-09-14 06:01:32 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:01:32 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:01:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=23, total=89)
2025-09-14 06:01:33 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=45, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:36 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=23
2025-09-14 06:01:36 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=89, loss_sum=60.293091, avg_loss=0.677450, seen=89, correct=49, accuracy=0.550562
2025-09-14 06:01:36 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:36 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:37 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:37 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 89, 'val_loss': 60.2930908203125, 'val_avg_loss': 0.6774504586551966, 'val_seen': 89, 'val_correct': 49, 'val_acc': 0.550561797752809}
2025-09-14 06:01:37 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:01:37 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 89, 'val_loss': 60.2930908203125, 'val_avg_loss': 0.6774504586551966, 'val_seen': 89, 'val_correct': 49, 'val_acc': 0.550561797752809}
2025-09-14 06:01:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 23, 'val_loss': 16.32270574569702, 'val_avg_loss': 0.7096828585085662, 'val_seen': 23, 'val_correct': 12, 'val_acc': 0.5217391304347826}}
2025-09-14 06:01:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 89, 'val_loss': 60.2930908203125, 'val_avg_loss': 0.6774504586551966, 'val_seen': 89, 'val_correct': 49, 'val_acc': 0.550561797752809}}
2025-09-14 06:01:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:38 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:01:38 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.213354, avg_loss=0.630334, seen=40, correct=26, accuracy=0.650000
2025-09-14 06:01:38 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:40 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:41 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:41 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.213354110717773, 'test_avg_loss': 0.6303338527679443, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:01:41 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 89, 'val_loss': 60.2930908203125, 'val_avg_loss': 0.6774504586551966, 'val_seen': 89, 'val_correct': 49, 'val_acc': 0.550561797752809}
2025-09-14 06:01:41 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.213354110717773, 'test_avg_loss': 0.6303338527679443, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:01:41 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.365331411361694, 'test_avg_loss': 0.6365331411361694, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 06:01:41 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #43', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.213354110717773, 'test_avg_loss': 0.6303338527679443, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 06:01:41 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.213354110717773, 'test_avg_loss': 0.6303338527679443, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 89, 'val_loss': 60.2930908203125, 'val_avg_loss': 0.6774504586551966, 'val_seen': 89, 'val_correct': 49, 'val_acc': 0.550561797752809, 'test_total': 40, 'test_loss': 25.213354110717773, 'test_avg_loss': 0.6303338527679443, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:01:41 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:01:41 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:01:42 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 06:01:42 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:42 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:47 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 06:01:47 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=140.994476, avg_loss=0.704972, seen=200, correct=100, accuracy=0.500000
2025-09-14 06:01:47 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:47 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:48 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:49 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:49 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 140.99447631835938, 'val_avg_loss': 0.7049723815917969, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-14 06:01:49 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:01:49 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 140.99447631835938, 'val_avg_loss': 0.7049723815917969, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-14 06:01:49 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 35.32053941488266, 'val_avg_loss': 0.7064107882976532, 'val_seen': 50, 'val_correct': 25, 'val_acc': 0.5}}
2025-09-14 06:01:49 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 140.99447631835938, 'val_avg_loss': 0.7049723815917969, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}}
2025-09-14 06:01:49 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:01:49 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:49 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:01:50 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.841896, avg_loss=0.671047, seen=40, correct=23, accuracy=0.575000
2025-09-14 06:01:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:52 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:52 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.841896057128906, 'test_avg_loss': 0.6710474014282226, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:01:52 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 140.99447631835938, 'val_avg_loss': 0.7049723815917969, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5}
2025-09-14 06:01:52 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.841896057128906, 'test_avg_loss': 0.6710474014282226, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:01:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.590664505958557, 'test_avg_loss': 0.6590664505958557, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 06:01:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #44', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.841896057128906, 'test_avg_loss': 0.6710474014282226, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 06:01:52 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.841896057128906, 'test_avg_loss': 0.6710474014282226, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 200, 'val_loss': 140.99447631835938, 'val_avg_loss': 0.7049723815917969, 'val_seen': 200, 'val_correct': 100, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 26.841896057128906, 'test_avg_loss': 0.6710474014282226, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:01:52 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:01:52 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:01:53 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=25, total=100)
2025-09-14 06:01:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:53 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=50, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:56 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=25
2025-09-14 06:01:56 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=100, loss_sum=65.630035, avg_loss=0.656300, seen=100, correct=58, accuracy=0.580000
2025-09-14 06:01:56 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:57 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:01:57 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:01:57 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 100, 'val_loss': 65.63003540039062, 'val_avg_loss': 0.6563003540039063, 'val_seen': 100, 'val_correct': 58, 'val_acc': 0.58}
2025-09-14 06:01:57 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:01:57 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 100, 'val_loss': 65.63003540039062, 'val_avg_loss': 0.6563003540039063, 'val_seen': 100, 'val_correct': 58, 'val_acc': 0.58}
2025-09-14 06:01:57 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 25, 'val_loss': 15.39999035000801, 'val_avg_loss': 0.6159996140003204, 'val_seen': 25, 'val_correct': 16, 'val_acc': 0.64}}
2025-09-14 06:01:57 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 100, 'val_loss': 65.63003540039062, 'val_avg_loss': 0.6563003540039063, 'val_seen': 100, 'val_correct': 58, 'val_acc': 0.58}}
2025-09-14 06:01:58 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:01:58 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:01:58 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:01:59 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:01:59 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=29.172892, avg_loss=0.729322, seen=40, correct=19, accuracy=0.475000
2025-09-14 06:01:59 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:01:59 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:00 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:01 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:01 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 29.17289161682129, 'test_avg_loss': 0.7293222904205322, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 06:02:01 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 100, 'val_loss': 65.63003540039062, 'val_avg_loss': 0.6563003540039063, 'val_seen': 100, 'val_correct': 58, 'val_acc': 0.58}
2025-09-14 06:02:01 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 29.17289161682129, 'test_avg_loss': 0.7293222904205322, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 06:02:01 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.59634244441986, 'test_avg_loss': 0.8596342444419861, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 06:02:01 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #45', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 29.17289161682129, 'test_avg_loss': 0.7293222904205322, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-14 06:02:01 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 29.17289161682129, 'test_avg_loss': 0.7293222904205322, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 100, 'val_loss': 65.63003540039062, 'val_avg_loss': 0.6563003540039063, 'val_seen': 100, 'val_correct': 58, 'val_acc': 0.58, 'test_total': 40, 'test_loss': 29.17289161682129, 'test_avg_loss': 0.7293222904205322, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 06:02:01 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:02:01 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:02:02 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=28, total=110)
2025-09-14 06:02:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:02 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=55, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=28
2025-09-14 06:02:05 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=110, loss_sum=74.803772, avg_loss=0.680034, seen=110, correct=59, accuracy=0.536364
2025-09-14 06:02:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:07 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:07 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 110, 'val_loss': 74.80377197265625, 'val_avg_loss': 0.6800342906605114, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-14 06:02:07 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:02:07 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 110, 'val_loss': 74.80377197265625, 'val_avg_loss': 0.6800342906605114, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-14 06:02:07 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 28, 'val_loss': 19.433644115924835, 'val_avg_loss': 0.694058718425887, 'val_seen': 28, 'val_correct': 13, 'val_acc': 0.4642857142857143}}
2025-09-14 06:02:07 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 110, 'val_loss': 74.80377197265625, 'val_avg_loss': 0.6800342906605114, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}}
2025-09-14 06:02:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:02:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:07 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:09 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:02:09 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.806023, avg_loss=0.645151, seen=40, correct=24, accuracy=0.600000
2025-09-14 06:02:09 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:09 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:10 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:10 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:10 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.80602264404297, 'test_avg_loss': 0.6451505661010742, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:02:10 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 110, 'val_loss': 74.80377197265625, 'val_avg_loss': 0.6800342906605114, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364}
2025-09-14 06:02:10 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.80602264404297, 'test_avg_loss': 0.6451505661010742, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:02:10 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 5.601252496242523, 'test_avg_loss': 0.5601252496242524, 'test_seen': 10, 'test_correct': 7, 'test_acc': 0.7}}
2025-09-14 06:02:10 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #46', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.80602264404297, 'test_avg_loss': 0.6451505661010742, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}}
2025-09-14 06:02:10 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.80602264404297, 'test_avg_loss': 0.6451505661010742, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}, metrics={'val_total': 110, 'val_loss': 74.80377197265625, 'val_avg_loss': 0.6800342906605114, 'val_seen': 110, 'val_correct': 59, 'val_acc': 0.5363636363636364, 'test_total': 40, 'test_loss': 25.80602264404297, 'test_avg_loss': 0.6451505661010742, 'test_seen': 40, 'test_correct': 24, 'test_acc': 0.6}
2025-09-14 06:02:10 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:02:10 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:02:11 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=37, total=147)
2025-09-14 06:02:11 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:11 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=74, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:16 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=37
2025-09-14 06:02:16 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=147, loss_sum=101.877045, avg_loss=0.693041, seen=147, correct=76, accuracy=0.517007
2025-09-14 06:02:16 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:17 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:17 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:17 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 147, 'val_loss': 101.87704467773438, 'val_avg_loss': 0.6930411202566964, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-14 06:02:17 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:02:17 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 147, 'val_loss': 101.87704467773438, 'val_avg_loss': 0.6930411202566964, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-14 06:02:17 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 37, 'val_loss': 24.79200828075409, 'val_avg_loss': 0.6700542778582186, 'val_seen': 37, 'val_correct': 20, 'val_acc': 0.5405405405405406}}
2025-09-14 06:02:17 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 147, 'val_loss': 101.87704467773438, 'val_avg_loss': 0.6930411202566964, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}}
2025-09-14 06:02:17 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:02:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:18 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:18 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:02:18 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.948286, avg_loss=0.648707, seen=40, correct=23, accuracy=0.575000
2025-09-14 06:02:18 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:18 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:20 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:20 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:21 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.948286056518555, 'test_avg_loss': 0.6487071514129639, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:02:21 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 147, 'val_loss': 101.87704467773438, 'val_avg_loss': 0.6930411202566964, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885}
2025-09-14 06:02:21 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.948286056518555, 'test_avg_loss': 0.6487071514129639, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:02:21 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.211257576942444, 'test_avg_loss': 0.6211257576942444, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 06:02:21 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #47', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.948286056518555, 'test_avg_loss': 0.6487071514129639, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}}
2025-09-14 06:02:21 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.948286056518555, 'test_avg_loss': 0.6487071514129639, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}, metrics={'val_total': 147, 'val_loss': 101.87704467773438, 'val_avg_loss': 0.6930411202566964, 'val_seen': 147, 'val_correct': 76, 'val_acc': 0.5170068027210885, 'test_total': 40, 'test_loss': 25.948286056518555, 'test_avg_loss': 0.6487071514129639, 'test_seen': 40, 'test_correct': 23, 'test_acc': 0.575}
2025-09-14 06:02:21 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:02:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:02:22 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=12, total=46)
2025-09-14 06:02:22 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:22 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=23, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:23 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=12
2025-09-14 06:02:23 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=46, loss_sum=31.206394, avg_loss=0.678400, seen=46, correct=25, accuracy=0.543478
2025-09-14 06:02:23 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:23 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:24 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:25 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:25 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 46, 'val_loss': 31.20639419555664, 'val_avg_loss': 0.6783998738164487, 'val_seen': 46, 'val_correct': 25, 'val_acc': 0.5434782608695652}
2025-09-14 06:02:25 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:02:25 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 46, 'val_loss': 31.20639419555664, 'val_avg_loss': 0.6783998738164487, 'val_seen': 46, 'val_correct': 25, 'val_acc': 0.5434782608695652}
2025-09-14 06:02:25 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 12, 'val_loss': 7.936906456947327, 'val_avg_loss': 0.6614088714122772, 'val_seen': 12, 'val_correct': 8, 'val_acc': 0.6666666666666666}}
2025-09-14 06:02:25 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 46, 'val_loss': 31.20639419555664, 'val_avg_loss': 0.6783998738164487, 'val_seen': 46, 'val_correct': 25, 'val_acc': 0.5434782608695652}}
2025-09-14 06:02:25 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:02:25 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:25 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:26 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:02:26 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=28.752172, avg_loss=0.718804, seen=40, correct=19, accuracy=0.475000
2025-09-14 06:02:26 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:26 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:26 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:27 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:27 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 28.752172470092773, 'test_avg_loss': 0.7188043117523193, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 06:02:27 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 46, 'val_loss': 31.20639419555664, 'val_avg_loss': 0.6783998738164487, 'val_seen': 46, 'val_correct': 25, 'val_acc': 0.5434782608695652}
2025-09-14 06:02:27 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 28.752172470092773, 'test_avg_loss': 0.7188043117523193, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 06:02:27 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.396341562271118, 'test_avg_loss': 0.7396341562271118, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 06:02:27 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #48', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 28.752172470092773, 'test_avg_loss': 0.7188043117523193, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}}
2025-09-14 06:02:27 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 28.752172470092773, 'test_avg_loss': 0.7188043117523193, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}, metrics={'val_total': 46, 'val_loss': 31.20639419555664, 'val_avg_loss': 0.6783998738164487, 'val_seen': 46, 'val_correct': 25, 'val_acc': 0.5434782608695652, 'test_total': 40, 'test_loss': 28.752172470092773, 'test_avg_loss': 0.7188043117523193, 'test_seen': 40, 'test_correct': 19, 'test_acc': 0.475}
2025-09-14 06:02:27 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:02:27 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:02:28 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=33, total=132)
2025-09-14 06:02:28 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:28 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=66, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:32 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=33
2025-09-14 06:02:32 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=132, loss_sum=85.354683, avg_loss=0.646626, seen=132, correct=83, accuracy=0.628788
2025-09-14 06:02:32 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:32 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:33 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:33 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:33 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 132, 'val_loss': 85.35468292236328, 'val_avg_loss': 0.6466263857754794, 'val_seen': 132, 'val_correct': 83, 'val_acc': 0.6287878787878788}
2025-09-14 06:02:33 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:02:33 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 132, 'val_loss': 85.35468292236328, 'val_avg_loss': 0.6466263857754794, 'val_seen': 132, 'val_correct': 83, 'val_acc': 0.6287878787878788}
2025-09-14 06:02:33 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 33, 'val_loss': 18.35378348827362, 'val_avg_loss': 0.5561752572204127, 'val_seen': 33, 'val_correct': 25, 'val_acc': 0.7575757575757576}}
2025-09-14 06:02:33 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 132, 'val_loss': 85.35468292236328, 'val_avg_loss': 0.6466263857754794, 'val_seen': 132, 'val_correct': 83, 'val_acc': 0.6287878787878788}}
2025-09-14 06:02:33 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:34 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:34 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:02:34 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=31.061611, avg_loss=0.776540, seen=40, correct=18, accuracy=0.450000
2025-09-14 06:02:34 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:34 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:36 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:37 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:37 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 31.06161117553711, 'test_avg_loss': 0.7765402793884277, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 06:02:37 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 132, 'val_loss': 85.35468292236328, 'val_avg_loss': 0.6466263857754794, 'val_seen': 132, 'val_correct': 83, 'val_acc': 0.6287878787878788}
2025-09-14 06:02:37 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 31.06161117553711, 'test_avg_loss': 0.7765402793884277, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 06:02:37 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.89062225818634, 'test_avg_loss': 0.789062225818634, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 06:02:37 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #49', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 31.06161117553711, 'test_avg_loss': 0.7765402793884277, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}}
2025-09-14 06:02:37 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 31.06161117553711, 'test_avg_loss': 0.7765402793884277, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}, metrics={'val_total': 132, 'val_loss': 85.35468292236328, 'val_avg_loss': 0.6466263857754794, 'val_seen': 132, 'val_correct': 83, 'val_acc': 0.6287878787878788, 'test_total': 40, 'test_loss': 31.06161117553711, 'test_avg_loss': 0.7765402793884277, 'test_seen': 40, 'test_correct': 18, 'test_acc': 0.45}
2025-09-14 06:02:37 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:02:37 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:02:38 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=34, total=133)
2025-09-14 06:02:38 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:38 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=67, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:41 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=34
2025-09-14 06:02:41 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=133, loss_sum=91.607086, avg_loss=0.688775, seen=133, correct=78, accuracy=0.586466
2025-09-14 06:02:41 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:41 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:43 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:43 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:43 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 133, 'val_loss': 91.60708618164062, 'val_avg_loss': 0.6887750840724859, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}
2025-09-14 06:02:43 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:02:43 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 133, 'val_loss': 91.60708618164062, 'val_avg_loss': 0.6887750840724859, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}
2025-09-14 06:02:43 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 34, 'val_loss': 25.283766269683838, 'val_avg_loss': 0.7436401844024658, 'val_seen': 34, 'val_correct': 15, 'val_acc': 0.4411764705882353}}
2025-09-14 06:02:43 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 133, 'val_loss': 91.60708618164062, 'val_avg_loss': 0.6887750840724859, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}}
2025-09-14 06:02:44 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:02:44 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:44 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:45 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:02:45 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=27.860191, avg_loss=0.696505, seen=40, correct=21, accuracy=0.525000
2025-09-14 06:02:45 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:45 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:46 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:46 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:47 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 27.860191345214844, 'test_avg_loss': 0.696504783630371, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 06:02:47 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 133, 'val_loss': 91.60708618164062, 'val_avg_loss': 0.6887750840724859, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338}
2025-09-14 06:02:47 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 27.860191345214844, 'test_avg_loss': 0.696504783630371, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 06:02:47 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.191845715045929, 'test_avg_loss': 0.7191845715045929, 'test_seen': 10, 'test_correct': 4, 'test_acc': 0.4}}
2025-09-14 06:02:47 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #50', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 27.860191345214844, 'test_avg_loss': 0.696504783630371, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}}
2025-09-14 06:02:47 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 27.860191345214844, 'test_avg_loss': 0.696504783630371, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}, metrics={'val_total': 133, 'val_loss': 91.60708618164062, 'val_avg_loss': 0.6887750840724859, 'val_seen': 133, 'val_correct': 78, 'val_acc': 0.5864661654135338, 'test_total': 40, 'test_loss': 27.860191345214844, 'test_avg_loss': 0.696504783630371, 'test_seen': 40, 'test_correct': 21, 'test_acc': 0.525}
2025-09-14 06:02:47 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:02:47 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:02:48 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=21, total=83)
2025-09-14 06:02:48 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:48 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=42, num_train_batch_last_epoch=200, num_train_epoch=3, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:50 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=21
2025-09-14 06:02:50 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=83, loss_sum=56.220360, avg_loss=0.677354, seen=83, correct=52, accuracy=0.626506
2025-09-14 06:02:50 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:50 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:51 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:52 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:52 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 83, 'val_loss': 56.220359802246094, 'val_avg_loss': 0.6773537325571819, 'val_seen': 83, 'val_correct': 52, 'val_acc': 0.6265060240963856}
2025-09-14 06:02:52 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:02:52 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 83, 'val_loss': 56.220359802246094, 'val_avg_loss': 0.6773537325571819, 'val_seen': 83, 'val_correct': 52, 'val_acc': 0.6265060240963856}
2025-09-14 06:02:52 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 21, 'val_loss': 16.325818955898285, 'val_avg_loss': 0.7774199502808707, 'val_seen': 21, 'val_correct': 9, 'val_acc': 0.42857142857142855}}
2025-09-14 06:02:52 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 83, 'val_loss': 56.220359802246094, 'val_avg_loss': 0.6773537325571819, 'val_seen': 83, 'val_correct': 52, 'val_acc': 0.6265060240963856}}
2025-09-14 06:02:52 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:02:52 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:52 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:02:53 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:02:53 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.475689, avg_loss=0.661892, seen=40, correct=25, accuracy=0.625000
2025-09-14 06:02:53 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:02:53 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:54 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:02:55 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:02:55 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.475688934326172, 'test_avg_loss': 0.6618922233581543, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 06:02:55 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 83, 'val_loss': 56.220359802246094, 'val_avg_loss': 0.6773537325571819, 'val_seen': 83, 'val_correct': 52, 'val_acc': 0.6265060240963856}
2025-09-14 06:02:55 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.475688934326172, 'test_avg_loss': 0.6618922233581543, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 06:02:55 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 7.008278906345367, 'test_avg_loss': 0.7008278906345368, 'test_seen': 10, 'test_correct': 6, 'test_acc': 0.6}}
2025-09-14 06:02:55 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #51', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.475688934326172, 'test_avg_loss': 0.6618922233581543, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}}
2025-09-14 06:02:55 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.475688934326172, 'test_avg_loss': 0.6618922233581543, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}, metrics={'val_total': 83, 'val_loss': 56.220359802246094, 'val_avg_loss': 0.6773537325571819, 'val_seen': 83, 'val_correct': 52, 'val_acc': 0.6265060240963856, 'test_total': 40, 'test_loss': 26.475688934326172, 'test_avg_loss': 0.6618922233581543, 'test_seen': 40, 'test_correct': 25, 'test_acc': 0.625}
2025-09-14 06:02:55 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:02:55 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:02:56 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=47, total=188)
2025-09-14 06:02:56 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:02:56 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=94, num_train_batch_last_epoch=200, num_train_epoch=2, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:03:02 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=47
2025-09-14 06:03:02 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=188, loss_sum=129.244232, avg_loss=0.687469, seen=188, correct=103, accuracy=0.547872
2025-09-14 06:03:02 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:03:02 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:03:03 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:03:03 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:03:03 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 188, 'val_loss': 129.24423217773438, 'val_avg_loss': 0.6874693200943318, 'val_seen': 188, 'val_correct': 103, 'val_acc': 0.5478723404255319}
2025-09-14 06:03:03 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:03:03 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 188, 'val_loss': 129.24423217773438, 'val_avg_loss': 0.6874693200943318, 'val_seen': 188, 'val_correct': 103, 'val_acc': 0.5478723404255319}
2025-09-14 06:03:03 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 47, 'val_loss': 33.21217542886734, 'val_avg_loss': 0.7066420304014328, 'val_seen': 47, 'val_correct': 24, 'val_acc': 0.5106382978723404}}
2025-09-14 06:03:03 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 188, 'val_loss': 129.24423217773438, 'val_avg_loss': 0.6874693200943318, 'val_seen': 188, 'val_correct': 103, 'val_acc': 0.5478723404255319}}
2025-09-14 06:03:03 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:03:03 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:03:04 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:03:05 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:03:05 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=25.380028, avg_loss=0.634501, seen=40, correct=22, accuracy=0.550000
2025-09-14 06:03:05 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:03:05 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:03:06 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:03:06 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:03:06 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 25.380027770996094, 'test_avg_loss': 0.6345006942749023, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 06:03:06 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 188, 'val_loss': 129.24423217773438, 'val_avg_loss': 0.6874693200943318, 'val_seen': 188, 'val_correct': 103, 'val_acc': 0.5478723404255319}
2025-09-14 06:03:06 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 25.380027770996094, 'test_avg_loss': 0.6345006942749023, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 06:03:06 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 6.971916675567627, 'test_avg_loss': 0.6971916675567627, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 06:03:06 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #52', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 25.380027770996094, 'test_avg_loss': 0.6345006942749023, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}}
2025-09-14 06:03:06 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 25.380027770996094, 'test_avg_loss': 0.6345006942749023, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}, metrics={'val_total': 188, 'val_loss': 129.24423217773438, 'val_avg_loss': 0.6874693200943318, 'val_seen': 188, 'val_correct': 103, 'val_acc': 0.5478723404255319, 'test_total': 40, 'test_loss': 25.380027770996094, 'test_avg_loss': 0.6345006942749023, 'test_seen': 40, 'test_correct': 22, 'test_acc': 0.55}
2025-09-14 06:03:06 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:03:06 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:07 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'val' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=50, total=200)
2025-09-14 06:03:07 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-val][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:03:08 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=100, num_train_batch_last_epoch=200, num_train_epoch=1, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:03:14 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=50
2025-09-14 06:03:14 (federatedscope.llm.trainer.trainer:1217) INFO: [val|final] total=200, loss_sum=133.428116, avg_loss=0.667141, seen=200, correct=121, accuracy=0.605000
2025-09-14 06:03:14 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'val_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:03:14 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:03:15 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:03:16 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:03:16 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after val] eval_metrics            = {'val_total': 200, 'val_loss': 133.42811584472656, 'val_avg_loss': 0.6671405792236328, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 06:03:16 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after val] metrics (merged so far) = {}
2025-09-14 06:03:16 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after val] ctx.eval_metrics        = {'val_total': 200, 'val_loss': 133.42811584472656, 'val_avg_loss': 0.6671405792236328, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 06:03:16 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'val', 'Rank': '0/4', 'Local': True, 'Results': {'val_total': 50, 'val_loss': 32.080185651779175, 'val_avg_loss': 0.6416037130355835, 'val_seen': 50, 'val_correct': 33, 'val_acc': 0.66}}
2025-09-14 06:03:16 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'val', 'Aggregated': True, 'Results_raw': {'val_total': 200, 'val_loss': 133.42811584472656, 'val_avg_loss': 0.6671405792236328, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}}
2025-09-14 06:03:16 (federatedscope.llm.trainer.trainer:310) INFO: Dataloader for 'test' has been reset and recreated. (sharded=True, world_size=4, rank=0, local_count=10, total=40)
2025-09-14 06:03:16 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=after-reset-test][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:03:16 (federatedscope.llm.trainer.trainer:808) INFO: [fit-start] batch_or_epoch=None, train_iter=None, num_train_batch=20, num_train_batch_last_epoch=200, num_train_epoch=5, grad_accum_step=2, accum_in_accel=2
2025-09-14 06:03:17 (federatedscope.core.trainers.trainer:443) INFO: [agg debug] using_accel=True, world=4, rank=0, local_total=10
2025-09-14 06:03:17 (federatedscope.llm.trainer.trainer:1217) INFO: [test|final] total=40, loss_sum=26.326414, avg_loss=0.658160, seen=40, correct=26, accuracy=0.650000
2025-09-14 06:03:17 (federatedscope.llm.trainer.trainer:1245) INFO: [Memory Cleanup] Deleted ctx attrs: ['val_loader', 'test_loader', 'test_loader_iter', 'loss_batch_total', 'loss_regular_total', 'ys_true', 'ys_pred', 'data_batch']
2025-09-14 06:03:17 (federatedscope.llm.misc.debug_utils:120) INFO: [DBG_EMB][tag=before-accel-free_memory][rank=0] tok_len=151643 | base=AdapterModel | in_emb=(Embedding) num=151646 ptr=139859033325568 | out_emb=(None) num=None ptr=None | lora_ptr=None
2025-09-14 06:03:18 (federatedscope.llm.trainer.trainer:1269) INFO: Accelerator memory has been freed (object preserved).
2025-09-14 06:03:19 (federatedscope.llm.trainer.trainer:1292) INFO: [VRAM] round=0 reserved=2284MB allocated=2200MB
2025-09-14 06:03:19 (federatedscope.llm.llm_local.client:335) INFO: [DEBUG][after test] eval_metrics            = {'test_total': 40, 'test_loss': 26.326414108276367, 'test_avg_loss': 0.6581603527069092, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:03:19 (federatedscope.llm.llm_local.client:336) INFO: [DEBUG][after test] metrics (merged so far) = {'val_total': 200, 'val_loss': 133.42811584472656, 'val_avg_loss': 0.6671405792236328, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605}
2025-09-14 06:03:19 (federatedscope.llm.llm_local.client:337) INFO: [DEBUG][after test] ctx.eval_metrics        = {'test_total': 40, 'test_loss': 26.326414108276367, 'test_avg_loss': 0.6581603527069092, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:03:19 (federatedscope.core.workers.client:463) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'test', 'Rank': '0/4', 'Local': True, 'Results': {'test_total': 10, 'test_loss': 8.555004477500916, 'test_avg_loss': 0.8555004477500916, 'test_seen': 10, 'test_correct': 5, 'test_acc': 0.5}}
2025-09-14 06:03:19 (federatedscope.core.workers.client:475) INFO: {'Role': 'Client #53', 'Round': 0, 'Split': 'test', 'Aggregated': True, 'Results_raw': {'test_total': 40, 'test_loss': 26.326414108276367, 'test_avg_loss': 0.6581603527069092, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}}
2025-09-14 06:03:19 (federatedscope.llm.llm_local.client:364) INFO: [DEBUG][before write] agg_all={'test_total': 40, 'test_loss': 26.326414108276367, 'test_avg_loss': 0.6581603527069092, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}, metrics={'val_total': 200, 'val_loss': 133.42811584472656, 'val_avg_loss': 0.6671405792236328, 'val_seen': 200, 'val_correct': 121, 'val_acc': 0.605, 'test_total': 40, 'test_loss': 26.326414108276367, 'test_avg_loss': 0.6581603527069092, 'test_seen': 40, 'test_correct': 26, 'test_acc': 0.65}
2025-09-14 06:03:19 (federatedscope.llm.llm_local.client:377) INFO: [DEBUG] combined keys=['test_total', 'test_loss', 'test_avg_loss', 'test_seen', 'test_correct', 'test_acc', 'val_total', 'val_loss', 'val_avg_loss', 'val_seen', 'val_correct', 'val_acc'], has_val=True, has_test=True
2025-09-14 06:03:19 (federatedscope.core.workers.server:772) INFO: {'Role': 'Server #', 'Round': 0, 'Results_weighted_avg': {'val_total': 124.9622641509434, 'val_loss': 104.45416076504067, 'val_avg_loss': 0.6686122476127633, 'val_acc': 0.5927827268609391, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971021, 'test_acc': 0.5764150943396227}, 'Results_avg': {'val_total': 124.9622641509434, 'val_loss': 83.5513003007421, 'val_avg_loss': 0.6668241485407689, 'val_acc': 0.599155698783686, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971023, 'test_acc': 0.5764150943396226}, 'Results_fairness': {'val_total': 124.9622641509434, 'test_total': 40.0, 'val_loss_std': 41.964863524700306, 'val_loss_bottom_decile': 19.350215911865234, 'val_loss_top_decile': 135.22088623046875, 'val_loss_min': 6.796031951904297, 'val_loss_max': 141.12095642089844, 'val_loss_bottom10%': 9.855106353759766, 'val_loss_top10%': 137.58502960205078, 'val_loss_cos1': 0.8936162391787014, 'val_loss_entropy': 3.8201426134008445, 'val_avg_loss_std': 0.02490274240682854, 'val_avg_loss_bottom_decile': 0.637953987121582, 'val_avg_loss_top_decile': 0.6992599303463856, 'val_avg_loss_min': 0.6046942472457886, 'val_avg_loss_max': 0.7217738363477919, 'val_avg_loss_bottom10%': 0.6177389938486244, 'val_avg_loss_top10%': 0.7069205263012353, 'val_avg_loss_cos1': 0.9993033931584694, 'val_avg_loss_entropy': 3.9695915200631435, 'val_acc_std': 0.04839462071801657, 'val_acc_bottom_decile': 0.5434782608695652, 'val_acc_top_decile': 0.6567164179104478, 'val_acc_min': 0.5, 'val_acc_max': 0.7272727272727273, 'val_acc_bottom10%': 0.517434462649609, 'val_acc_top10%': 0.6888817312365334, 'val_acc_cos1': 0.9967538684369384, 'val_acc_entropy': 3.967042531389332, 'test_loss_std': 1.7411168790157556, 'test_loss_bottom_decile': 24.862163543701172, 'test_loss_top_decile': 29.395231246948242, 'test_loss_min': 23.658571243286133, 'test_loss_max': 31.06161117553711, 'test_loss_bottom10%': 24.111371994018555, 'test_loss_top10%': 30.22962538401286, 'test_loss_cos1': 0.9979264024294009, 'test_loss_entropy': 3.968225238629628, 'test_avg_loss_std': 0.043527921975393895, 'test_avg_loss_bottom_decile': 0.6215540885925293, 'test_avg_loss_top_decile': 0.734880781173706, 'test_avg_loss_min': 0.5914642810821533, 'test_avg_loss_max': 0.7765402793884277, 'test_avg_loss_bottom10%': 0.6027842998504639, 'test_avg_loss_top10%': 0.7557406346003214, 'test_avg_loss_cos1': 0.9979264024294007, 'test_avg_loss_entropy': 3.9682252386355845, 'test_acc_std': 0.08103328781695249, 'test_acc_bottom_decile': 0.475, 'test_acc_top_decile': 0.675, 'test_acc_min': 0.375, 'test_acc_max': 0.725, 'test_acc_bottom10%': 0.42000000000000004, 'test_acc_top10%': 0.7000000000000001, 'test_acc_cos1': 0.9902625206399759, 'test_acc_entropy': 3.960120961118567}}
2025-09-14 06:03:19 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 6.796031951904297, 'val_total': 11.0, 'val_avg_loss': 0.6046942472457886, 'val_acc': 0.7272727272727273, 'test_total': 40.0, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_acc': 0.725}}
2025-09-14 06:03:19 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 6.796031951904297, 'val_total': 11.0, 'val_avg_loss': 0.6046942472457886, 'val_acc': 0.7272727272727273, 'test_total': 40.0, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_acc': 0.725}, 'client_summarized_weighted_avg': {'val_loss': 104.45416076504067, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6686122476127633, 'val_acc': 0.5927827268609391, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971021, 'test_acc': 0.5764150943396227}}
2025-09-14 06:03:19 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 6.796031951904297, 'val_total': 11.0, 'val_avg_loss': 0.6046942472457886, 'val_acc': 0.7272727272727273, 'test_total': 40.0, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_acc': 0.725}, 'client_summarized_weighted_avg': {'val_loss': 104.45416076504067, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6686122476127633, 'val_acc': 0.5927827268609391, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971021, 'test_acc': 0.5764150943396227}, 'client_summarized_avg': {'val_loss': 83.5513003007421, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6668241485407689, 'val_acc': 0.599155698783686, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971023, 'test_acc': 0.5764150943396226}}
2025-09-14 06:03:19 (root:790) INFO: Find new best result: {'client_best_individual': {'val_loss': 6.796031951904297, 'val_total': 11.0, 'val_avg_loss': 0.6046942472457886, 'val_acc': 0.7272727272727273, 'test_total': 40.0, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_acc': 0.725}, 'client_summarized_weighted_avg': {'val_loss': 104.45416076504067, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6686122476127633, 'val_acc': 0.5927827268609391, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971021, 'test_acc': 0.5764150943396227}, 'client_summarized_avg': {'val_loss': 83.5513003007421, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6668241485407689, 'val_acc': 0.599155698783686, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971023, 'test_acc': 0.5764150943396226}, 'client_summarized_fairness': {'val_loss_entropy': 3.8201426134008445, 'val_loss_cos1': 0.8936162391787014, 'val_loss_top10%': 137.58502960205078, 'val_loss_bottom10%': 9.855106353759766, 'val_loss_max': 141.12095642089844, 'val_loss_min': 6.796031951904297, 'val_loss_top_decile': 135.22088623046875, 'val_loss_bottom_decile': 19.350215911865234, 'val_loss_std': 41.964863524700306, 'val_total': 124.9622641509434, 'test_total': 40.0, 'val_avg_loss_std': 0.02490274240682854, 'val_avg_loss_bottom_decile': 0.637953987121582, 'val_avg_loss_top_decile': 0.6992599303463856, 'val_avg_loss_min': 0.6046942472457886, 'val_avg_loss_max': 0.7217738363477919, 'val_avg_loss_bottom10%': 0.6177389938486244, 'val_avg_loss_top10%': 0.7069205263012353, 'val_avg_loss_cos1': 0.9993033931584694, 'val_avg_loss_entropy': 3.9695915200631435, 'val_acc_std': 0.04839462071801657, 'val_acc_bottom_decile': 0.5434782608695652, 'val_acc_top_decile': 0.6567164179104478, 'val_acc_min': 0.5, 'val_acc_max': 0.7272727272727273, 'val_acc_bottom10%': 0.517434462649609, 'val_acc_top10%': 0.6888817312365334, 'val_acc_cos1': 0.9967538684369384, 'val_acc_entropy': 3.967042531389332, 'test_loss_std': 1.7411168790157556, 'test_loss_bottom_decile': 24.862163543701172, 'test_loss_top_decile': 29.395231246948242, 'test_loss_min': 23.658571243286133, 'test_loss_max': 31.06161117553711, 'test_loss_bottom10%': 24.111371994018555, 'test_loss_top10%': 30.22962538401286, 'test_loss_cos1': 0.9979264024294009, 'test_loss_entropy': 3.968225238629628, 'test_avg_loss_std': 0.043527921975393895, 'test_avg_loss_bottom_decile': 0.6215540885925293, 'test_avg_loss_top_decile': 0.734880781173706, 'test_avg_loss_min': 0.5914642810821533, 'test_avg_loss_max': 0.7765402793884277, 'test_avg_loss_bottom10%': 0.6027842998504639, 'test_avg_loss_top10%': 0.7557406346003214, 'test_avg_loss_cos1': 0.9979264024294007, 'test_avg_loss_entropy': 3.9682252386355845, 'test_acc_std': 0.08103328781695249, 'test_acc_bottom_decile': 0.475, 'test_acc_top_decile': 0.675, 'test_acc_min': 0.375, 'test_acc_max': 0.725, 'test_acc_bottom10%': 0.42000000000000004, 'test_acc_top10%': 0.7000000000000001, 'test_acc_cos1': 0.9902625206399759, 'test_acc_entropy': 3.960120961118567}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:518) INFO: Server: Final evaluation is finished! Starting merging results.
2025-09-14 06:03:19 (federatedscope.core.workers.server:644) INFO: {'Role': 'Server #', 'Round': 'Final', 'Results_raw': {'client_best_individual': {'val_loss': 6.796031951904297, 'val_total': 11.0, 'val_avg_loss': 0.6046942472457886, 'val_acc': 0.7272727272727273, 'test_total': 40.0, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_acc': 0.725}, 'client_summarized_weighted_avg': {'val_loss': 104.45416076504067, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6686122476127633, 'val_acc': 0.5927827268609391, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971021, 'test_acc': 0.5764150943396227}, 'client_summarized_avg': {'val_loss': 83.5513003007421, 'val_total': 124.9622641509434, 'val_avg_loss': 0.6668241485407689, 'val_acc': 0.599155698783686, 'test_total': 40.0, 'test_loss': 26.994448427884084, 'test_avg_loss': 0.6748612106971023, 'test_acc': 0.5764150943396226}, 'client_summarized_fairness': {'val_loss_entropy': 3.8201426134008445, 'val_loss_cos1': 0.8936162391787014, 'val_loss_top10%': 137.58502960205078, 'val_loss_bottom10%': 9.855106353759766, 'val_loss_max': 141.12095642089844, 'val_loss_min': 6.796031951904297, 'val_loss_top_decile': 135.22088623046875, 'val_loss_bottom_decile': 19.350215911865234, 'val_loss_std': 41.964863524700306, 'val_total': 124.9622641509434, 'test_total': 40.0, 'val_avg_loss_std': 0.02490274240682854, 'val_avg_loss_bottom_decile': 0.637953987121582, 'val_avg_loss_top_decile': 0.6992599303463856, 'val_avg_loss_min': 0.6046942472457886, 'val_avg_loss_max': 0.7217738363477919, 'val_avg_loss_bottom10%': 0.6177389938486244, 'val_avg_loss_top10%': 0.7069205263012353, 'val_avg_loss_cos1': 0.9993033931584694, 'val_avg_loss_entropy': 3.9695915200631435, 'val_acc_std': 0.04839462071801657, 'val_acc_bottom_decile': 0.5434782608695652, 'val_acc_top_decile': 0.6567164179104478, 'val_acc_min': 0.5, 'val_acc_max': 0.7272727272727273, 'val_acc_bottom10%': 0.517434462649609, 'val_acc_top10%': 0.6888817312365334, 'val_acc_cos1': 0.9967538684369384, 'val_acc_entropy': 3.967042531389332, 'test_loss_std': 1.7411168790157556, 'test_loss_bottom_decile': 24.862163543701172, 'test_loss_top_decile': 29.395231246948242, 'test_loss_min': 23.658571243286133, 'test_loss_max': 31.06161117553711, 'test_loss_bottom10%': 24.111371994018555, 'test_loss_top10%': 30.22962538401286, 'test_loss_cos1': 0.9979264024294009, 'test_loss_entropy': 3.968225238629628, 'test_avg_loss_std': 0.043527921975393895, 'test_avg_loss_bottom_decile': 0.6215540885925293, 'test_avg_loss_top_decile': 0.734880781173706, 'test_avg_loss_min': 0.5914642810821533, 'test_avg_loss_max': 0.7765402793884277, 'test_avg_loss_bottom10%': 0.6027842998504639, 'test_avg_loss_top10%': 0.7557406346003214, 'test_avg_loss_cos1': 0.9979264024294007, 'test_avg_loss_entropy': 3.9682252386355845, 'test_acc_std': 0.08103328781695249, 'test_acc_bottom_decile': 0.475, 'test_acc_top_decile': 0.675, 'test_acc_min': 0.375, 'test_acc_max': 0.725, 'test_acc_bottom10%': 0.42000000000000004, 'test_acc_top10%': 0.7000000000000001, 'test_acc_cos1': 0.9902625206399759, 'test_acc_entropy': 3.960120961118567}}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #1', 'Round': 1, 'Results_raw': {'val_total': 146, 'val_loss': 96.26714324951172, 'val_avg_loss': 0.6593639948596693, 'val_acc': 0.5958904109589042, 'test_total': 40, 'test_loss': 26.258567810058594, 'test_avg_loss': 0.6564641952514648, 'test_acc': 0.65}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #2', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 7.420977592468262, 'val_avg_loss': 0.6746343265880238, 'val_acc': 0.6363636363636364, 'test_total': 40, 'test_loss': 25.547168731689453, 'test_avg_loss': 0.6386792182922363, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #3', 'Round': 1, 'Results_raw': {'val_total': 36, 'val_loss': 25.983858108520508, 'val_avg_loss': 0.7217738363477919, 'val_acc': 0.5833333333333334, 'test_total': 40, 'test_loss': 27.077810287475586, 'test_avg_loss': 0.6769452571868897, 'test_acc': 0.525}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #4', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 7.269842147827148, 'val_avg_loss': 0.660894740711559, 'val_acc': 0.6363636363636364, 'test_total': 40, 'test_loss': 30.471088409423828, 'test_avg_loss': 0.7617772102355957, 'test_acc': 0.425}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #5', 'Round': 1, 'Results_raw': {'val_total': 14, 'val_loss': 9.448443412780762, 'val_avg_loss': 0.6748888151986259, 'val_acc': 0.6428571428571429, 'test_total': 40, 'test_loss': 26.81270980834961, 'test_avg_loss': 0.6703177452087402, 'test_acc': 0.55}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #6', 'Round': 1, 'Results_raw': {'val_total': 134, 'val_loss': 86.28258514404297, 'val_avg_loss': 0.643899889134649, 'val_acc': 0.6567164179104478, 'test_total': 40, 'test_loss': 29.395231246948242, 'test_avg_loss': 0.734880781173706, 'test_acc': 0.525}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #7', 'Round': 1, 'Results_raw': {'val_total': 57, 'val_loss': 36.80078125, 'val_avg_loss': 0.6456277412280702, 'val_acc': 0.631578947368421, 'test_total': 40, 'test_loss': 24.47749900817871, 'test_avg_loss': 0.6119374752044677, 'test_acc': 0.7}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #8', 'Round': 1, 'Results_raw': {'val_total': 69, 'val_loss': 49.002586364746094, 'val_avg_loss': 0.7101824110832767, 'val_acc': 0.5797101449275363, 'test_total': 40, 'test_loss': 29.282154083251953, 'test_avg_loss': 0.7320538520812988, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #9', 'Round': 1, 'Results_raw': {'val_total': 188, 'val_loss': 122.9746322631836, 'val_avg_loss': 0.6541203843786362, 'val_acc': 0.6117021276595744, 'test_total': 40, 'test_loss': 26.637611389160156, 'test_avg_loss': 0.6659402847290039, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #10', 'Round': 1, 'Results_raw': {'val_total': 63, 'val_loss': 40.27643585205078, 'val_avg_loss': 0.6393085055881076, 'val_acc': 0.6190476190476191, 'test_total': 40, 'test_loss': 23.73523712158203, 'test_avg_loss': 0.5933809280395508, 'test_acc': 0.675}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #11', 'Round': 1, 'Results_raw': {'val_total': 32, 'val_loss': 19.350215911865234, 'val_avg_loss': 0.6046942472457886, 'val_acc': 0.6875, 'test_total': 40, 'test_loss': 24.862163543701172, 'test_avg_loss': 0.6215540885925293, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #12', 'Round': 1, 'Results_raw': {'val_total': 137, 'val_loss': 90.62240600585938, 'val_avg_loss': 0.6614774161011633, 'val_acc': 0.6131386861313869, 'test_total': 40, 'test_loss': 26.351844787597656, 'test_avg_loss': 0.6587961196899415, 'test_acc': 0.675}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #13', 'Round': 1, 'Results_raw': {'val_total': 72, 'val_loss': 48.65612030029297, 'val_avg_loss': 0.6757794486151801, 'val_acc': 0.5277777777777778, 'test_total': 40, 'test_loss': 26.04688262939453, 'test_avg_loss': 0.6511720657348633, 'test_acc': 0.55}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #14', 'Round': 1, 'Results_raw': {'val_total': 160, 'val_loss': 106.65338897705078, 'val_avg_loss': 0.6665836811065674, 'val_acc': 0.60625, 'test_total': 40, 'test_loss': 23.658571243286133, 'test_avg_loss': 0.5914642810821533, 'test_acc': 0.725}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #15', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 134.8588104248047, 'val_avg_loss': 0.6742940521240235, 'val_acc': 0.61, 'test_total': 40, 'test_loss': 27.46124267578125, 'test_avg_loss': 0.6865310668945312, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #16', 'Round': 1, 'Results_raw': {'val_total': 136, 'val_loss': 88.8609848022461, 'val_avg_loss': 0.6533895941341624, 'val_acc': 0.6029411764705882, 'test_total': 40, 'test_loss': 27.07958221435547, 'test_avg_loss': 0.6769895553588867, 'test_acc': 0.55}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #17', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 129.4142303466797, 'val_avg_loss': 0.6470711517333985, 'val_acc': 0.62, 'test_total': 40, 'test_loss': 27.64134979248047, 'test_avg_loss': 0.6910337448120117, 'test_acc': 0.625}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #18', 'Round': 1, 'Results_raw': {'val_total': 135, 'val_loss': 94.2452392578125, 'val_avg_loss': 0.6981128833912037, 'val_acc': 0.5481481481481482, 'test_total': 40, 'test_loss': 24.38504981994629, 'test_avg_loss': 0.6096262454986572, 'test_acc': 0.7}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #19', 'Round': 1, 'Results_raw': {'val_total': 110, 'val_loss': 74.29232788085938, 'val_avg_loss': 0.6753847989169034, 'val_acc': 0.5454545454545454, 'test_total': 40, 'test_loss': 29.987918853759766, 'test_avg_loss': 0.7496979713439942, 'test_acc': 0.4}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #20', 'Round': 1, 'Results_raw': {'val_total': 126, 'val_loss': 85.25604248046875, 'val_avg_loss': 0.676635257781498, 'val_acc': 0.5555555555555556, 'test_total': 40, 'test_loss': 25.611896514892578, 'test_avg_loss': 0.6402974128723145, 'test_acc': 0.7}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #21', 'Round': 1, 'Results_raw': {'val_total': 153, 'val_loss': 103.04515838623047, 'val_avg_loss': 0.6734977672302646, 'val_acc': 0.5882352941176471, 'test_total': 40, 'test_loss': 28.27863311767578, 'test_avg_loss': 0.7069658279418946, 'test_acc': 0.525}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #22', 'Round': 1, 'Results_raw': {'val_total': 11, 'val_loss': 6.796031951904297, 'val_avg_loss': 0.6178210865367543, 'val_acc': 0.7272727272727273, 'test_total': 40, 'test_loss': 30.753204345703125, 'test_avg_loss': 0.7688301086425782, 'test_acc': 0.475}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #23', 'Round': 1, 'Results_raw': {'val_total': 30, 'val_loss': 18.34023666381836, 'val_avg_loss': 0.6113412221272786, 'val_acc': 0.7, 'test_total': 40, 'test_loss': 25.24478530883789, 'test_avg_loss': 0.6311196327209473, 'test_acc': 0.575}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #24', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 135.85008239746094, 'val_avg_loss': 0.6792504119873047, 'val_acc': 0.55, 'test_total': 40, 'test_loss': 28.554889678955078, 'test_avg_loss': 0.713872241973877, 'test_acc': 0.475}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #25', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 141.12095642089844, 'val_avg_loss': 0.7056047821044922, 'val_acc': 0.555, 'test_total': 40, 'test_loss': 28.888559341430664, 'test_avg_loss': 0.7222139835357666, 'test_acc': 0.375}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #26', 'Round': 1, 'Results_raw': {'val_total': 161, 'val_loss': 101.89190673828125, 'val_avg_loss': 0.6328689859520574, 'val_acc': 0.6832298136645962, 'test_total': 40, 'test_loss': 24.30050277709961, 'test_avg_loss': 0.6075125694274902, 'test_acc': 0.65}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #27', 'Round': 1, 'Results_raw': {'val_total': 123, 'val_loss': 79.22407531738281, 'val_avg_loss': 0.6440981733120554, 'val_acc': 0.6260162601626016, 'test_total': 40, 'test_loss': 25.828842163085938, 'test_avg_loss': 0.6457210540771484, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #28', 'Round': 1, 'Results_raw': {'val_total': 75, 'val_loss': 48.962554931640625, 'val_avg_loss': 0.6528340657552083, 'val_acc': 0.6, 'test_total': 40, 'test_loss': 27.13894271850586, 'test_avg_loss': 0.6784735679626465, 'test_acc': 0.575}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #29', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 136.6398468017578, 'val_avg_loss': 0.683199234008789, 'val_acc': 0.585, 'test_total': 40, 'test_loss': 26.746126174926758, 'test_avg_loss': 0.668653154373169, 'test_acc': 0.625}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #30', 'Round': 1, 'Results_raw': {'val_total': 170, 'val_loss': 112.35128784179688, 'val_avg_loss': 0.6608899284811581, 'val_acc': 0.5823529411764706, 'test_total': 40, 'test_loss': 27.057708740234375, 'test_avg_loss': 0.6764427185058594, 'test_acc': 0.575}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #31', 'Round': 1, 'Results_raw': {'val_total': 193, 'val_loss': 133.45672607421875, 'val_avg_loss': 0.6914856273275582, 'val_acc': 0.5958549222797928, 'test_total': 40, 'test_loss': 27.524927139282227, 'test_avg_loss': 0.6881231784820556, 'test_acc': 0.475}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #32', 'Round': 1, 'Results_raw': {'val_total': 112, 'val_loss': 69.66057586669922, 'val_avg_loss': 0.6219694273812431, 'val_acc': 0.6785714285714286, 'test_total': 40, 'test_loss': 29.708698272705078, 'test_avg_loss': 0.7427174568176269, 'test_acc': 0.5}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #33', 'Round': 1, 'Results_raw': {'val_total': 74, 'val_loss': 51.780006408691406, 'val_avg_loss': 0.6997298163336676, 'val_acc': 0.5540540540540541, 'test_total': 40, 'test_loss': 26.651521682739258, 'test_avg_loss': 0.6662880420684815, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #34', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 129.06765747070312, 'val_avg_loss': 0.6453382873535156, 'val_acc': 0.64, 'test_total': 40, 'test_loss': 25.79199981689453, 'test_avg_loss': 0.6447999954223633, 'test_acc': 0.65}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #35', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 127.5907974243164, 'val_avg_loss': 0.637953987121582, 'val_acc': 0.65, 'test_total': 40, 'test_loss': 27.70172691345215, 'test_avg_loss': 0.6925431728363037, 'test_acc': 0.55}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #36', 'Round': 1, 'Results_raw': {'val_total': 54, 'val_loss': 35.95836639404297, 'val_avg_loss': 0.6658956739637587, 'val_acc': 0.6296296296296297, 'test_total': 40, 'test_loss': 26.752498626708984, 'test_avg_loss': 0.6688124656677246, 'test_acc': 0.625}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #37', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 135.68392944335938, 'val_avg_loss': 0.6784196472167969, 'val_acc': 0.585, 'test_total': 40, 'test_loss': 27.01873016357422, 'test_avg_loss': 0.6754682540893555, 'test_acc': 0.65}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #38', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 132.57188415527344, 'val_avg_loss': 0.6628594207763672, 'val_acc': 0.6, 'test_total': 40, 'test_loss': 26.696176528930664, 'test_avg_loss': 0.6674044132232666, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #39', 'Round': 1, 'Results_raw': {'val_total': 83, 'val_loss': 58.03857421875, 'val_avg_loss': 0.6992599303463856, 'val_acc': 0.5060240963855421, 'test_total': 40, 'test_loss': 28.949342727661133, 'test_avg_loss': 0.7237335681915283, 'test_acc': 0.45}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #40', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 129.94406127929688, 'val_avg_loss': 0.6497203063964844, 'val_acc': 0.605, 'test_total': 40, 'test_loss': 26.59191131591797, 'test_avg_loss': 0.6647977828979492, 'test_acc': 0.575}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #41', 'Round': 1, 'Results_raw': {'val_total': 119, 'val_loss': 80.42697143554688, 'val_avg_loss': 0.6758569028197217, 'val_acc': 0.5966386554621849, 'test_total': 40, 'test_loss': 26.881980895996094, 'test_avg_loss': 0.6720495223999023, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #42', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 135.22088623046875, 'val_avg_loss': 0.6761044311523438, 'val_acc': 0.585, 'test_total': 40, 'test_loss': 26.023921966552734, 'test_avg_loss': 0.6505980491638184, 'test_acc': 0.7}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #43', 'Round': 1, 'Results_raw': {'val_total': 89, 'val_loss': 60.2930908203125, 'val_avg_loss': 0.6774504586551966, 'val_acc': 0.550561797752809, 'test_total': 40, 'test_loss': 25.213354110717773, 'test_avg_loss': 0.6303338527679443, 'test_acc': 0.65}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #44', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 140.99447631835938, 'val_avg_loss': 0.7049723815917969, 'val_acc': 0.5, 'test_total': 40, 'test_loss': 26.841896057128906, 'test_avg_loss': 0.6710474014282226, 'test_acc': 0.575}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #45', 'Round': 1, 'Results_raw': {'val_total': 100, 'val_loss': 65.63003540039062, 'val_avg_loss': 0.6563003540039063, 'val_acc': 0.58, 'test_total': 40, 'test_loss': 29.17289161682129, 'test_avg_loss': 0.7293222904205322, 'test_acc': 0.475}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #46', 'Round': 1, 'Results_raw': {'val_total': 110, 'val_loss': 74.80377197265625, 'val_avg_loss': 0.6800342906605114, 'val_acc': 0.5363636363636364, 'test_total': 40, 'test_loss': 25.80602264404297, 'test_avg_loss': 0.6451505661010742, 'test_acc': 0.6}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #47', 'Round': 1, 'Results_raw': {'val_total': 147, 'val_loss': 101.87704467773438, 'val_avg_loss': 0.6930411202566964, 'val_acc': 0.5170068027210885, 'test_total': 40, 'test_loss': 25.948286056518555, 'test_avg_loss': 0.6487071514129639, 'test_acc': 0.575}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #48', 'Round': 1, 'Results_raw': {'val_total': 46, 'val_loss': 31.20639419555664, 'val_avg_loss': 0.6783998738164487, 'val_acc': 0.5434782608695652, 'test_total': 40, 'test_loss': 28.752172470092773, 'test_avg_loss': 0.7188043117523193, 'test_acc': 0.475}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #49', 'Round': 1, 'Results_raw': {'val_total': 132, 'val_loss': 85.35468292236328, 'val_avg_loss': 0.6466263857754794, 'val_acc': 0.6287878787878788, 'test_total': 40, 'test_loss': 31.06161117553711, 'test_avg_loss': 0.7765402793884277, 'test_acc': 0.45}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #50', 'Round': 1, 'Results_raw': {'val_total': 133, 'val_loss': 91.60708618164062, 'val_avg_loss': 0.6887750840724859, 'val_acc': 0.5864661654135338, 'test_total': 40, 'test_loss': 27.860191345214844, 'test_avg_loss': 0.696504783630371, 'test_acc': 0.525}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #51', 'Round': 1, 'Results_raw': {'val_total': 83, 'val_loss': 56.220359802246094, 'val_avg_loss': 0.6773537325571819, 'val_acc': 0.6265060240963856, 'test_total': 40, 'test_loss': 26.475688934326172, 'test_avg_loss': 0.6618922233581543, 'test_acc': 0.625}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #52', 'Round': 1, 'Results_raw': {'val_total': 188, 'val_loss': 129.24423217773438, 'val_avg_loss': 0.6874693200943318, 'val_acc': 0.5478723404255319, 'test_total': 40, 'test_loss': 25.380027770996094, 'test_avg_loss': 0.6345006942749023, 'test_acc': 0.55}}
2025-09-14 06:03:19 (federatedscope.core.workers.server:665) INFO: {'Role': 'Client #53', 'Round': 1, 'Results_raw': {'val_total': 200, 'val_loss': 133.42811584472656, 'val_avg_loss': 0.6671405792236328, 'val_acc': 0.605, 'test_total': 40, 'test_loss': 26.326414108276367, 'test_avg_loss': 0.6581603527069092, 'test_acc': 0.65}}
2025-09-14 06:03:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #0, the system-related metrics are: {'id': 0, 'fl_end_time_minutes': 233.60204376666667, 'total_model_size': 0, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 102384, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:19 (federatedscope.core.workers.client:842) INFO: ================= client 1 received finish message =================
2025-09-14 06:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #1, the system-related metrics are: {'id': 1, 'fl_end_time_minutes': 233.60387236666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:19 (federatedscope.core.workers.client:842) INFO: ================= client 2 received finish message =================
2025-09-14 06:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #2, the system-related metrics are: {'id': 2, 'fl_end_time_minutes': 233.53582523333336, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:19 (federatedscope.core.workers.client:842) INFO: ================= client 3 received finish message =================
2025-09-14 06:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #3, the system-related metrics are: {'id': 3, 'fl_end_time_minutes': 233.49668123333333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:19 (federatedscope.core.workers.client:842) INFO: ================= client 4 received finish message =================
2025-09-14 06:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #4, the system-related metrics are: {'id': 4, 'fl_end_time_minutes': 233.45723031666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:19 (federatedscope.core.workers.client:842) INFO: ================= client 5 received finish message =================
2025-09-14 06:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #5, the system-related metrics are: {'id': 5, 'fl_end_time_minutes': 233.41750368333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:19 (federatedscope.core.workers.client:842) INFO: ================= client 6 received finish message =================
2025-09-14 06:03:19 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:19 (federatedscope.core.monitors.monitor:268) INFO: In worker #6, the system-related metrics are: {'id': 6, 'fl_end_time_minutes': 233.3780131, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:19 (federatedscope.core.workers.client:842) INFO: ================= client 7 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #7, the system-related metrics are: {'id': 7, 'fl_end_time_minutes': 233.33854655, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 8 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #8, the system-related metrics are: {'id': 8, 'fl_end_time_minutes': 233.29949636666666, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 9 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #9, the system-related metrics are: {'id': 9, 'fl_end_time_minutes': 233.26054299999998, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 10 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #10, the system-related metrics are: {'id': 10, 'fl_end_time_minutes': 233.22164386666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 11 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #11, the system-related metrics are: {'id': 11, 'fl_end_time_minutes': 233.18300206666666, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 12 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #12, the system-related metrics are: {'id': 12, 'fl_end_time_minutes': 233.14398108333336, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 13 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #13, the system-related metrics are: {'id': 13, 'fl_end_time_minutes': 233.10500066666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 14 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #14, the system-related metrics are: {'id': 14, 'fl_end_time_minutes': 233.0480952833333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 15 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #15, the system-related metrics are: {'id': 15, 'fl_end_time_minutes': 233.0054394, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 16 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #16, the system-related metrics are: {'id': 16, 'fl_end_time_minutes': 232.96607553333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 17 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #17, the system-related metrics are: {'id': 17, 'fl_end_time_minutes': 232.92660625, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 18 received finish message =================
2025-09-14 06:03:20 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:20 (federatedscope.core.monitors.monitor:268) INFO: In worker #18, the system-related metrics are: {'id': 18, 'fl_end_time_minutes': 232.88721991666668, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:20 (federatedscope.core.workers.client:842) INFO: ================= client 19 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #19, the system-related metrics are: {'id': 19, 'fl_end_time_minutes': 232.84815823333332, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 20 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #20, the system-related metrics are: {'id': 20, 'fl_end_time_minutes': 232.8089522, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 21 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #21, the system-related metrics are: {'id': 21, 'fl_end_time_minutes': 232.76987666666668, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 22 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #22, the system-related metrics are: {'id': 22, 'fl_end_time_minutes': 232.73102636666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 23 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #23, the system-related metrics are: {'id': 23, 'fl_end_time_minutes': 232.69228148333335, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 24 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #24, the system-related metrics are: {'id': 24, 'fl_end_time_minutes': 232.65237993333332, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 25 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #25, the system-related metrics are: {'id': 25, 'fl_end_time_minutes': 232.61042243333335, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 26 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #26, the system-related metrics are: {'id': 26, 'fl_end_time_minutes': 232.57122130000002, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 27 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #27, the system-related metrics are: {'id': 27, 'fl_end_time_minutes': 232.51697061666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 28 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #28, the system-related metrics are: {'id': 28, 'fl_end_time_minutes': 232.47842516666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 29 received finish message =================
2025-09-14 06:03:21 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:21 (federatedscope.core.monitors.monitor:268) INFO: In worker #29, the system-related metrics are: {'id': 29, 'fl_end_time_minutes': 232.43925731666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:21 (federatedscope.core.workers.client:842) INFO: ================= client 30 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #30, the system-related metrics are: {'id': 30, 'fl_end_time_minutes': 232.40036705, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 31 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #31, the system-related metrics are: {'id': 31, 'fl_end_time_minutes': 232.3606450833333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 32 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #32, the system-related metrics are: {'id': 32, 'fl_end_time_minutes': 232.31993961666666, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 33 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #33, the system-related metrics are: {'id': 33, 'fl_end_time_minutes': 232.27931345, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 34 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #34, the system-related metrics are: {'id': 34, 'fl_end_time_minutes': 232.23886958333333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 35 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #35, the system-related metrics are: {'id': 35, 'fl_end_time_minutes': 232.19702933333335, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 36 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #36, the system-related metrics are: {'id': 36, 'fl_end_time_minutes': 232.15261778333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 37 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #37, the system-related metrics are: {'id': 37, 'fl_end_time_minutes': 232.11186286666668, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 38 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #38, the system-related metrics are: {'id': 38, 'fl_end_time_minutes': 232.07139516666666, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 39 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #39, the system-related metrics are: {'id': 39, 'fl_end_time_minutes': 232.03046206666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 40 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #40, the system-related metrics are: {'id': 40, 'fl_end_time_minutes': 231.97226556666666, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 41 received finish message =================
2025-09-14 06:03:22 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:22 (federatedscope.core.monitors.monitor:268) INFO: In worker #41, the system-related metrics are: {'id': 41, 'fl_end_time_minutes': 231.93168468333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:22 (federatedscope.core.workers.client:842) INFO: ================= client 42 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #42, the system-related metrics are: {'id': 42, 'fl_end_time_minutes': 231.89122186666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 43 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #43, the system-related metrics are: {'id': 43, 'fl_end_time_minutes': 231.8507788, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 44 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #44, the system-related metrics are: {'id': 44, 'fl_end_time_minutes': 231.81026793333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 45 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #45, the system-related metrics are: {'id': 45, 'fl_end_time_minutes': 231.76757206666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 46 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #46, the system-related metrics are: {'id': 46, 'fl_end_time_minutes': 231.72401045, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 47 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #47, the system-related metrics are: {'id': 47, 'fl_end_time_minutes': 231.68332748333333, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 48 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #48, the system-related metrics are: {'id': 48, 'fl_end_time_minutes': 231.64250345, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 49 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #49, the system-related metrics are: {'id': 49, 'fl_end_time_minutes': 231.60134196666667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 50 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #50, the system-related metrics are: {'id': 50, 'fl_end_time_minutes': 231.56034815, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 51 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #51, the system-related metrics are: {'id': 51, 'fl_end_time_minutes': 231.5197890166667, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 52 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #52, the system-related metrics are: {'id': 52, 'fl_end_time_minutes': 231.47903061666665, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133224, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.workers.client:842) INFO: ================= client 53 received finish message =================
2025-09-14 06:03:23 (federatedscope.core.trainers.torch_trainer:169) INFO: [Model Sync] lenient load | loaded=2016 skipped=0 missing=291 unexpected=0
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:268) INFO: In worker #53, the system-related metrics are: {'id': 53, 'fl_end_time_minutes': 231.43855203333334, 'total_model_size': 520167552, 'total_flops': 0, 'total_upload_bytes': 0, 'total_download_bytes': 2133216, 'global_convergence_round': 0, 'local_convergence_round': 0, 'global_convergence_time_minutes': 0, 'local_convergence_time_minutes': 0}
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:441) INFO: We will compress the file eval_results.raw into a .gz file, and delete the old one
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:359) INFO: After merging the system metrics from all works, we got avg: defaultdict(None, {'id': 'sys_avg', 'sys_avg/fl_end_time_minutes': 232.53761087932102, 'sys_avg/total_model_size': '486.88M', 'sys_avg/total_flops': '0.0', 'sys_avg/total_upload_bytes': '0.0', 'sys_avg/total_download_bytes': '2.0M', 'sys_avg/global_convergence_round': 0.0, 'sys_avg/local_convergence_round': 0.0, 'sys_avg/global_convergence_time_minutes': 0.0, 'sys_avg/local_convergence_time_minutes': 0.0})
2025-09-14 06:03:23 (federatedscope.core.monitors.monitor:360) INFO: After merging the system metrics from all works, we got std: defaultdict(None, {'id': 'sys_std', 'sys_std/fl_end_time_minutes': 0.6432998364557915, 'sys_std/total_model_size': '66.88M', 'sys_std/total_flops': '0.0', 'sys_std/total_upload_bytes': '0.0', 'sys_std/total_download_bytes': '267.37K', 'sys_std/global_convergence_round': 0.0, 'sys_std/local_convergence_round': 0.0, 'sys_std/global_convergence_time_minutes': 0.0, 'sys_std/local_convergence_time_minutes': 0.0})
